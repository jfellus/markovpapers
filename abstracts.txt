%X Morphological Component Analysis (MCA) is a new method which takes advantage of the sparse representation of structured data in large overcomplete dictionaries to separate features in the data based on the diversity of their morphology. It is an efficient technique in such problems as separating an image into texture and piecewise smooth parts or for inpainting applications. The MCA algorithm consists of an iterative alternating projection and thresholding scheme, using a successively decreasing threshold towards zero with each iteration. In this paper, the MCA algorithm is extended to the analysis of spherical data maps as may occur in a number of areas such as geophysics, astrophysics or medical imaging. Practically, this extension is made possible thanks to the variety of recently developed transforms on the sphere including several multiscale transforms such as the undecimated isotropic wavelet transform on the sphere, the ridgelet and curvelet transforms on the sphere. An MCA-inpainting method is then directly extended to the case of spherical maps allowing us to treat problems where parts of the data are missing or corrupted. We demonstrate the usefulness of these new tools of spherical data analysis by focusing on a selection of challenging applications in physics and astrophysics.
%X Purpose The non-invasive examination of the cardiac functions is an important diagnostic, and follow up, tool for detection and evaluation of cardiovascular dysfunctions. Magnetic resonance (MR) cardiac tagging is a technique imposing temporary spatial variation in longitudinal magnetization around the heart area, so that temporary tag patterns deforming with cardiac motion can be captured in a sequence of MR images. The tag patterns play the role of noninvasive fiducial markers, by tracking of which an accurate estimation of cardiac motion can be achieved in vivo. This motion pattern can be subsequently analyzed and abnormalities can be detected and assessed. Segmentation of myocardium is an essential step for most of the approaches used to estimate dynamic displacement field since only the tag lines inside the myocardium are of importance for the deformation estimation. However, in most of the literature, segmentation of myocardium is done manually, which involves a large amount of tedious, timeconsuming and error-prone work. In the work presented here a fully automatic segmentation method is proposed for delineation of myocardial boundaries in the tagged MR images. Methods A fully-automatic method has been developed by the authors to segment myocardium of left ventricle by identifying epicardial and endocardial contours using implicit active contour models driven by tag structures and constrained by shape priors. The method has been developed to work, and tested with a temporal short-axis sequences of tagged cardiac MR images acquired using the SPAMM sequence. Results The initial assessment of the proposed method was performed against manual segmentation carried out by clinicians. The evaluation data consisted of five data sets each containing five or six tagged MR images. The results obtained for the six images from the third data set are shown in Fig. 2, where manual delineation and automatic delineation are represented by thick and thin lines respectively. It can be seen that in general there is relatively good correlation between manual and automatic delineation of the myocardial boundaries. It has to be noted that for the reported initial validation results no information is available about intra- or inter-observer variability and therefore it is impossible to quantitatively assess quality of the ground truth. Indeed a closer look at the images in Fig. 2 reveals that on some occasions (e.g. for images 2 and 5) the automatically selected contours are closer to the endocardial boundaries than the manual ones.
%X Purpose: Noninvasive imaging assessment of cardiac function is important in cardiovascular disease diagnosis, especially for evaluation of local cardiac motion. Tagged Cardiac MRI has been developed for this purpose, but evaluation of the results requires quantification and automation. Method: Two methods utilizing active contour modeling for wall motion extraction based on tagged cardiac MRI scans were evaluated based on properties of tracking methods in the image domain and frequency domain. Three criteria were used: accuracy, inter-subject and intra-subject sensitivity. The tracking results were evaluated by a medical expert. The evaluation methodology and its possible generalization to other diagnostic methods were considered. Results: Image-domain and frequency-domain analysis of tagged cardiac MRI data sets were evaluated demonstrating that the image domain method provides better results. The image-domain method method is much more resistant to changes in the data, this time, due to a different subject being scanned. The frequency domain approach is not suitable for clinical applications, as the global error is significantly increased (more than 20%). Conclusion: The image-domain method was found most effective, and it can generate a set of clearly identified parameters. The evaluation approach can be an interesting alternative to classical psychovisual studies which are time consuming and often fastidious for clinicians.
%X In the last decade, the 3D modeling of cities has become one of the challenges of multimedia search and an important focus in object recognition. We are interested in locating various primitives, especially windows, in Paris façades. An image of edges is readable by the human visual system, which performs a perceptual grouping and is thus able to catch the entities of the image.We tried in this paper to model this mecanism. Images are represented as adjacency graphs of contour segments, attributed with orientation and proximity information. To achieve the graph matching, we propose several variants of a new similarity based on sets of paths carried by the graphs, able to perform the perceptual grouping and robust to scale changes. The similarity between paths takes into account the similarity between sets of edges and the similarity between regions defined by these paths. The retrieval of images containing a specific object is performed thanks to a SVM or knn classifier. For object localization, we use a vote system from the paths selected by the matching algorithm. Our results show that we are able to extract the windows from images of façades from examples.
%X In this paper we present a method of image indexing and retrieval which takes into account the relative positions of the regions within the image. Indexing is based on a segmentation of the image into fuzzy regions ; we propose an algorithm which produces a fuzzy segmentation. The image retrieval is based on inexact graph matching, taking into account both the similarity between regions and the spatial relation between them. We propose, on one hand a solution to reduce the combinatorial complexity of the graph matching, and on the other hand, a measure of similarity between graphs allowing the result images ranking. A relevance feedback process based on region classifiers allows then a good generalization to a large variety of the regions. The method is adapted to partial queries, aiming for example at retrieving images containing a specific type of object. Applications may be of two types, firstly an on-line search from a partial query, with a relevance feedback aiming at interactively leading the search, and secondly an off-line learning of categories from a set of examples of the object. The name of the system is FReBIR for Fuzzy Region-Based Image Retrieval.
%X An original method to increase the tuning range of a monolithic-microwave integrated-circuit (MMIC) varactor diode is presented in this paper. An active circuit simulating a negative capacitance is connected to the varactor diode. This method allows to increase the varactor's tuning range more than ten times and to compensate its series resistance at the same time. A MMIC simulating a negative capacitance have been successfully fabricated and measured. To the best of the authors' knowledge, this is the first realization of a MMIC simulating a negative capacitance
%X In the context of frame-based multimedia wireless transmission, a link adaptation strategy is proposed, assuming that the source decoder may accept some remaining errors at the output of the channel decoder. Based on a target mean bit error rate for erro- neous frames, a minimum bit-energy-to-equivalent-noise ratio is chosen. Under this constraint, a new link adaptation criterion is proposed: the maximization of the minimum user's information rate through dynamic spreading gain and power control, al- lowing to guarantee a transmission for each and every user. An analytical solution to this constrained optimization problem is proposed and its performance is studied in a Rayleigh-fading environment.
%X We present a search engine dedicated to 3D object databases. The originality of the method is to represent models by adjacency graphs of surfacic regions. After segmentation of the 3D surface of a model, regions are described by various shape descriptors. The similarity between graphs is computed by kernels on graphs computed from kernels on walks. These kernel functions take into account both the similarity between regions and their spatial relationship. The search engine performs interactive research in a database from a query object, by using semi-supervised classication. The system is applied to a database of 3D high resolution artwork models. We show that a graph representation outperforms a global description of the objects, when using the same descriptors.
%X Neurobiology studies showed that the role of the Anterior Cingulate Cortex of the brain is primarily responsible for avoiding repeated mistakes. According to vigilance threshold, which denotes the tolerance to risks, we can differentiate between a learning mechanism that takes risks, and one that averts risks. The tolerance to risk plays an important role in such learning mechanism. Results have shown the differences in learning capacity between risk-taking and risk avert behaviors. In this paper we propose a learning mechanism that is able to learn from negative and positive feedback. It is composed of two phases, evaluation and decision-making phase. In the evaluation phase, we use a Kohonen Self Organizing Map technique to represent success and failure. Decision-making is based on an early warning mechanism that enables to avoid repeating past mistakes. Our approach is presented with an implementation on a simulated planar biped robot, controlled by a reflexive lowlevel neural controller. The learning system adapts the dynamics and range of a hip sensor neuron of the controller in order for the robot to walk on flat and slope terrain. Results show that success and failure maps can learn better with a threshold that is more tolerant to risk. This gives rise to robustness to the controller even in the presence of slope variations.
%X Neurobiological studies showed the important role of Cen- teral Pattern Generators for spinal cord in the control and sensory feed- back of animals' locomotion. In this paper, this role is taken into account in modeling bipedal locomotion of a robot. Indeed, as a rhythm gener- ator, a non-classical model of a neuron that can generate oscillatory as well as diverse motor patterns is presented. This allows di®erent motion patterns on the joints to be generated easily. Complex tasks, like walk- ing, running, and obstacle avoidance require more than just oscillatory movements. Our model provides the ability to switch between intrinsic behaviors, to enable the robot to react against environmental changes quickly. To achieve complex tasks while handling external perturbations, a new space for joints' patterns is introduced. Patterns are generated by our learning mechanism based on success and failure with the concept of vigilance. This allows the robot to be prudent at the beginning and adventurous at the end of the learning process, inducing a more e±cient exploration for new patterns. Motion patterns of the joint are classi¯ed into classes according to a metric, which re°ects the kinetic energy of the limb. Due to the classi¯cation metric, high-level control for action learning is introduced. For instance, an adaptive behavior of the rhythm generator neurons in the hip and the knee joints against external per- turbation are shown to demonstrate the e®ectiveness of the proposed learning approach.
%X This paper shows how to use a DC motor and its PID controller, to behave analogously to a muscle. A model of the muscle that has been learned by a NNARX (Neural Network Auto Regressive eXogenous) structure is used. The PID parameters are tuned by an MLP Network with a special indirect online learning algorithm. The calculation of the learning algorithm is performed based on a mathematical equation of the DC motor or with a Neural Network identification of the motor. For each of the two algorithms, the output of the muscle model is used as a reference for the DC motor control loop. The results show that we succeeded in forcing the physical system to behave in the same way as the muscle model with acceptable margin of error. An implementation in the knees of a simulated biped robot is realized. Simulation compares articular trajectories with and without the muscle emulator and shows that with muscle emulator, articular trajectories become closer to the human being ones and that total power consumption is reduced.
%X This paper deals with a research work aimed to develop a new three degrees of freedom (DOF) mechanism for humanoid robots. The main idea is to build hybrid (3DOF) mechanism, which avoids the drawbacks of the serial and parallel mechanisms. The new solution has to merge the advantages of both classical (serial and parallel) structures in order to achieve optimal performances. The proposed mechanism can be used as a solution for several modules in humanoid robot. The hip mechanism is taken as an example to illustrate the contribution of this paper. To evaluate the performances of the system, simulation of this new mechanism is carried out with Adams software. Geometrical and Kinematic models are developed and included in the simulation tool. Based on biomechanical data, analysis of the new kinematic structure is carried out. The design of the proposed solution is then described. Finally the first prototype developed for the HYDROïD robot's hip is presented. This mechanism is a part of an International patent accepted at INPI- France.
%X In this paper, we show how it is possible to obtain a walk – Standup - walk cycle with a biped robot using only a state machine controller. This work is a continuation of another in which the same approach was used to control the walking gait of the same robot. We introduce four critical angles that affect robot speed and step length. Our control approach consists of two control levels. The low level is a PID controller and the high level is a Dynamic Walking Algorithm, inspired from human locomotion, which tunes the PD controllers and impose the reference signals based on the walking which increases walk stability. This method could be easily implemented in real time because it needs acceptable calculation time. We validated the control approach to a dynamic simulation of our 14DOF biped called ROBIAN. We prove that we can maintain robot walking and stopping stability and walk cycle's repetition without referencing a predefined trajectory or detecting the center of pressure. Results show that the walk of the biped is very similar to human one.
%X It is now well established that iterative decoding approaches the performance of Maximum Likelihood Decoding of sparse graph codes, asymptotically in the block length. For a finite length sparse code, iterative decoding fails on specific subgraphs generically termed as trapping sets. Trapping sets give rise to error floor, an abrupt degradation of the code error performance in the high signal to noise ratio regime. In this paper, we will study a recently introduced class of quantized iterative decoders, for which the messages are defined on a finite alphabet and which sucessfully decode errors on subgraphs that are uncorrectable by conventional decoders such as min-sum or the belief propagation. We will especially study the performance of the proposed finite alphabet iterative decoders on the famous 155,64,20) Tanner code.
%X In this paper we present a model of reinforcement learning (RL) which can be used to solve goal-oriented navigation tasks. Our model supposes that transitions between places are learned in the hip- pocampus (CA pyramidal cells) and associated with information coming from path-integration. The RL neural network acts as a bias on these transitions to perform action selection. RL originates in the basal ganglia and matches observations of reward-based activity in dopaminergic neu- rons. Experiments were conducted in a simulated environment. We show that our model using transitions and inspired by Q-learning performs more efficiently than traditional actor-critic models of the basal ganglia based on temporal difference (TD) learning and using static states.
%X We propose a model of the hippocampus aimed at learning the timed association between subsequent sensory events. The properties of the neural network allow it to learn and predict the evolution of con- tinuous rate-coded signals as well as the occurrence of transitory events, using both spatial and non-spatial information. The system is able to provide predictions based on the time trace of past sensory events. Per- formance of the neural network in the precise temporal learning of spatial and non-spatial signals is tested in a simulated experiment. The ability of the hippocampus proper to predict the occurrence of upcoming spatio- temporal events could play a crucial role in the carrying out of tasks requiring accurate time estimation and spatial localization.
%X In a previous model [3], a spectral timing neural network [4] was used to account for the role of the Hs in the acquisition of classical conditioning. The ability to estimate the timing between separate events was then used to learn and predict transitions between places in the environment. We propose a neural architecture based on this work and explaining the out-of-field activities in the Hs along with their temporal prediction capabilities. The model uses the hippocampo-cortical pathway as a means to spread reward signals to entorhinal neurons. Secondary predictions of the reward signal are then learned, based on transition learning, by pyramidal neurons of the CA region.
%X SVM et Mélanges de Gaussiennes pour la recherche d'images
%X Appliquée à la gestion des base d'images, l'analyse automatique a encore des difficultés à représenter les concepts abstraits que les utilisateurs recherchent. Bien que les stratégies de recherche interactives améliorent les résultats de la recherche, les performances d'un système dépendent toujours de la représentation de la base. Nous proposons dans ce papier deux méthodes d'optimisation supervisée d'un ensemble de vecteurs -- l'une vectorielle, et l'autre basée sur la théorie des noyaux. A partir d'un ensemble incomplet d'annotations partielles, ces méthodes améliorent la representation des données, même si la taille, le nombre, et la structure des concepts sont inconnus. Afin de valider ces approches, une étude expérimentale est menée sur une grande base d'images.
%X Dans le cadre de la recherche interactive d'images dans une base de données, nous nous intéressons à des mesures de similarité d'image qui per- mettent d'améliorer l'apprentissage et utilisables en temps réel lors de la re- cherche. Les images sont représentées sous la forme de graphes d'adjacence de régions ﬂoues. Pour comparer des graphes valués nous employons des noyaux de graphes s'appuyant sur des ensembles de chaînes, extraites des graphes compa- rés. Nous proposons un cadre général permettant l'emploi de différents noyaux et différents types de chaînes(sans cycle, avec boucles) autorisant des apparie- ments inexacts. Nous avons effectué des comparaisons sur deux bases issues de Columbia et Caltech et montré que des chaînes de très faible dimension (lon- gueur inférieur à 3) sont les plus efﬁcaces pour retrouver des classes d'objets.
%X In this paper, we present a new version of our content-based image retrieval system RETIN. It is based on adaptive quantization of the color space, together with new features aiming at representing the spatial relationship between colors. Color analysis is also extended to texture. Using these powerful indexes, an original interactive retrieval strategy is introduced. The process is based on two steps for handling the retrieval of very large image categories. First, a controlled exploration method of the database is presented. Second, a relevance feedback method based on statistical learning is proposed. All the steps are evaluated by experiments on a generalist database.
%X For the management of digital document collections, automatic database analysis still has difﬁculties to deal with semantic queries and abstract concepts that users are looking for. Whenever interactive learning strategies may improve the results of the search, system performances still depend on the representation of the document collection. We introduce in this paper a weakly supervised optimization of a feature vectors set. According to an incomplete set of partial labels, the method improves the representation of the collection, even if the size, the number, and the structure of the concepts are unknown. Experiments have been carried out on synthetic and real data in order to validate our approach.
%X Abstract. This paper deals with content-based image retrieval. When the user is looking for large categories, statistical classiﬁcation techniques are efﬁcient as soon as the training set is large enough. We introduce a two-step – exploration, classiﬁcation – interactive strategy designed for category retrieval. The ﬁrst step aims at getting a useful initial training set for the classiﬁcation step. A stochastic image selection process is used instead of the usual strategy based on a similarity score ranking. This process is dedicated to explore the database in order to collect examples as various as possible of the searched category. The second step aims at providing the best classiﬁcation between relevant and irrelevant images. Based on SVM, the classiﬁcation applies an active learning strategy through user interaction. A quality assessment is carried out on the ANN and COREL databases in order to compare and validate our approach.
%X This paper presents a search engine architecture, RETIN, aiming at retrieving com- plex categories in large image databases. For indexing, a scheme based on a two-step quantization process is presented to compute visual codebooks. The similarity be- tween images is represented in a kernel framework. Such a similarity is combined with online learning strategies motivated by recent Machine-Learning developments such as Active Learning. Additionally, an offine supervised learning is embedded in the kernel framework, offering a real opportunity to learn semantic categories. Experiments with real scenario carried out from the Corel Photo database demon- strate the effciency and the relevance of the RETIN strategy and its outstanding performances in comparison to up-to-date strategies.
%X Active learning methods have been considered with increased interest in the statistical learning community. Initially developed within a classiﬁcation framework, a lot of extensions are now being proposed to handle multimedia applications. This paper provides algorithms within a statistical framework to extend active learning for online content-based image retrieval (CBIR). The classiﬁcation framework is presented with experi- ments to compare several powerful classiﬁcation techniques in this information retrieval context. Focusing on interactive meth- ods, active learning strategy is then described. The limitations of this approach for CBIR are emphasized before presenting our new active selection process RETIN. First, as any active method is sensitive to the boundary estimation between classes, the RETIN strategy carries out a boundary correction to make the retrieval process more robust. Second, the criterion of generalization error to optimize the active learning selection is modiﬁed to better represent the CBIR objective of database ranking. Third, a batch processing of images is proposed. Our strategy leads to a fast and efﬁcient active learning scheme to retrieve sets of online images (query concept). Experiments on large databases show that the RETIN method performs well in comparison to several other active strategies.
%X This paper presents a new algorithm based on boost- ing for interactive object retrieval in images. Recent works propose ”online boosting” algorithms where weak classiﬁer sets are iteratively trained from data. These algorithms are proposed for visual tracking in videos, and are not well adapted to ”online boosting” for interactive retrieval. We propose in this paper to iteratively build weak classiﬁers from images, labeled as positive by the user during a retrieval session. A novel active learning strategy for the selection of im- ages for user annotation is also proposed. This strategy is used to enhance the strong classiﬁer resulting from ”boosting” process, but also to build new weak classi- ﬁers. Experiments have been carried out on a generalist database in order to compare the proposed method to a SVM based reference approach.
%X Recent approaches of graph comparison consider graphs as sets of paths. Kernels on graphs are then computed from kernels on paths. A common strat- egy for graph retrieval is to perform pairwise compar- isons. In this paper, we propose to follow a different strategy, where we collect a set of paths into a dictio- nary, and then project each graph to this dictionary. Then, graphs can be classiﬁed using powerful classiﬁ- cation methods, such as SVM. Furthermore, we collect the paths through interaction with a user. This strategy is ten times faster than a straight comparisons of paths. Experiments have been carried out on a database of city windows.
%X In the framework of the interactive search in image databases, we are interested in similarity measures able to learn during the search and usable in real-time. Im- ages are represented by adjacency graphs of fuzzy re- gions. In order to compare attributed graphs, we em- ploy kernels on graphs built on sets of paths. In this pa- per, we introduce a fast kernel function whose similar- ity is based on several matches. We also introduce new features for edges in the graph. Experiments on a spe- ciﬁc database having objects with heterogeneous back- grounds show the performance of our object retrieval technique.
%X We propose in this paper a general kernel framework to deal with database object retrieval embedded in images with het- erogeneous background. We use local features computed on fuzzy regions for image representation summarized in bags, and we propose original kernel functions to deal with sets of features and spatial constraints. Combined with SVMs clas- siﬁcation and online learning scheme, the resulting algorithm satisﬁes the robustness requirements for representation and classiﬁcation of objects. Experiments on a speciﬁc database having objects with heterogeneous backgrounds show the per- formance of our object retrieval technique.
%X In this paper, a kernel-based method for multi-object re- trieval in large image database is presented. First, our approach exploits a fuzzy region segmentation ap- proach in order to get robust local feature extraction and characterization. All the region features are summarized in bags representing the image index. The main part of this work concerns the kernel functions to deal with sets of fea- tures. Based on the linear combination of minor kernels, a family of kernels on bags is introduced. Several weighting schemes and combinations are proposed. Their introduction are motivated in the speciﬁc context of dealing with multi- object recognition with heterogeneous background. Com- bined with SVMs classiﬁcation and interactive online learn- ing framework, the resulting algorithm satisﬁes the robust- ness requirements for representation and classiﬁcation of ob- jects. Experiments and comparisons demonstrate the good performances of our multi-object retrieval technique.
%X Active learning methods have been considered with an increased interest in the content-based image retrieval (CBIR) community. Those methods used to be based on classical classiﬁcation problems, and do not deal with the particular characteristics of the CBIR. One of those characteristics is the criteria to optimize, for instance the er- ror of generalization for classiﬁcation, which is not the most adapted to CBIR context. Thus, we introduce in this paper an active selection which chooses the image the user should label such as the Mean Av- erage Precision is increased. The method is smartly combined with previous propositions, and lead to a fast and efﬁcient active learning scheme. Experiments on a large database have carried out in order to compare our approach to several other methods.
%X The automatic computation of features for content-based image re- trieval still has difﬁculties to represent the concepts the user has in mind. Whenever an additional learning strategy (such as relevance feedback) can improve the results of the search, the system perfor- mances still depend on the representation of the image collection. We introduce in this paper a supervised optimization of a set of fea- ture vectors. According to an incomplete set of partial labels, the method improves the representation of the image collection, even if the size, the number, and the structure of the concepts are unknown. Experiments have been carried out on a large generalist database in order to validate our approach.
%X We present a new system, called Retimm, for searching databases made of documents containing images and text. Images are indexed by colour and texture distributions.. Colour and texture classes are obtained by a quantization adapted to the whole database. Signatures are ranked m times, once for each dimension, but values are not stored. The search engine works as a vote system : the score for each document is the total of the votes of all coordinates, these last votes depending on a k-nn search on each dimension. Retimm is able to retrieve very quickly images from large databases from any request composed of one or several images and/or one or several words. The system is interactive, since the query can be modified at any moment by adding or removing images or words.
%X Content-based image retrieval systems still have difﬁculties to bridge the semantic gap between the low-level representation of images and the high level concepts the user is looking for. Rele- vance feedback methods deal with this problem using labels pro- vided by users, but only during the current retrieval session. In this paper, we introduce a semantic learning method to manage user la- bels in CBIR applications. Our approach uses a kernel matrix to represent semantic information in a statistical learning framework. The kernel matrix is updated according to labels provided by users after retrieval sessions. Experiments have been carried out on a large generalist database in order to validate our approach.
%X Active learning methods have been consid- ered with an increasing interest for user inter- active systems. In this paper, we propose an eﬃcient active learning scheme to deal with this particular context. An active boundary correction is proposed in order to deal with few training data. Experiments are carried out on the COREL photo database.
%X A lot of relevance feedback methods have been pro- posed to deal with Content-Based Image Retrieval (CBIR) problems. Their goal is to interactively learn the seman- tic queries that users have in mind. Interaction is used to ﬁll the gap between the semantic meaning and the low-level im- age representations. The purpose of this article is to analyze how to merge all the semantic information that users provided to the system during past retrieval sessions. We propose an approach to exploit the knowledge provided by user interaction based on binary annotations (relevant or irrelevant images). Such se- mantic annotations may be integrated in the similarity ma- trix of the database images. This similarity matrix is ana- lyzed in the kernel matrix framework. In this context, a ker- nel adaptation method is proposed, but taking care of pre- serving the properties of kernels. Using this approach, a se- mantic kernel is incrementally learnt. To deal with practical constraint implementations, an eigendecomposition of the whole matrix is considered, and a efﬁcient scheme is proposed to compute a low-rank ap- proximated kernel matrix. It allows a strict control of the required memory space and of the algorithm complexity, which is linear to the database size. Experiments have been carried out on a large generalist database in order to validate the approach.
%X Active learning methods have been considered with an increasing interest in the content-based image retrieval (CBIR) community. In this article, we propose an efﬁ- cient method based on active learning strategy to retrieve large image categories. At each feedback step, the system optimizes the image set presented to the user in order to speed up the retrieval. Experimental tests on COREL photo database have been carried out.
%X Statistical learning methods are currently considered with an increasing interest in the content-based image retrieval (CBIR) community. We compare in this article two leader techniques for classiﬁcation tasks. The ﬁrst method uses one-class and two-class SVM to discriminate data. The second approach is based on Gaussian Mixture to model classes. To deal with the speciﬁcity of the CBIR classiﬁca- tion task, adaptations have been proposed. Experimental tests on a generalist database have been carried out. Ad- vantages and drawbacks are discussed for each method.
%X This paper deals with content-based image indexing and category retrieval in general databases. Statistical learning approaches have been recently introduced in CBIR. Labelled images are considered as training data in learning strategy based on classiﬁcation process. We introduce an active learning strategy to select the most difﬁcult images to classify with only few training data. Experimentations are carried out on the COREL database. We compare seven clas- siﬁcation strategies to evaluate the active learning contribution in CBIR.
%X Cet article présente un nouvel algorithme de recherche in- teractive d'objets dans les images basé sur le principe du boosting. De récents travaux ont proposé des algorithmes de boosting "en ligne" dans lesquels des ensembles de clas- siﬁeurs faibles sont itérativement construit à partir des données. Dédiées au suivi d'objets dans les vidéos, ces mé- thodes ne s'appliquent pas efﬁcacement au contexte de la recherche interactive. Nous proposons ici de sélectionner les classiﬁeurs faibles itérativement à partir des images an- notées par l'utilisateur dans un processus de recherche in- teractive. Le choix des images à annoter intervient à deux niveaux. Tout d'abord il est utilisé comme stratégie de sé- lection active aﬁn d'optimiser le classiﬁeur (fort) courant. Il participe aussi à l'élaboration des classiﬁeurs faibles à considérer dans le processus de boosting. Des expériences ont été menées pour comparer cette nouvelle approche à une méthode de référence basée sur des SVMs.
%X In this paper, a new class of low-density parity-check (LDPC) codes, named hybrid LDPC codes, is introduced. Hybrid LDPC codes are characterized by an irregular connectivity profile and heterogeneous orders of the symbols in the codeword. It is shown in particular that the class of hybrid LDPC codes can be asymptotically characterized and optimized using density evolution (DE) framework, and a technique to maximize the minimum distance of the code is presented. Numerical assessment of hybrid LDPC code performances is provided, by comparing them to protograph-based and multiedge-type (MET) LDPC codes. Hybrid LDPC codes are shown to allow to achieve an interesting tradeoff between good error-floor performance and good waterfall region with nonbinary coding techniques.
%X A wide variety of visual recognition systems are developed for precise tasks and types of objects. In this paper we would like to emphasize ways to build a more generic recognition system. Perception is one of these mechanisms that psychologists particularly pointed out as a fundamental one for actively organizing and making sense of input sensory information. Based on psychological assumptions, we propose to explore the concept of perception, infer formalization in the dynamical system framework and quantitatively analyze it on robotic platforms using a unique simple neuronal architecture based on the association of visual and motor information (movements of the body or part of the body). This coupling of sensory flows of information can be characterized by a sensorimotor invariant, a dynamical attractor that we identify as a perception function. For place, object or facial expression recognition, we show how simple sensori-motor architecture can be applied to accomplish each task in terms of behavioral recognition. In each application, some pertinent visual information, based on classical focus point detection, are organized as local views and associated to an action or an internal state corresponding to a set of actions, in order to reach a location, an object or recognize a facial expression. The active learning phase for different points of view or face expressions allows the emergence of a stable perception linked to a stable sensori-motor attractor and allows the robot to perform a stable behavior in very different initial conditions. We will show how the attractor/perception emerges during the learning phase and evaluate its spatial generalization properties.
%X Orthogonal frequency division multiplexing (OFDM) is the modulation technique used in most of the high-rate communication standards. However, OFDM signals exhibit high peak average to power ratio (PAPR) that makes them particularly sensitive to nonlinear distortions caused by high-power amplifiers. Hence, the amplifier needs to operate at large output backoff, thereby decreasing the average efficiency of the transmitter. One way to reduce PAPR consists in clipping the amplitude of the OFDM signal introducing an additional noise that degrades the overall system performance. In that case, the receiver needs to set up an algorithm that compensates this clipping noise. In this paper, we propose three new iterative receivers with growing complexity and performance that operate at severe clipping: the first and simplest receiver uses a Viterbi algorithm as channel decoder whereas the other two implement a soft-input soft-output (SISO) decoder. Each soft receiver is analyzed through EXIT charts for different mappings. Finally, the performances of the receivers are simulated on both short time-varying channel and AWGN channel.
%X II. N OTATIONS AND D EFINITIONS The progressive edge-growth (PEG) construction is a well known algorithm for constructing bipartite graphs with good girth properties. In this letter, we propose some improvements in the PEG algorithm which greatly improve the girth properties of the resulting graphs: given a graph size, they increase the girth g achievable by the algorithm, and when the girth cannot be increased, our modified algorithm minimizes the number of cycles of length g. As a main illustration, we focus on regular column-weight two graphs (dv = 2), although our algorithm can be applied to any graph connectivity. The class of dv = 2 graphs is often used for non-binary low density parity check codes that can be seen as monopartite graphs: for a given target girth gt , this new instance of the PEG algorithm allows to construct cages, i.e. graphs with the minimal size such that a graph of girth gt exists, which is the best result one might hope for.
%X This paper presents a new approach to decode turbo codes using a nonbinary belief propagation decoder. The proposed approach can be decomposed into two main steps. First, a nonbinary Tanner graph representation of the turbo code is derived by clustering the binary parity-check matrix of the turbo code. Then, a group belief propagation decoder runs several iterations on the obtained nonbinary Tanner graph. We show in particular that it is necessary to add a preprocessing step on the parity-check matrix of the turbo code in order to ensure good topological properties of the Tanner graph and then good iterative decoding performance. Finally, by capitalizing on the diversity which comes from the existence of distinct efficient preprocessings, we propose a new decoding strategy, called decoder diversity, that intends to take benefits from the diversity through collaborative decoding schemes.
%X In this paper, a method to design regular (2, dc)- LDPC codes over GF(q) with both good waterfall and error floor properties is presented, based on the algebraic properties of their binary image. First, the algebraic properties of rows of the parity check matrix H associated with a code are characterized and optimized to improve the waterfall. Then the algebraic properties of cycles and stopping sets associated with the underlying Tanner graph are studied and linked to the global binary minimum distance of the code. Finally, simulations are presented to illustrate the excellent performance of the designed codes.
%X We are interested in the analysis and optimization of Raptor codes under a joint decoding framework, that is, when the precode and the fountain code exchange soft information iteratively. We develop an analytical asymptotic convergence analysis of the joint decoder, derive an optimization method for the design of efficient output degree distributions, and show that the new optimized distributions outperform the existing ones, both at long and moderate lengths. We also show that jointly decoded Raptor codes are robust to channel variation: they perform reasonably well over a wide range of channel capacities. This robustness property was already known for the erasure channel but not for the Gaussian channel. Finally, we discuss some finite length code design issues. Contrary to what is commonly believed, we show by simulations that using a relatively low rate for the precode (Rp≃0.9), we can improve greatly the error floor performance of the Raptor code.
%X In this paper, we propose a new implementation of the Extended Min-Sum (EMS) decoder for non-binary LDPC codes. A particularity of the new algorithm is that it takes into accounts the memory problem of the non-binary LDPC decoders, together with a significant complexity reduction per decoding iteration. The key feature of our decoder is to truncate the vector messages of the decoder to a limited number n m of values in order to reduce the memory requirements. Using the truncated messages, we propose an efficient implementation of the EMS decoder which reduces the order of complexity to O(n m log2 n m ). This complexity starts to be reasonable enough to compete with binary decoders. The performance of the low complexity algorithm with proper compensation is quite good with respect to the important complexity reduction, which is shown both with a simulated density evolution approach and actual simulations.
%X First unequal error protection (UEP) proposals date back to the 1960's (Masnick and Wolf; 1967), but now with the introduction of scalable video, UEP develops to a key concept for the transport of multimedia data. The paper presents an overview of some new approaches realizing UEP properties in physical transport, especially multicarrier modulation, or with LDPC and Turbo codes. For multicarrier modulation, UEP bit-loading together with hierarchical modulation is described allowing for an arbitrary number of classes, arbitrary SNR margins between the classes, and arbitrary number of bits per class. In Turbo coding, pruning, as a counterpart of puncturing is presented for flexible bit-rate adaptations, including tables with optimized pruning patterns. Bit- and/or check-irregular LDPC codes may be designed to provide UEP to its code bits. However, irregular degree distributions alone do not ensure UEP, and other necessary properties of the parity-check matrix for providing UEP are also pointed out. Pruning is also the means for constructing variable-rate LDPC codes for UEP, especially controlling the check-node profile.
%X In this paper, we discuss the rate splitting issue for the design of finite length Raptor codes, in a joint decoding framework. We show that the choice of a rate lower than usually proposed for the precede enables to design Raptor codes that perform well at small lengths, with almost no asymptotic loss. We show in particular that the error floor can be greatly reduced by properly choosing the rate splitting between the precede and the LT code. Those behaviors are demonstrated both on the BEC and the BIAWGN channel.
%X We are interested in knowing how a robot head can learn to recognize facial expressions without su- pervision. Our starting point is a mathematical model showing that a sensory-motor architecture is able to express its emotions succeedes to recognize on-line the facial expression of a caregiver if this latter naturally tends to imitate or to resonate with the system.
%X We are interested in knowing how a robot head can learn to recognize facial expressions without supervision. Our starting point is a mathematical model showing that a sensory-motor architecture is able to express its emotions succeedes to recognize on-line the facial expression of a caregiver if this latter naturally tends to imitate or to resonate with the system. Interestingly, our works also show that, learning autonomously to recognize a face/non face is more complex than to recognize a facial expression. We propose an architecture using the interaction rhythm to allow first a robust learning of the facial expression without a face tracking and next to perform the learning of the face/non face recognition. Finally we emphasize the importance of the emotions as a mechanism to ensure the dynamical coupling between individuals allowing to learn more and more complex tasks.
%X The aim of this study is to show how robots learning could be easier and accessible to non experts if it relies on emotional interactions, more precisely on social referencing abilities, rather than on specialized supervised learning technics. To test this idea, we coupled two systems : a robotic head able to learn to recognize and imitate emotional facial expressions and a mobile robot able to learn autonomous visual navigation tasks in a real environment. Two possible solutions for coupling these two systems are tested. First, the emotional interactions are used to qualify the robot's behavior. The robot shows its ability to learn how to reach a goal-place of its environment using emotional interaction signal from the experimentator. These signals are giving the robot information about the quality of its behavior and allow it to learn place-actions associations to construct an attraction basin around the goal-place. Second, the emotional interactions are used to qualify the robot's immediat environment. The robot shows its ability to learn how to avoid a place of its environment by associating it with the experimentator's anger facial expression. The first strategy allows the experimentator to teach the robot to reach a specific place from anywhere in its environment. However, this strategy takes more learning time than the second strategy that is very fast but seems to be inappropriate to learn to reach a place instead of avoiding it. While these two different strategies achieve satisfactory results, there is no reason why they should be mutually exclusive. In conclusion, we discuss the coupling of both type of learning. Our results also show that relying on the natural expertise of humans in recognizing and expressing emotions is a very promising approach to human-robot interactions. Furthermore, our approach can provide new interesting insights about how, in their early age, humans can develop high level social referencing capabilities from low level sensorimotors dynamics.
%X In this work, we are interesting in understanding how emotional interactions with a social partner can bootstrap increasingly complex behaviors, which is important both for robotics application and un- derstanding development. In particular, we pro- pose that social referencing, gathering information through emotional interaction, fulfills this goal. So- cial referencing, a developmental process incorporat- ing the ability to recognize, understand, respond to and alter behavior in response to the emotional ex- pressions of a social partner, allows an infant, or a robot, to seek information from another individual and use that information to guide his behavior to- ward an object or event
%X Mimicry and deferred imitation have often been considered as separate kinds of imitation. In this paper, we present a simple architecture for robotic arm control which can be used for both. The model is based on some dynamical equations, which provide a motor control with exploring and converging capacities. A visuo-motor map is used to associate positions of the end effector in the visual space with proprioceptive position of the robotic arm. It enables a fast learning of the visuo-motor associations without needing to embed a priori information. The controller can be used both for accurate control and interaction. It has been implemented on a minimal robotic setup showing some interesting emergent properties. The robot can reproduce simple gestures in mimicry situation and finalized actions in deferred imitation situation. Moreover, it can even show some “intention” recognition abilities. Finally, the experiment of deferred imitation which inherits from learning by demonstration, also provides a good basis for cooperative and interactive experiments.
%X Since Piaget, it is well accepded that higher level cognitive functions are settled on low level sensori-motor associations. In this paper, we will show that different kinds of interactive behaviors can emerge according to the kind of proprioceptive function available in a given sensori-motor system. We will study two different examples. In the first one, an internal proprioceptive signal is avaible for the learning of the visuo-motor coordination between an arm and a camera. An imitation behavior can emerge when the robot 's eye focuses on the hand of the experimenter instead of its own hand. The imitative behavior results from the error minimization between the visual signal and the proprioceptive signal. Here, the imitation results from the perception ambiguity: the robot mistakes its hand with the experimenter hand! In the second example, a robot head has to recognize the facial expression of the human caregiver. Yet, the robot has no visual feedback of its own facial expression. The human expressive resonance will allows the robot to select the visual features relevant for a particual facial expression. As a result, after few minutes of interactions, the robot can imitates the facial expression of the human partner. We will show the different proprioceptive signals used in both examples can be seen as bootstrap mechanisms for more complex interactions. Applied as a crude model of the human, we will propose that these mechanisms play an important role in the process of individuation.
%X In this work, we are interested in understanding how emo- tional interactions with a social partner can bootstrap increasingly com- plex behaviors such as social referencing. Our idea is that social refer- encing as well as facial expression recognition can emerge from a simple sensori-motor system involving emotional stimuli. Without knowing that the other is an agent, the robot is able to learn some complex tasks if the human partner has some “empathy” or at least “resonate” with the robot head (low level emotional resonance). Hence we advocate the idea that social referencing can be bootstrapped from a simple sensori-motor system not dedicated to social interactions.
%X The proposed work raises the problem of autonomous learnings in robotics. Whatever the morphology of the robots and the various skills they could acquire according to their morphology, robots need to be able to self-evaluate, not only in order to guide their autonomous development trough the vast sensory-motor space, but also in order to verify that previous learning are still pertinent and to readapt their knowledge when previous learnings becomes erroneous. Such capabilities appear crucial in developmental robotics and should largely enrich the possible manifolds of social and physical interactions in which robots could be involved. As a support to this assumption, the presentation will focus on a mature bio-inspired neural network architecture, which uses an autonomous, online, and interactive learning, allowing a robot to achieve sensory-motor tasks and planning, in the frame of navigation. The role of self-evaluation for interactive learnings and self-development is highlighted
%X Since Piaget, it is well accepded that higher level cognitive functions are settled on low level sensori-motor associations. In this paper, we will show that different kinds of interactive behaviors can emerge according to the kind of proprioceptive function available in a given sensori-motor system. We will study two different examples. In the first one, an internal proprioceptive signal is avaible for the learning of the visuo-motor coordination between an arm and a camera. An imitation behavior can emerge when the robot 's eye focuses on the hand of the experimenter instead of its own hand. The imitative behavior results from the error minimization between the visual signal and the proprioceptive signal. Here, the imitation results from the perception ambiguity: the robot mistakes its hand with the experimenter hand! In the second example, a robot head has to recognize the facial expression of the human caregiver. Yet, the robot has no visual feedback of its own facial expression. The human expressive resonance will allows the robot to select the visual features relevant for a particual facial expression. As a result, after few minutes of interactions, the robot can imitates the facial expression of the human partner. We will show the different proprioceptive signals used in both examples can be seen as bootstrap mechanisms for more complex interactions. Applied as a crude model of the human, we will propose that these mechanisms play an important role in the process of individuation.
%X We are interested in understanding how babies learn to recognize facial expressions without having a teaching signal allowing to associate a facial expression to a given abstract label (i.e the name of the facial expression 'sadness', 'happiness'...). Our starting point was a mathematical model showing that if the baby uses a sensory motor architecture for the recognition of the facial expression then the parents must imitate the baby facial expression to allow the on-line learning. In this paper, a first series of robotics experiments showing that a simple neural network model can control the robot head and learn on-line to recognize the facial expressions (the human partner imitates the robot prototypical facial expressions) is presented. We emphasize the importance of the emotions as a mechanism to ensure the dynamical coupling between individuals allowing to learn more complex tasks.
%X A wide variety of visual recognition systems are developed for precise tasks and types of objects. In this paper we would like to emphasize ways to build a more generic recognition system. Perception is one of these mechanisms that psychologists particularly pointed out as a fundamental one for actively organizing and making sense of input sensory information. Based on psychological assumptions, we propose to explore the concept of perception, infer formalization in the dynamical system framework and quantitatively analyze it on robotic platforms using a unique simple neuronal architecture based on the association of visual and motor information (movements of the body or part of the body). This coupling of sensory flows of information can be characterized by a sensorimotor invariant, a dynamical attractor that we identify as a perception function. For place, object or facial expression recognition, we show how simple sensori-motor architecture can be applied to accomplish each task in terms of behavioral recognition. In each application, some pertinent visual information, based on classical focus point detection, are organized as local views and associated to an action or an internal state corresponding to a set of actions, in order to reach a location, an object or recognize a facial expression. The active learning phase for different points of view or face expressions allows the emergence of a stable perception linked to a stable sensori-motor attractor and allows the robot to perform a stable behavior in very different initial conditions. We will show how the attractor/perception emerges during the learning phase and evaluate its spatial generalization properties.
%X We study incremental redundancy hybrid ARQ (IR-HARQ) schemes based on punctured, finite-length, LDPC codes. The transmission is assumed to take place over time varying binary erasure channels, such as mobile wireless channels at the applications layer. We analyze and optimize the throughput and delay performance of these IR-HARQ protocols under iterative, message-passing decoding. We derive bounds on the performance that are achievable by such schemes, and show that, with a simple extension, the iteratively decoded, punctured LDPC code based IR-HARQ protocol can be made rateless, and operating close to the general theoretical optimum for a wide range of channel erasure rates.
%X This paper provides a performance analysis of the regular (c, d) LDPC code ensemble of codelength n with parity-check matrices defined over the general linear group GL(2m). The transmission is assumed to take place over the binary erasure channel with erasure probability ε. In this work, the scaling approximation of the block erasure rate is generalized to the non-binary case, and the scaling parameter α of the approximation is derived. The proposed estimation is then compared with numerical results, showing that it predicts well the slope of the block erasure rate vs. channel erasure probability.
%X This paper is the first part of an investigation if the capacity of a binary-input memoryless symmetric channel under ML decoding can be achieved asymptotically by using non-binary LDPC codes. We consider (l, r)-regular LDPC codes both over finite fields and over the general linear group and compute their asymptotic binary weight distributions in the limit of large blocklength and of large alphabet size. A surprising fact, the average binary weight distributions that we obtain do not tend to the binomial one for values of normalized binary weights Â¿ smaller than 1-2-l/r. However, it does not mean that non-binary codes do not achieve the capacity asymptotically, but rather that there exists some exponentially small fraction of codes in the ensemble, which contains an exponentially large number of codewords of poor weight. The justification of this fact is beyond the scope of this paper and will be given in.
%X In this paper, we propose and study a new family of error-correcting codes. These achieve excellent error performance under an iterative decoding over the binary-input noisy channel and solves the memory space requirements problem of the non-binary LDPC decoders. We named this class of codes, Split non-binary LDPC codes. The main particularity of this new family of codes is that the variable and the check nodes are not defined over the same finite field GF(2p), like in the case of classical non-binary LDPC codes. The class of Split non-binary LDPC codes is obviously larger than that of existing types of codes, which gives more degrees of freedom to find good codes when the existing codes show their limits. We provide two examples of interesting split NB-LDPC codes.
%X In this paper we study the problem of mining all frequent queries in a relational table, a problem known to be intractable even for conjunctive queries. We restrict our attention to projection-selection queries and we assume that the table to be mined satis- es a set of functional dependencies. Under these assumptions we dene two preorderings with respect to which the support measure is shown to be anti-monotonic. Moreover, each of these pre-orderings induces an equivalence relation for which all queries of the same equivalence class have the same support. The goal of this paper is not to provide algorithms for the computation of frequent queries, but rather to characterize the preorderings and their associated equivalence relations. Basic computational implications of these characterizations are discussed in the paper, based on our previous work.
%X Distributed multimedia applications have emerged at an increasing rate during the last decade in several domains (video conferencing, e-health, virtual meeting rooms, etc). This has created several new challenging problems related to the data integration and fragmentation, user-oriented and adaptive interfaces, real time and network performances, etc. In this paper, we focus on the problem of data(base) fragmentation in a multimedia context. We recall in this respect that data fragmentation consists of reducing irrelevant data accesses by grouping data frequently accessed together in dedicated segments. We mainly address the issue of query and predicate implication required in current fragmentation algorithms, and provide a formal approach to identify such implications, in order to partition multimedia data efficiently. Our approach is capable of considering multimedia-based as well as semantic comparisons, both ignored in current studies but required when multimedia data come to play.
%X In this paper, we address the issue of mining gradual classification rules. In general, gradual patterns refer to regularities such as "The older a person, the higher his salary". Such patterns are extensively and successfully used in command-based systems, especially in fuzzy command applications. However, in such applications, gradual patterns are supposed to be known and/or provided by an expert, which is not always realistic in practice. In this work, we aim at mining from a given training dataset such gradual patterns for the generation of gradual classification rules. Gradual classification rules thus refer to rules where the antecedent is a gradual pattern and the conclusion is a class value.
%X The Radon transform (RT) on straight lines deals as mathematical foundation for many imaging systems (e.g. X-ray scanner, Positron Emission Tomography) op- erating only with non-scattered (primary) radiation. Using Compton scattered radiation has turned out to be an attractive alternative to conventional emission imaging. In this paper, we propose a new two-dimensional emission imaging from Compton scattered gamma-rays. Its modeling leads to a Radon transform deﬁned on a pair of half-lines forming a vertical letter V (TV). Moreover we establish the analytic inverse formula of this new TV, which forms the mathematical basis for image reconstruction. Through simulations, image formation and reconstruction results show the feasibility and the relevance of this new imaging. The main advantage is to use a one-dimensional non-moving detector for two-dimensional image reconstruction.
%X A new Radon transform deﬁned on a discontinuous curve formed by a pair of half-lines forming a letter V is deﬁned and studied. We establish its analytic inverse formula, its related ﬁltered back-projection reconstruction procedure and its numerical analysis. These theoretical results allow the reconstruction of two-dimensional images of a radiating object from its Compton scattered rays measured on a one-dimensional collimated camera. Numerical simulations results illustrate the performance of the new imaging process.
%X Conventional tomography (X-ray scanner, Computed Tomography : CT, Single Photon Emission CT : SPECT,...) is widely used in numerous fields such as medical imaging and non-destructive testing. In theses tomographies, a detector rotates in space to collect primary radiation emitted by an object under investigation. In this case Compton scattered radiation behaves as noise hindering image quality and consequently correction to scatter should be applied. However recently an interesting new imaging concept, which uses precisely scattered radiation as imaging agent, has been advocated. The camera records now images labeled by scattered photon energy or equivalently scattering angle. Then it is shown that the three dimensional image reconstruction from scattered radiation data is feasible [1, 2, 3, 4, 5]. In this work we propose a new form of Compton scattering tomography (CST), akin to the X-ray scanning tomography, in the sense that it works in transmission but uses Compton scattered radiation. The new image formation modeling is based on a new class of Radon transforms on circular arcs. Through simulation results we show the feasibility and the relevance of this new process.
%X Until recently the known invertible classes of Radon transforms on circles of R2 are those defined on circles with their centers on a line or on another circle and those defined on circles that go through a fixed point. In this work we discuss a new class of Radon transforms which are defined on a family of circles which have a common chord of fixed length rotating around its middle point. An analytic inverse formula is derived leading the way to the reconstruction of L1(R2)-functions with compact support. We implement this inversion using a numerical approach to illustrate function reconstruction. This transformhas also its application in themodeling of a newmodality in Compton scatter Tomography, which is in particular relevant for medical imaging and non-destructive evaluation.
%X Human vision rely on attention to select only a few regions to process and thus reduce the complexity and the processing time of visual task. Artificial vision systems can benefit from a bio-inspired attentional process relying on neural models. In such applications, what is the most efficient neural model: spiked-based or frequency-based? We propose an evaluation of both neural model, in term of complexity and quality of results (on artificial and natural images).
%X The report proposed an interpretation for the mechanism of noise-enhanced image restoration with nonlinear PDE (Partial Differential Equation) recently demonstrated in literature. A link is established between the action of noise in a nonlinear Perona-Malik anisotropic diffusion and stochastic resonance in memoryless nonlinear systems for 1-D signals.
%X The progressive edge-growth (PEG) construction is a well known algorithm for constructing bipartite graphs with good girth properties. In this paper, we propose some improvements in the PEG algorithm which greatly improve the girth properties of the resulting graphs: given a graph size, they increase the girth g achievable by the algorithm, and when the girth cannot be increased, our modified algorithm minimizes the number of cycles of length g. First, we consider regular column-weight two graphs, which are a class of graphs used for the design of non-binary LDPC codes: for a given target girth gt, this new instance of the PEG algorithm allows to construct graphs with the minimal size such that a graph of girth gt exists, which is the best result one might hope for. We illustrate the interest of such minimal constructions with simulation results. Then, we illustrate the usefulness of our algorithm for constructing regular binary LDPC codes that perform well in the error floor region.
%X Starting from neurobiological hypotheses on the existence of place cells (PC) in the brain, the aim of this article is to show how little assumptions at both individual and social levels can lead to the emergence of non-trivial global behaviors in a multiagent system (MAS). In particular, we show that adding a simple, hebbian learning mechanism on a cognitive map allows autonomous, situated agents to adapt themselves in a dynamically changing environment, and that even using simple agentfollowing strategies (driven either by similarities in the agent movement, or by individual marks - “signatures” - in agents) can dramatically improve the global performance of the MAS, in terms of survival rate of the agents. Moreover, we show that analogies can be made between such a MAS and the emergence of certain social behaviors
%X Mathematical programming approaches, like linear programming (LP) or mix-integer programming (MIP) are widely used as an optimization tool, since they can compute the best possible configuration of a constrained system. However, the needed computing resources and time cannot always be figured out in advance, and happen to be beyond available resources in some unexpected cases. Moreover, these approaches do not naturally fit with dynamically changing configurations. As an alternative, Multi-agent systems (MAS) prove to be more adaptive and predictable in terms of needed resources, but of course do not guarantee to provide the best solution, not even always a “good” solution, as far as one can define what a good solution is. In this paper we try to compare the two approaches on a simple optimization (warehouse location) problem.
%X In this letter, the estimation of the IQ imbalance in the presence of a Carrier Frequency Offset (CFO) is addressed. The proposed algorithm has a low complexity, can be used for any Orthogonal Frequency Division Multiplexing (OFDM) system and needs a single pilot symbol. Through numerical simulation and in the context of quick estimations of front-end non-idealities, our algorithm is shown to outperform conventional approaches for sensible values of IQ imbalance and CFO.
%X In data packet communication systems over multipath frequency-selective channels, hybrid automatic repeat request (HARQ) protocols are usually used in order to ensure data reliability. For single-carrier packet transmission in slow fading environment, an identical retransmission of the same packet, due to a decoding failure, does not fully exploit the available time diversity in retransmission-based HARQ protocols. In this paper, we compare two transmit diversity techniques, namely, cyclic frequency- shift diversity and bit-interleaving diversity. Both techniques can be integrated in the HARQ scheme in order to improve the performance of the joint detector. Their performance in terms of pairwise error probability is investigated using maximum likelihood detection and decoding. The impact of the channel memory and the modulation order on the performance gain is emphasized. In practice, we use low complexity linear filter-based equalization which can be efficiently implemented in the frequency domain. The use of iterative equalization and decoding is also considered. The performance gain in terms of frame error rate and data throughput is evaluated by numerical simulations.
%X We address the problem of resource allocation on the downlink of an OFDMA single-cell system under fairness constraints with limited channel state information (CSI). Target QoS corresponds to a minimum user data rate, a target bit-error rate and a maximum BER-outage probability. The channel model includes path-loss, shadowing, and fading. The only available CSI is the channel average gain of each user. This partial CSI defines a shadowed path-loss that yields a modified user distribution. Resource allocation is based on the shadowed user distribution that we characterize analytically. Thus, under the target QoS, we provide the optimal resource allocation that maximizes the user rate. Compared to full-CSI-based allocation schemes, our solution offers a significant complexity and feedback reduction. Finally, the performance of our method is compared to other existing methods and the robustness of its outage performance to CSI errors is shown.
%X In the past few years, street-level geoviewers has become a very popular web-application. In this paper, we focus on a first urban concept which has been identified as useful for indexing then retrieving a building or a location in a city: the windows. The work can be divided into three successive processes: first, object detection, then object characterization, finally similarity function design (kernel design). Contours seem intuitively relevant to hold architecture information from building facades. We first provide a robust window detector for our unconstrained data, present some results and compare our method with the reference one. Then, we represent objects by fragments of contours and a relational graph on these contour fragments. We design a kernel similarity function for structured sets of contours which will take into account the variations of contour orientation inside the structure set as well as spatial proximity. One difficulty to evaluate the relevance of our approach is that there is no reference database available. We made, thus, our own dataset. The results are quite encouraging regarding what was expected and what provide methods the literature.
%X When applied to practical problems, tuning parameters of optimization methods can be a critical problem for engineer users. In this article we suggest a novel approach to consider constrained optimization problems. Its principal caracteristics are flexibility, adaptativity and user friendship. Indeed constraints are totally separated from the cost function. It allows to be directly coupled with any local search algorithm in order to explore the solution space which is reduced in an adaptive manner. Its implementation is compared to other existing methods with discrete and continuous classical problems. Results show that the method approaches the best results thus offering a good trade-off between flexibity and quality for the solutions.
%X This article presents an optimization technic of static real-time scheduling. The goal is to minimize the size in embedded memory of the scheduling tables defined at compile-time. This compression of the results of classical scheduling algorithms exploits Idle times in multiprocessors systems in order to identify cyclic patterns. The proposed approach is based on a compact description format near to the Synchronous Data Flow Graphs (SDFG). When applied to our case studies, the average compression rate of our technic is near to 70% of the initial schedule size.
%X The development of street-level geoviewers become recently a very active and challenging research topic. In this context, the detection, representation and classification of windows can be beneficial for the identification of the respective facade. In this paper, a novel method for windows and facade retrieval is presented. This method, based on a similarity of graph of contours, introduces a new kernel on graph for inexact graph matching. We design a kernel similarity function for structured sets of contours which will take into account the variations of contour orientation inside a structure set, as well as spatial proximity. Then we are able to extract a window as a sub-graph of the graph of all contours of the facade image and to retrieve similar windows from a database of images of facades.
%X The growth of personal image collections has boosted the creation of many applications, many of which depend on the existence of fast schemes to match similar image descriptors. In this paper we present multicurves, a new indexing method for multimedia descriptors, able to handle high dimensionalities (100 dimensions and over) and large databases (millions of descriptors). The technique allows a fast implementation of approximate kNN search, and deals easily with data updating (insertions and deletions). The index is based on the simultaneous use of several moderate-dimensional space-filling curves. The combined effect of having more than one curve, and reducing the dimensionality of each individual curve allows to overcome undesirable boundary effects. In empirical evaluations, the method compares favorably with state-of-the-art methods, especially when the constraints of secondary storage are considered.
%X Browsing and finding pictures in large-scale and heterogeneous collections is an important issue, most particularly for online photo sharing applications. Since such services know a huge growing of their database, the tag-based indexing strategy and the results displayed in a traditional “in a single file” representation are not efficient to browse and query image collections. Naturally, data clustering appeared as a good solution by presenting a summarized view of an image set instead of an exhaustive but useless list of its element. We present a new method for image clustering based on a shared nearest neighbors approach that could be processed on both content-based features and textual descriptions (tags). We describe, discuss and evaluate the SNN method for image clustering and present some experimental results using the Flickr collections showing that our approach provides useful representations of an image set.
%X The aim of this study is to show how robots learning could be easier and accessible to non experts if it relies on emotional interactions, more precis ely on social referencing abilities, rather than on specialized supervised learning technics. To test this idea, we coupled two systems : a robotic head able to learn to recognize and imitate emotional facial expressions and a mobile robot able to learn autonomous visual navigation tasks in a real environment. Two possible solutions for coupling these two systems are tested. First, the emotional interactions are used to qualify the robot's behavior. The robot shows its ability to learn how to reach a goal-place of its environment using emotional interaction signal from the experimentator. These signals are giving the robot information about the quality of its behavior and allow it to learn place-actions associations to construct an attraction basin around the goal-place. Second, the emotional interactions are used to qualify the robot's immediat environment. The robot shows its ability to learn how to avoid a place of its environment by associating it with the experimentator's anger facial expression. The first strategy allows the experimentator to teach the robot t o reach a specific place from anywhere in its environment. However, this strategy takes more learning time than the second strategy that is very fast but seems to be inappropriate to learn to reach a pla ce instead of avoiding it. While these two different strategies achieve satisfactory results, there is no reason why they should be mutually exclusive. In conclusion, we discuss the coupling of both type of learning. Our results also show that relying on the natural expertise of humans in recognizi ng and expressing emotions is a very promising approach to human-robot interactions. Furthermore, our approach can provide new interesting insights about how, in their early age, humans can develop high level social referencing capabilities from low level sensorimotors dynamics.
%X Biologically inspired models for navigation use mechanisms like path integration or sensori-motor learning. This paper describes the use of a proprioceptive working memory to give path integration the potential to store several goals. Then we coupled the path integration working memory to place cell sensori-motor learning to test the potential autonomy this gives to the robot. This navigation architecture intends to combine the benefits of both strategies in order to overcome their drawbacks. The robot use a low level motivational system. Experimental evaluation is done with a robot in a real environment.
%X Inspired by the emotional conditionings performed by the amygdala, we describe a simulated neural network able to learn the meaning of a previously neutral stimulation. A robot using this neural network can learn the conditioning of a non specific sensor activated by the experimentator and its internal state of pain or pleasure. This biologically inspired adaptative and natural way to interact with the robot is tested with a mobile robot learning navigation tasks in a real environment.
%X This paper explores the use of a mechanism to auto-regulate the robot behavior in situations of persistent failures. In order to give more autonomy to a mobile robot, a generical frustration mechanism based on the automonitoring of the progress (in terms of goal distance reduction) is studied in different situations and on different parts of the robot architecture. To escape failure situations and deadlocks, the frustration reaction can inhibit the robot navigation strategies, goals or drives.
%X This paper presents a framework for the indexing and retrieval of artwork 3D models, allowing global and partial model classification and retrieval. The first part of the paper deals with database classification based on global shape descriptors. A search engine "RETIN-3D", using a SVM classifier coupled with an active learning strategy allows to retrieve categories of similar objects. In a second part, the classification is improved thanks to a local description of the models. A new framework for 3D surface segmentation is proposed. Shape descriptors are adapted to surface regions and kernels on descriptor bags are used to perform the database classification. Our system is designed for classifying and retrieving in ancient artwork 3D databases, and results from this application domain are presented and commented along the paper.
%X This paper describes experimental results regarding the real time implementation of continuous time recurrent neural networks (CTRNN) and the dynamic back-propagation through time (BPTT) algorithm for the on-line learning control laws. Experiments are carried out to control the balance of a biped robot prototype in its standing posture. The neural controller is trained to compensate for external perturbations by controlling the torso's joint motions. Algorithms are embedded in the real time electronic unit of the robot. On-line learning implementations are presented in detail. The results on learning behavior and control performance demonstrate the strength and the efficiency of the proposed approach.
%X We introduce the use of fast flat histogram (FFH) method employing Wang Landau algorithm in an adaptive noise sampling framework using random walk to find out the pseudo-codewords and consequently the pseudo-weights for the belief propagation (BP) decoding of LDPC codes over an additive white Gaussian noise (AWGN) channel. The FFH method enables us to tease out pseudo-codewords at very high signal-to-noise ratios (SNRs) exploring the error floor region of a wide range of codes varying in length and structure. We present the pseudo-weight (effective distance) spectra for these codes and analyze their respective behavior.
%X In this paper, we deal with the time slot allocation in time-hopping ultra-wide band (TH-UWB) ad-hoc networks in order to maximize the total sum rate. This problem is known to be a discrete NP-hard optimization problem and hence cannot be solved with a computationally efficient algorithm. Moreover, the sum rate maximization often leads to starvation of links experiencing bad channel conditions. Our main contribution is to propose an algorithm which approaches the maximum sum rate while keeping the starvation rate under a threshold. We first propose a time slot allocation scheme based on an accept/reject criterion. The performance of the algorithm depends on the acceptance threshold. In a second approach, we add the starvation rate in constraints leading to a new algorithm maximizing the sum rate and keeping the starvation rate under control. Extensive simulation results show that the proposed algorithm achieves reasonable sum rate performances under restrictive starvation threshold.
%X An autonomous robot finding its way on an unknown terrain, a video decoder changing decompression format according to signal strength, a broadband electronic counter-measure system, an adaptive image tracking algorithm for automotive ... Many emerging embedded or mission-critical applications show dynamic behavior, with strong dependency on the unpredictable environment. Where statically resolved worst-case allocation was an answer to strong real time constraints, flexibility is now a requirement. At the same time, the needs for computing power never slow down, both in terms of reactivity and data throughput, while the pressure on power consumption is stronger than ever.
%X In this paper, a 10th Order Chebyshev Gm-C active filter in a standard 0.35 µm SiGe BiCMOS process is presented. The filter has a simulated bandwidth of 75 MHz in the 1800 MHz frequency domain, 50 dB rejection in the stop band, a 6V power supply, and it is suitable for use in GSM 1800 applications. The calculations included in this paper can be considered a methodology for designing a real, implementable bandpass filter in any technology.
%X The paper presents an improved tuning method implemented for a differential active inductor based RF bandpass filter. Derived from a previous designed transistor-only second order filter topology, independent frequency and quality factor tuning are demonstrated in 0.18 μm CMOS technology. The circuit has been basically designed for a maximum frequency of 2.4 GHz, high enough to cover the main wireless standards, has small power consumption and can exhibit very high Q values. A wide frequency tuning range is obtained by using MOS varactors. The circuit absorbs 1 mW from a 1.8 V supply at 2.4 GHz central frequency.
%X Dans la perspective d'étudier les architectures des réseaux sur puce optique et optimiser leurs performances dans le cadre des interconnexions en optique guidée, nous allons étudier les performances des commutateurs optiques (microrésonateurs en anneau) et proposer ainsi une nouvelle configuration d'un réseau sur puce optique (ONoC) en développant un algorithme qui calcule le plus court chemin et les pertes de champ au niveau de chaque routeur.
%X This paper describes a 2.4 GHz single-ended switched gain low noise amplifier (SG-LNA) in a 0.35 mum SiGe BiCMOS process. In the design, specific architecture decisions were made in consideration of system-on-chip implementation. The architecture profits from a two cascode stage topology with a shunt resistive feedback in the first cascade-topology stage. The SG-LNA achieved a maximum small signal gain of 34.3 dB within input 1-dB compression point (ICP1dB) of -22 dBm in high-gain mode (HGM), a gain of 25.4 dB within ICP1dB of -13.8 dBm in medium-gain mode (MGM) , and a minimum gain of 18.3 dB within ICP1dB of -6.8 dBm in low-gain mode (LGM). The noise figures (NF) are 2.9 dB, 5.5 dB and 5.9 dB in HGM, MGM and LGM, respectively. Because of using a Common-Gate topology as an active input matching, the SG-LNA presented a good input and output return losses in all modes. All biases applied are active. The SG-LNA consumes a maximum DC current of 42 mA from a 3.3 volt DC supply.
%X A 2.4GHz front-end system design for wide spectrum WLAN applications is presented in a 0.35 μm SiGe BiCMOS Technology. This transceiver front-end contains a receive (Rx) chain with a two-stage cascode low noise amplifier (LNA) and an active down-conversion Rx mixer, and a transmit (Tx) chain composed of a Gilbert-Cell core up-conversion Tx mixer and a high-gain Driver Amplifier (DA). The high linear LNA shows a gain of 15.5 dB, an noise figure (NF) of 2.28 dB and an input- referred third-order intercept point (IP3) of +2.4 dBm with 1-dB gain bandwidth (BW) of 1.5 GHz. The single-balanced Rx mixer exhibited a gain and 1-dB gain BW of +6.8 dB and 1.5 GHz. Also a double-balanced Tx mixer with a gain and input/output return loss of -1.3dB and below -35dB, respectively, and a DA with a gain and output-referred IP3 of +29.2dB and +21.2dBm, respectively, is developed. The NF, input-referred IP3 and DC power consumption of Rx string (from antenna to Rx mixer) were achieved 4.4 dB, -15.4 dBm and 30 mW respectively. The output-referred IP3 and power consumption of Tx chain were +20.5 dBm and 125 mW, respectively.
%X This paper describes a monolithic tunable active time delay with variable gain, designed using monolithic-microwave integrated-circuit (MMIC) technology, targeting in 3.5-4.5 GHz range with a low power consumption of 9.4 mW. It is impossible to realize such great delays with ideal transmission line in MMIC, because of the length required for the line. In this paper We present analytical and computer-simulated results using 0.35 mum SiGe BiCMOS process for a tunable active time delay in 3.5-4.5 GHz range. It works as a pure time delay over 800 MHz frequency band.
%X Fully differential voltage-controlled oscillator (VCO) design with a low phase noise and extra linear VCO gain (Kvco) for 5-GHz wireless applications in 0.35-mum SiGe (Silicon Germanium) BiCMOS technology is discussed in this paper. The phase noise level is -120.976 dBc/Hz at 1 MHz offset at an oscillation frequency of 5.4 GHz. The Kvco changes from 214 MHz/V to 271 MHz/V. The tunability of the structure covers 786 MHz, from 4.642 GHz up to 5.428 GHz. Considering a current consumption of 3.13 mA, with 3.0 V voltage supply.
%X This paper describes a low voltage low noise amplifier (LNA), designed using 0.35Â¿m SiGe BiCMOS process, targeting a center frequency of 5.8GHz with a voltage supply 1.2V. A power gain of 12.1dB at 5.8GHz has been achieved with a low power consumption of 3.8mW, including all biasing circuitry. The overall noise figure of the LNA is 3dB with both input and output impedance matched to 50Â¿.
%X A new method for improving the frequency response of an all transistor simulated CMOS inductor bandpass filter is proposed. It is shown that a significant increase of the central frequency up to 800 MHz or even more can be obtained by introducing a supplementary resistor connected to the gate of one transistor. The method makes also use of negative resistances to compensate the inductor losses. Small signal models and limitations of the method are discussed. High quality factors are obtained without stability problems or extra power consumption. The simulations prove that the frequency enhancement depends on the particular configuration of the active inductor.
%X The paper presents an improved tuning method implemented for a differential active inductor based RF bandpass filter. Derived from a previous designed transistor-only second order filter topology, independent frequency and quality factor tuning are demonstrated in 0.18 mum CMOS technology. The circuit has been basically designed for 2.4 GHz, a frequency high enough to cover the main wireless standards, has small power consumption and can exhibit very high Q values. A wide frequency tuning range is obtained by using on.chip varactors. The circuit absorbs 1 mW from a 1.8 V supply at 2.4 GHz central frequency.
%X This paper presents an improved low power CMOS active inductor topology suitable for RF filtering. The circuit is derived from a previous designed transistor-only active inductor by adding a floating voltage source to one transistor gate fact that positively changes the overall dc biasing. If a current source with high output resistance is used for active inductor, this method can lead to lower interdependence between the self resonant frequency and quality factor. The simulations were carried out in 0.18 mum CMOS technology.
%X We propose the design of a transceiver directly converting an OOK optical signal into a microwave carrier that is either FSK or PSK modulated. The transceiver consists of a microwave oscillator based upon a silicon-germanium (SiGe) heterojunction phototransistor (HPT). The simulations show that a binary rate of 200 Mb/s is possible for a carrier of 5.2 GHz and reaches 1 Gb/s with a carrier at 12 GHz.
%X Many architectures of transistor only simulated inductors (TOSI) have been proposed until now in literature. Exhibiting tuning possibilities, low chip area and offering integration facility, they constitute promising architectures to replace passive inductors in RF circuits. An improved CMOS active inductor topology is proposed in this paper. With a novel loss compensation scheme, frequency increase up to 1.1 GHz (30%-66%) of the inductor self resonant frequency is achieved in the frequency band 1.5-3.3 GHz with large quality factors and very low current consumption. Besides, a more accurate passive model is proposed for CMOS TOSI active inductors and tested for this particular topology. Consisting of four parallel branches, it is still second order even though it contains three conservative elements. The model is sufficient general and proves superior performances over the classical RLC model mainly for higher frequencies. The simulations were carried out in a 0.18 um CMOS process.
%X In joint source-channel arithmetic coding (JSCAC) schemes, additional redundancy may be introduced into an arithmetic source code in order to be more robust against transmission errors. The purpose of this work is to provide analytical tools to predict and evaluate the effectiveness of that redundancy. Integer binary Arithmetic Coding (AC) is modeled by a reduced-state automaton in order to obtain a bit-clock trellis describing the encoding process. Considering AC as a trellis code, distance spectra are then derived. In particular, an algorithm to compute the free distance of an arithmetic code is proposed. The obtained code properties allow to compute upper bounds on both bit error and symbol error probabilities and thus provide an objective criterion to analyze the behavior of JSCAC schemes when used on noisy channels. This criterion is then exploited to design efficient error-correcting arithmetic codes. Simulation results highlight the validity of the theoretical error bounds and show that for equivalent rate and complexity, a simple optimization yields JSCACs that outperform classical tandem schemes at low to medium SNR.
%X Wyner-Ziv coding of continuous sources with uncertain side information quality is defined by modeling the correlation noise as a Gaussian-mixture. The analysis of the theoretical rate-distortion performance is presented, along with a coding solution not relying on the presence of a feedback channel. The attainable performance of the coding scheme is derived, and a brief discussion on implementation issues concludes the paper.
%X This paper considers the optimization of Error- Correcting Variable-Length Codes (EC-VLC), which are a class of joint-source channel codes. The aim is to find a prefix-free codebook with the largest possible free distance for a given set of codeword lengths, ℓ = (ℓ1, ℓ2, . . . , ℓM). The proposed approach consists in ordering all possible codebooks associated to ℓ on a tree, and then to apply an efficient branch-and-prune algorithm to find a codebook with maximal free distance. Three methods for building the tree of codebooks are presented and their efficiency is compared.
%X This paper considers the optimization of a class of joint source-channel codes described by finitestate encoders (FSEs) generating variable-length codes. It focuses on FSEs associated to joint-source channel integer arithmetic codes, which are uniquely decodable codes by design. An efficient method for computing the free distance of such codes using Dijkstra's algorithm is proposed. To facilitate the search for codes with good distance properties, FSEs are organized within a tree structure which allows the use of efficient branch-and-prune techniques avoiding a search of the whole tree.
%X We propose a visual recognition system for robotic applications in which distance to the visual objects can change a lot (for instance, trying to recognize a distant object learned from a short distance). Our system takes advantage of a single pan-tilt camera controllable in zoom and focus. Focus control allows to detect plans of sharpness in the scene and indirectly to compute a distance. Hence, this information can be used to gain structural information of the visual scene (to segment objects from the ground, to count the number of depth plans in the visual field...) without complex computation. This distance information is then used to control either a software or a hardware zoom to keep the size of the object invariant. The image thus created can be used by view based recognition systems. In a second time we show how by using focus points and neural networks we can improve the detection of sharpness plans in complex scenes. Finally we present a simple method to dynamically control the focus and stabilize it on the plan of sharpness of an object in the scene.
%X It is progressively realized that noise can play a constructive role in nonlinear formation processes. The starting point of the investigation of such useful noise effect has been the study of the Stochastic Resonance (SR) effect. The goal of this article is to propose a direct application of SR phenomenon in image processing, for the interest of SR in that domain is growing-up. As a prolongation of previous work already presented in the literature by author, we propose to quantitatively show that a purposely injection of a gaussian noise in a classical nonlinear image process, as image binarization, can play a constructive action. This work can also be interpreted as a first step for a better understanding of SR in image processing relating it to classical results obtained in a nonlinear signal processing framework for classical low-level image processing tool.
%X We consider in this paper the problem of blind frame synchronization of systems using reed-Solomon (RS) codes and other related families. We present first of all three techniques of blind frame synchronization based on the non-binary parity check matrix of RS codes. While the first two techniques involve the calculation of hard and soft values of the syndrome elements respectively, the third one perform an adaptation step of the parity check matrix before applying the soft criterion. Although RS codes are constructed from non-binary symbols, we show in this paper that it is also possible to synchronize them using the binary image expansion of their parity check matrix. Simulation results show that the synchronization algorithm based on the adaptation of the binary parity check matrix of RS codes has the best synchronization performance among all other techniques. Furthermore, the Frame Error Rate (FER) curves obtained after synchronization and decoding are very close to the perfect synchronization curves.
%X Purpose Image segmentation of MRI data is a challenging problem due to the specificity of the acquisition process and more precisely to the particularity of the associated corrupting noise that can be modeled by a Rician distribution. This particular distribution leads to multimodal area within MR image where foreground and target organs are characterized by different probability density functions (pdf) (mainly Rayleigh distribution and Gaussian distribution). Due to this particularity, classical automatic segmentation technique like Chan-Vese often failed in obtaining satisfying results of delineations in MR images where pdf of organs could overlap with background. This is the case for example in MR prostate image analysis, where bladder, prostate, and rectum can be hardly segmented thanks to classical approaches. Method In the framework of active contour segmentation [2], we propose to evaluate for MRI segmentation performances of histogram based method [1,3]. Main idea of this approach is to define, in a variationnal framework of region based active contours segmentation, evolution criteria accounting for the distances between probability density functions (pdf) of the inner and outer regions of the active contour and given pdfs of reference of objects and background. To achieve the performances evaluation, first a statistical study of the accuracy of the technique is proposed on synthetic MR data for different levels of corrupting noise and for different distance criteria between two given pdf and, second, a prospective study made on prostate MRI is shown for bladder, prostate and rectum segmentation, which respective delineation is of primary interest in the radiotherapy treatment of prostatic adenocarcinoma. Results A statistical study performed on synthetic MR images shows that histogram based active contour method outperform classical approach like the Chan-Vese one in terms of accuracy and that depending on the distance criterion selected, an adaptation to the level of corrupting noise is possible to reach the best accuracy possible. The prospective study made on prostate MRI shows that the proposed approach suits well to the particular MR distribution function: mainly, bladder, prostate and rectum segmentation are better performed by the proposed approach than with more classical active contour approaches.
%X In this paper we investigate an active vision technique implemented in an embedded system for 3D shapes reconstruction. The main objective of the work is to have a balance in the accuracy of all components in the system where the size and autonomy of such an embedded sensor are hard constraints. This is achieved through the improvement of the pre-processing algorithms by reducing the time needed to compute the spots centers. In addition, lens distortion of the camera is included in the model to increase accuracy when reconstructing objects. Experimental evaluation shows that the size and the time are reduced, precision increased, when the resources spent on processing are relatively acceptable in comparison to the benefits.
%X The IRIM group is a consortium of French teams working on Multimedia Indexing and Retrieval. This paper describes our participation to the TRECVID 2010 semantic indexing and instance search tasks. For the semantic indexing task, we evaluated a number of different descriptors and tried different fusion strategies, in particular hierarchical fusion. The best IRIM run has a Mean Inferred Average Precision of 0.0442, which is above the task median performance. We found that fusion of the classification scores from different classifier types improves the performance and that even with a quite low individual performance, audio descriptors can help. For the instance search task, we used only one of the example images in our queries. The rank is nearly in the middle of the list of participants. The experiment showed that HSV features outperform the concatenation of HSV and Edge histograms or the Wavelet features.
%X In this paper, the transmission over the binary erasure channel (BEC) using non-binary LDPC (NBLDPC) codes is considered. The concept of peeling decoder and stopping sets is generalized to NBLDPC codes. Using these generalizations, a combinatorial characterization of decoding failures of NBLDPC codes is given, under assumption that the Belief Propagation (BP) decoder is used. Then, the residual ensemble of codes resulted by the BP decoder is defined and the design rate and the expectation of total number of codewords of the residual ensemble are computed. The decoding failure criterion combined with the density evolution analysis helps us to compute the asymptotic residual degree distribution for NBLDPC codes. Our approach to compute the residual degree distribution on the check node side is not efficient as it is based on enumeration of all the possible connections on the check node side which satisfy the decoding failure criterion. So, the computation of the asymptotic check node side residual degree distribution and further part of our analysis is performed for NBLDPC codes over GF2m with m = 2 . In order to show that asymptotically almost every code in such LDPC ensemble has a rate equal to the design rate, we generalize the argument of the Maxwell construction to NBLDPC codes, defined over GF22. It is also observed that, like in the binary setting, the Maxwell construction, relating the performance of MAP and BP decoding holds in this setting.
%X This paper1 presents our investigation on iterative decoding performances of some sparse-graph codes on block-fading Rayleigh channels. The considered code ensembles are standard LDPC codes and Root-LDPC codes, first proposed in [1] and shown to be able to attain the full transmission diversity. We study the iterative threshold performance of those codes as a function of fading gains of the transmission channel and propose a numerical approximation of the iterative threshold versus fading gains, both both LDPC and Root-LDPC codes. Also, we show analytically that, in the case of 2 fading blocks, the iterative threshold γ*root of Root-LDPC codes is proportional to (α1α2)-1, where α1 and α2 are corresponding fading gains. From this result, the full diversity property of Root-LDPC codes immediately follows.
%X The incremental redundancy hybrid ARQ (IR-HARQ) schemes based on finite-length LDPC codes are considered in this paper. The transmission is assumed to take place over time varying binary erasure channels, such as mobile wireless channels at the applications layer for instance. Also, the iterative decoding is supposed. Two following issues are addressed: a) establishment of transmission rules for the m-th transmission after m-1 unsuccessful ones; b) approximation of the throughput and delay performance which would be accurate for finite codelengths, down to several hundreds of bits.
%X Within the Content Based Image Retrieval (CBIR) framework, three main points can be highlighted: visual descriptors extraction, image signatures and their associated similarity measures, and machine learning based relevance functions. While the first and the last points have vastly improved in recent years, this paper addresses the second point. We propose a novel approach to compute vector representations extending state of the art methods in the field. Furthermore, our method can be viewed as a linearization of efficient well known kernel methods. The evaluation shows that our representation significantly improve state of the art results on the difficult VOC2007 database by a fair margin.
%X The paper reports on a novel method for reconstruction of nuclei and cell membranes from actin tagged fluorescence confocal microscopy images. Such reconstruction can provide spatial context for subsequent quantitative analysis of actin properties, cell morphology and shape changes in both controlled and stressed cell cultures. The proposed method is fully automatic and is formulated within active contour multiphase level set framework. The derived level set evolution PDEs combine previously proposed curvature and advection flows with propagation flow defined by specially designed set of geodesic distance maps. Additionally the proposed PDEs include additional components to impose known inclusion/exclusion topological constraints between cellular structures. The paper gives an overview of the proposed methodology as well as reports on an initial results obtained for monolayer of human prostate cells (PNT2) culture visualised using acting tagged fluorescence confocal microscopy.
%X This paper describes a novel method for active contour segmentation based on foreground/background alpha-divergence histogram distance measure. In recent years a number of variational segmentation techniques have been proposed for a region based active contour segmentation utilising different distance measures between probability density functions (pdf) describing foreground and background regions. The most common techniques use Ki2, Hellinger/Bhattacharya distances or Kullback-Leibler divergence. In this paper, it is proposed to generalize these methods by using the alpha-divergences distance function. This distance function depending on the selected value of its parameter encompasses mentioned above classical distances. The paper defines a partial differential equation, associated with alpha-divergence variational criterion, that governs the iterative deformations of the active contour. The experimental results on a synthetic data demonstrate that the proposed method outperforms previously proposed histogram based methods in terms of segmentation accuracy and robustness with respect to type and level of noise. The potential of the proposed technique for segmentation of cellular structures in fluorescence confocal microscopy data is also illustrated.
%X Cet article traite de la segmentation par contours actifs basée sur la minimisation d'un critère de distance interhistogrammes avec application à la segmentation des noyaux cellulaires en microscopie confocale tridimentionnelle. L'originalité de l'article réside en l'introduction des alpha-divergences comme critère de distance inter-histogrammes permettant d'une part, une généralisation des distances classiques utilisées pour ce type de méthodes et d'autre part un enrichissement des possibilités de paramétrisation en fonction du type de données considérées. Grâce à cette famille, on obtient une amélioration des performances (précision en particulier) de la segmentation par contours actifs intégrant les critères classiques de distance inter-histogrammes dans la cadre de la segmentation des noyaux cellulaires en microscopie confocale. Les résultats très satisfaisants de l'approche proposée par rapport aux méthodes classiques, et ce dans un contexte de bruit difficile ouvre des perspectives prometteuses.
%X Although the problem of computing frequent queries in relational databases is known to be intractable, it has been argued in our previous work that using functional and inclusion dependencies, computing frequent conjunctive queries becomes feasible for databases operating over a star schema. However, the implementation considered in this previous work showed severe limitations for large fact tables. The main contribution of this paper is to overcome these limitations using appropriate auxiliary tables. We thus introduce a novel algorithm, called Frequent Query Finder (FQF), and we report on experiments showing that our algorithm allows for an effective and efficient computation of frequent queries.
%X Danslafouillededonnéesmulti-tables,lesdonnéessontreprésentées sous un format relationnel dans lequel les individus de la table cible sont poten- tiellement associés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. La plupart des approches existantes opèrent en transfor- mant la représentation multi-tables, notamment par mise à plat. Par conséquent, on perd la représentation initiale naturellement compacte mais également on risque d'introduire des biais statistiques. Notre approche a pour objectif d'éva- luer l'informativité des variables explicatives originelles par rapport à la variable cible dans le contexte des relations un-à-plusieurs. Elle consiste à résumer l'in- formation contenue dans chaque variable par un tuple d'attributs représentant les effectifs des modalités de celle-ci. Des modèles en grilles multivariées sont alors employés pour qualifier l'information apportée conjointement par les nou- veaux attributs, ce qui revient à une estimation de densité conditionnelle de la variable cible connaissant la variable explicative en relation un-à-plusieurs. Les premières expérimentations sur des bases de données artificielles et réelles montrent qu'on arrive à identifier les variables explicatives potentiellement per- tinentes sur tout le domaine relationnel.
%X Sales on the Internet have increased significantly during the last decade, and so, it is crucial for companies to retain customers on their web site. Among all strategies towards this goal, providing customers with a flexible search tool is a crucial issue. In this paper, we propose an approach, called TIGER, for handling such flexibility automatically. More precisely, if the search criteria of a given query to a relational table or a Web catalog are too restrictive, our approach computes a new query combining extensions of the criteria. This new query maximizes the quality of the answer, while being as close as possible to the original query. Experiments show that our approach improves the quality of queries, in the sense explained just above.
%X In content-based image retrieval context, a classic strategy consists in computing off-line a dictionary of visual features. This visual dictionary is then used to provide a new representation of the data which should ease any task of classification or retrieval. This strategy, based on past research works in text retrieval, is suitable for the context of batch learning, when a large training set can be built either by using a strong prior knowledge of data semantics (like for textual data) or with an expensive off-line pre- computation. Such an approach has major drawbacks in the context of interactive retrieval, where the user iteratively builds the training data set in a semi-supervised approach by providing positive and negative annotations to the system in the relevance feedback loop. The training set is thus built for each retrieval session without any prior knowledge about the concepts of interest for this session. We propose a completely different approach to build the dictionary on-line from features extracted in relevant images. We design the corresponding kernel function, which is learnt during the retrieval session. For each new label, the kernel function is updated with a complexity linear with respect to the size of the database. We propose an efficient active learning strategy for the weakly supervised retrieval method developed in this paper. Moreover this framework allows the combination of features of different types. Experiments are carried out on standard databases, and show that a small dictionary can be dynamically extracted from the features with better performances than a global one.
%X This paper describes a novel method for shape detection and image segmentation. The proposed method combines statistical shape models and active contours implemented in a level set framework. The shape detection is achieved by minimizing the Gibbs energy of the posterior probability function. The statistical shape model is built as a result of a learning process based on nonparametric probability estimation in a PCA reduced feature space formed by the Legendre moments of training silhouette images. The proposed energy is minimized by iteratively evolving an implicit active contour in the image space and subsequent constrained optimization of the evolved shape in the reduced shape feature space. Experimental results are also presented to show that the proposed method has very robust performances for images with a large amount of noise.
%X Several algorithms for top-k query processing over web data sources have been proposed, where sources return relevance scores for some query predicate, aggregated through a composition function. They assume specific conditions for the type of source access (sorted and/or random) and for the access cost, and propose various heuristics for choosing the next source to probe, while generally trying to refine the score of the most promising candidate. We present BreadthRefine (BR), a generic top-k algorithm, working for any combination of source access types and any cost settings. It proposes a new heuristic strategy, based on refining all the current top-k candidates, not only the best one. We present a rich panel of experiments comparing BR with state-of-the art algorithms and show that BR adapts to the specific settings of these algorithms, with lower cost.
%X In this paper we present RoSeS (Really Open Simple and Efficient Syndication), a generic framework for content-based RSS feed querying and aggregation. RoSeS is based on a data-centric approach, using a combination of standard database concepts like declarative query languages, views and multi-query optimization. Users create personalized feeds by defining and composing content-based filtering and aggregation queries on collections of RSS feeds. Publishing these queries corresponds to defining views which can then be used for building new queries / feeds. This naturally reflects the publish-subscribe nature of RSS applications. The contributions presented in this paper are a declarative RSS feed aggregation language, an extensible stream algebra for building efficient continuous multi-query execution plans for RSS aggregation views, a multi-query optimization strategy for these plans and a running prototype based on a multi-threaded asynchronous execution engine.
%X In this paper, an efficient rate allocation scheme to maximize the throughput in uplink multi-rate time-hopping ultra-wideband (TH-UWB) communications is proposed. This is a challenging task due to the lack of a suitable closed form expression of the multiuser interference in a multi-rate context. Moreover, an exhaustive search for the optimal solution would be too prohibitive due to the high complexity induced. Our contribution can be summarized as follows: i) A new expression of the multiuser interference variance accounting for multi-rate communications is proposed. ii) Thanks to this, a computational efficient multi-rate allocation scheme is provided. The algorithm allows to achieve high throughput while keeping a relatively low starvation rate and it outperforms a Max-Min algorithm with a fix rate for all users. Moreover, the simulations based on the theoretical analysis give insights on the influence of the different UWB parameters on the global throughput.
%X Robots are expected to become reliable partners for working in human environments. One of the tasks that they could have to perform is collecting objects and gathering/sorting them for any purposes. In such a task the robot would have to explore its environment to discover objects of interest, navigate toward them, grasp them and navigate again to a storage area to drop the object. In this paper we show how the basic principles behind these complex behaviors can be obtained from fairly simple neural network controllers.
%X In this paper, we propose a bio-inspired torque controller based on a Neural Network architecture. This controller was used to reproduce demonstrated movements.
%X When a robot is brought into a new environment, it has a very limited knowledge of what surrounds it and what it can do. One way to build up that knowledge is through exploration but it is a slow process. Programming by demonstration is an efficient way to learn new things from interaction. A robot can imitate gestures it was shown through passive manipulation. Depending on the representation of the task, the robot may also be able to plan its actions and even adapt its representation when further interactions change its knowledge about the task to be done. In this paper we present a bio-inspired neural network used in a robot to learn arm gestures demonstrated through passive manipulation. It also allows the robot to plan arm movements according to activated goals. The model is applied to learning a pick-and-place task. The robot learns how to pick up objects at a specific location and drop them in two different boxes depending on their color. As our system is continuously learning, the behavior of the robot can always be adapted by the human interacting with it. This ability is demonstrated by teaching the robot to switch the goals for both types of objects.
%X The increasing need to reduce power consumption and interconnection complexity in optical network on chip requires new configurations and strategies to interconnect cores in one chip. In this paper, we study a new configuration for an optical router on chip ≪ROTAR≫. In this router, the number of microrings was reduced to 4, allowing 30% reduction of power consumption compared to Huaxi Gu et al. We study waveguide losses at crossing, bends and resonant rings using the numerical method FDTD. Then, we propose an algorithm to perform a global estimation of all type of losses in our optical network on chip, assuming 1mm2 area and use of 8*8 routers. Using the Fat-H-Tree topology, we can reduce the number of routers interconnecting 64 cores, compared to the configuration proposed by Huaxi Gu et al. We use GaAs as a substrate to facilitate the integration of optoelectrical devices and silicon waveguides (refraction index = 3,5) surrounded by a layer of silica (1,43) to achieve a strong field confinement in the waveguide. The use of such routers in OnoC has several benefits such as a static and simple routing algorithm and more interconnection capacity compared to λ-router.
%X We present RoSeS, a running system for large-scale content-based RSS feed filtering and aggregation. The implementation of RoSeS is based on standard database concepts like declarative query languages, views and multi-query optimization. Users create personalized feeds by defining and composing content-based filtering and aggregation queries on collections of RSS feeds. These queries are translated into continuous multi-query execution plans which are optimized using a new cost-based multi-query optimization strategy.
%X A new SiGe heterojunction bipolar phototransistor (HPT) based on a commercially available process was designed, realized, and experimentally characterized. Its internal characteristics, mainly the collector-to-base capacitance, vary significantly with the received light power, making it suitable as an active element of a light-controlled photo-oscillator. It can also be a key component of optical network-on-chip (ONoC). Its responsivity was improved and its transition frequency remains in the range of 30 GHz.
%X This paper describes a bio-inspired architectural approach to design highly adaptive systems in the context of mobile robotics. The concerned robots evolve in an indoor unknown environment and then exhibit several behaviours such as landscape learning, obstacle avoidance, path planning, sensorimotor control. We aim at designing the intelligent embedded controller of those robots. The controller will be able to selforganize its elements in order to adapt its architecture to the robot behaviour. We focus in this paper on the description of the neural network which was developed to self-organize the system. We present the simulation results of our method.
%X Future cellular networks are facing crucial architecture changes to cope with high throughput, energy and cost-efficiency demands. Emerging solutions are small-cells and femto-cells which will coexist with classical macro-cells technology. In these heterogeneous networks, we study the joint service pricing and bandwidth allocation problem at the operator level. Each user selfishly adopts the service that optimizes its satisfaction. The user-level problem is formulated as a general non-atomic game. The Wardrop equilibrium is proven to exist and an analytical expression is provided for arbitrary number of services. The equilibria multiplicity, the influence of pricing and bandwidth allocation policies are investigated numerically.
%X Orthogonal Frequency Division Multiplexing (OFDM) is very sensitive to carrier frequency offsets (CFO). The CFO results in intercarrier interference (ICI). This drawback of OFDM is inherited to Orthogonal Frequency Division Multiple Access (OFDMA). In this paper, we propose a new ICI model for an uplink OFDMA system that takes into account the presence of the cyclic prefix which is usually ignored. Simulations validate the accuracy of our derivation. The subcarrier allocation is revisited in the presence of CFO.
%X We analyze the power allocation problem for orthogonal multiple access channels by means of a non-cooperative potential game in which each user distributes his power over the channels available to him. When the channels are static, we show that this game possesses a unique equilibrium; moreover, if the network's users follow a distributed learning scheme based on the replicator dynamics of evolutionary game theory, then they converge to equilibrium exponentially fast. On the other hand, if the channels fluctuate stochastically over time, the associated game still admits a unique equilibrium, but the learning process is not deterministic; just the same, by employing the theory of stochastic approximation, we find that users still converge to equilibrium. Our theoretical analysis hinges on a novel result which is of independent interest: in finite-player games which admit a (possibly nonlinear) convex potential, the replicator dynamics converge to an ε-neighborhood of an equilibrium in time O(log(1/ε)).
%X This chapter is focused on defining and optimizing an energy-efficiency metric for MIMO systems. This metric, which expresses in bit per Joule, allows one to measure how much information is effectively transferred to the transmitter per unit cost of energy consumed at the transmitter. For a MIMO point-to-point communication (single user MIMO channels) this metric can be useful to determine what power level, precoding scheme, training length, or number of antennas have to be used for obtaining the maximum information that is effectively transferred per unit energy spent. Then, we move from a physical layer-type approach to a cross-layer design of energy-efficient power control by including the effects a queue with finite size at the transmitter. As a last step we study a distributed multiple user scenario (MIMO multiple access channels) where each user selfishly maximizes its energy-efficiency by choosing its best individual power allocation policy. Here, we present the most relevant results in this field in a concise and comprehensible manner.
%X In this paper, we propose a joint estimation method of carrier frequency offsets (CFO) and channel impulse responses (CIR) for an uplink OFDMA system. We consider the CFOs come into play from the emitters due to frequency mismatch between the terminals and base station oscillators. We also take into account the cyclic prefix to derive a new model of the received signal.
%X The increasing need to reduce power consumption and interconnection complexity in optical network on chip requires new configurations and strategies to interconnect cores in one chip. In this paper we present a new configuration of an optical network on chip (ONoC) in which the routing of control and payload data is done optically. So we can reduce the use of electric signals in the router itself. To validate and confirm our choice, we conducted a study of all types of losses that may happen to the signal across the network using FDTD-based simulators, and finally taking into account the results of this study, we present an algorithm that allows us to estimate the maximum loss of each connection between any processors of the network. This estimation allows us to assess the reliability of such configuration. In a network, the number of rings in each router was reduced to 4, allowing 30% reduction of power consumption compared to Huaxi Gu et al . We will study too active microresonator behavior, mainly resonance frequency in order to find the most suitable frequency to modulate signals. The use of such routers in ONoC has several benefits such as a static and simple routing algorithm and more interconnection capacity compared to other proposed routers.
%X In this paper, we evaluate different geometric consistency schemes, which can be used in tandem with an efficient architecture, based on voting and local descriptors, to retrieve multimedia documents. In many contexts the geometric consistency enforcement is essential to boost the retrieval performance. Our empirical results show however, that geometric consistency alone is unable to guarantee high-quality results in databases that contain too many non-discriminating descriptors.
%X Mirror neurons have often been considered as the explanation of how primates can imitate. In this paper, we show that a simple neural network architecture that learns visuo-motor associations can be enough to let low level imitation emerge without a priori mirror neurons. Adding sequence learning mechanisms and action inhibition allows to perform deferred imitation of gestures demonstrated visually or by body manipulation. With the building of a cognitive map giving the capability of learning plans, we can study in our model the emergence of both low level and high level resonances highlighted by Rizzolatti et al.
%X Segmentation of cellular structures is of primary interest in cell imaging for a 3D reconstruction of cell shape. Such an analysis provides crucial information about cell morphology and is instrumental in understanding of biological processes leading to development of a particular pathology. The work presented in this paper reports on a novel method for segmentation of cellular structures (nuclei and cell boundaries) from 3D single channel actin tagged fluorescence confocal microscopy images. The proposed segmentation method uses histogram-based image similarity measure in a level-set active-contour framework. The novelty of the method is in application of the alpha-divergence distance measure which can be seen as a generalization of classic Kullback-Leibler and Chi 2 measures. The resulting alpha-divergence level-set formulation leads to a single front evolution formula for both nuclei and cell boundaries segmentation, with no requirements for any enhancement or preprocessing of acquired cell images (a monolayer of human cells (PNT2) culture).
%X In the framework of online object retrieval with learning, we address the problem of graph matching using kernel functions. An image is represented by a graph of regions where the edges represent the spatial relationships. Kernels on graphs are built from kernel on walks in the graph. This paper firstly proposes new kernels on graphs and on walks, which are very efficient for graphs of regions. Secondly we propose fast solutions for exact or approximate computation of these kernels. Thirdly we show results for the retrieval of images containing a specific object with the help of very few examples and counter-examples in the framework of an active retrieval scheme.
%X This book is the practical guide about designing, building and programming a modern robotics system. It presents a collection of wide range research results from the robotics community. Various aspects of current research in robotics area are explored and discussed. In particular, the major part of the book is devoted to robotic vision systems, robot navigation, human-robot interaction and adaptive learning systems. Different approaches include theorems and formal proofs for these systems are provided. Some advanced mathematics will be involved. This book is aimed at students and researchers who have an avid interest in this area will appreciate this user-friendly presentation of a wealth of robotics topics.
%X Nonlinearity problems for power amplifiers are well known and many linearization techniques have been proposed in the literature, especially digital baseband predistortion. Orthogonal Frequency Division Multiplexing (OFDM) has a very high Peak-to-Average Power Ratio (PAPR), making it very sensitive to non linearities. We have chosen the Active Constellation Extension (ACE) to reduce the crest factor, which is not sufficient. In this paper, we show that the association of ACE with digital predistortion helps to improve the Adjacent Channel Power Ratio (ACPR) and so permits a good linearity improvement. The association is first validated in simulation. In measurements, for 42.5dBm RMS output power, which represents the nominal power, the ACPR is increased from 32dBc to 49dBc. For a given ACPR of 35dBc, the RMS output power is increased from 40.7dBm to 42.7dBm.
%X Les problèmes de non linéarité engendrés par les amplificateurs de puissance sont bien connus et beaucoup de techniques de linéarisation ont été proposées dans la littérature, dont une en particulier : la prédistorsion numérique. La modulation multiporteuse Orthogonal Frequency Division Multiplexing (OFDM) a une forte dynamique, caractérisée par son fort Peak-to-Average Power Ratio (PAPR), ce qui la rend très sensible à ces non linéarités. Nous avons choisi l'Active Constellation Extension (ACE) comme technique de réduction du facteur crète. Seule, cette technique n'est pas suffisante pour assurer une bonne linéarité. Dans ce papier, nous montrons que l'association de l'ACE avec la prédistorsion numérique permet d'augmenter l'Adjacent Channel Power Ratio (ACPR) du signal et ainsi d'améliorer les performances de linéarité. Cette association est tout d'abord validée en simulation, puis des mesures sont effectuées. En mesure, pour une puissance moyenne de sortie fixée de 42.5dBm, ce qui représente la puissance nominale de l'amplificateur, les ACPR sont améliorés de 17dB grâce à l'ajout de l'ACE et de la prédistorsion, en comparaison à une dégradation de plus de 15dB avec la prédistorsion seule. Pour un ACPR donné de 35dBc, la puissance moyenne de sortie est augmentée de 2dB grâce à l'ajout de l'ACE à la prédistorsion qui seule n'apporte pas de gain.
%X This paper presents the design of an integrated inductor with AMS CMOS 0.35 mum technology. This inductor is designed to ensure the wide band LNA circuit implementation on silicon. This inductor is designed with a coplanar transmission line. This line type achieves an inductance value of 0.38 nH on the whole operating frequency band from 2 to 6 GHz.
%X LNA is one very essential bloc in the RF receiver. Due to the growth of the standard evolution, this component must handle several frequency bands with the best performances. This chapter presents a wide band LNA design for IEEE802.16 standard with the CMOS 0.35µm technology. In this LNA, we use a CPW transmission line to design the inductive degeneration inductor of 0.38nH. This circuit has a S21 of 12dB, a noise figure less than 3dB and an input/output reflexion coefficient less than -10dB between 2 and 6GHz. The CPW line presents a characteristic impedance of 120O, an inductance of 0.38nH, a capacitance of few fF and a resistance less than 2O on the desired frequency band.
%X We propose a methodology to extract models for inductors from measurements. The methodology is applied to a one-turn solenoid and to spiral inductor.
%X Silicon technologies are now widely used for radio frequencies applications. Due to the substrate and conductor conductivity, losses and magnetic coupling affect the signal propagation in transmission line. We propose an equivalent scheme for a transmission line taking these phenomena into account. All the parameters are analytically extracted from measurements. The magnetic coupling, including eddy currents, are described as series elements. They are affected by skin effect. The electric losses are described as parallel elements. They depend on the different layers that the electric field lines meet.
%X A dual wide band low noise amplifier for WiMAX (802.16a) standard is presented. The circuit is designed with AMS 0.35 mum CMOS process. This LNA is designed to cover the two frequency ranges for licensed and unlicensed bands of the WiMAX 2-4 GHz and 5-6 GHz. The proposed amplifier achieves a wide band input matching with Sn lower than -10 dB for all the band 2-6 GHz, a flat gain for the two bands; 13 db between 2-4 GHz and 10 dB between 5-6 GHz. The noise figure is around 3 dB for all the band and 2-6 GHz. The presented dual wide band LNA employs a Chebychev filter for input matching and an inductive shunt feedback for output matching. The power consumption is 62.5 mW under a power supply voltage of 2.5 V.
%X It is progressively realized that noise can play a constructive role in the domain of nonlinear information processing. This phenomenon, also known as stochastic resonance (SR) effect, has experienced large varieties of extensions with variations concerning the type of noise, the type of information carrying signal or the type of nonlinear system interacting with the signal-noise mixture. In this article, we propose an interpretation for the mechanism of noise-enhanced image restoration with nonlinear PDE (Partial Differential Equation) recently demonstrated in literature. More precisely, a link is established between the action of noise in a nonlinear Perona-Malik anisotropic diffusion and stochastic resonance in memoryless nonlinear systems for 1-D signals. For illustration some preliminary results are presented on classical "cameraman" image and the inner of SR mechanism is theoretically and practically studied using a simple set of parameters regarding the PDE used and the modeling of boundaries within images.
%X This paper addresses the spectrum-sharing for wireless communication where a cognitive or secondary user shares a spectrum with an existing primary user (and interferes with it). We propose two lower bounds, for the primary user mean rate, depending on the channel state information available for the secondary-user power control and on the type of constraint for spectrum access. Several power control policies are investigated and the achieved primary-user mean rates are compared with the lower bounds. Specially, assuming all pairs of transmitter-receiver are achieving real-time delay-sensitive applications, we propose a novel secondary-user power control policy to ensure for both users, at a given occurrence, predefined minimum instantaneous rates. This power control uses only the secondary-user direct links gains estimations (secondary-to-secondary link and secondary-to-primary link). We take into account all links in the network.
%X Wireless video capsules can now carry out gastroenterological examinations. The images make it possible to analyze some diseases during postexamination, but the gastroenterologist could make a direct diagnosis if the video capsule integrated vision algorithms. The first step toward in situ diagnosis is the implementation of 3-D imaging techniques in the video capsule. By transmitting only the diagnosis instead of the images, the video capsule autonomy is increased. This paper focuses on the Cyclope project, an embedded active vision system that is able to provide 3-D and texture data in real time. The challenge is to realize this integrated sensor with constraints on size, consumption, and processing, which are inherent limitations of the video capsule. We present the hardware and software development of a wireless multispectral vision sensor which enables the transmission of the 3-D reconstruction of a scene in real time. An FPGA-based prototype has been designed to show the proof of concept. Experiments in the laboratory, in vitro, and in vivo on a pig have been performed to determine the performance of the 3-D vision system. A roadmap towardthe integrated system is set out.
%X Today's FGDRA could now be regarded, not only as prototyping platforms, but also as reliable alternative to ASICs for consumers products production platforms. To deal with such a system, we propose a middleware layer (RTOS) named SMILE, which could manage not only software process but also hardware tasks running on an FGDRA. Preemption issues for hardware tasks on such a system will be treated, introducing the concept of PDR-SoC2. In this paper, our work on hardware FGDRA based task contexts, its management and its evaluation, is exposed.
%X In the context of large versatile platform for embedded real time system on chip, a fine grained dynamically reconfigurable architecture could be used as one possible computational resource. In order to manage efficiently this resource we need a specific OS kernel able to manage such a hardware adaptable architecture. Both the history of micro-processor based system and our previous work based on currently available FPGA devices led us to think that not only an OS kernel must be defined to handle an FGDRA but a FGDRA must also be designed to handle this OS kernel. This article relate our original work in this direction. OLLAF1, an original FGDRA core that we have designed will be presented. A comparison with other methods used today using commercially available FPGA is also presented concerning the particular preemption service.
%X In the context of large versatile platform for embedded real time system on chip, a fine grained dynamically reconfigurable architecture (at a first approach an FPGA) can be used as one possible computational resource. In order to manage this resource in that kind of context we need an OS kernel able to manage such a resource. Both the history of micro-processor based system and our previous work based on currently available FPGA devices led us to think that not only an OS kernel must be defined to handle an FGDRA but a FGDRA must also be designed to handle this OS kernel. This article relate our original work in this direction. OLLAF\footnote{Operating system enabled Low LAtency Fgdra}, an original FGDRA core that we have designed and modelized using VHDL will be presented as well as a more general view of our approach of a FGDRA and its related OS kernel.
%X In the context of large versatile platform for embedded real time system on chip, a fine grained dynamically reconfigurable architecture could be used as one possible computational resource. In order to manage efficiently this resource we need a specific OS kernel able to manage such a hardware adaptable architecture. Both the history of micro-processor based system and our previous work based on currently available FPGA devices led us to think that not only an OS kernel must be defined to handle an FGDRA1 but a FGDRA must also be designed to handle this OS kernel. This article relate our original work in this direction. OLLAF2, an original FGDRA core that we have designed will be presented. A comparison with other methods used today using commercially available FPGA is also presented concerning the particular preemption service.
%X This article present an original hardware task model and the corresponding online API for Fine Grained Dynamically Reconfigurable Architecture. We cover the integration of this API in the OLLAF platform and more specifically its application to memory access management in a dynamically reconfigurable environment. Methods offered by this platform are compared to existing software and hardware solutions. We also discuss of the design complexity of an application using difference solutions. We demonstrate that our solution cans give application developer the same flexibility than with a software implementation, with a very close design complexity while ensuring the same performance gain a common FPGA based IP would permit.
%X Numerical inversions via circular harmonic decomposition for two classes of circular Radon transforms are established. The first class deals with the Radon transform (RT) defi ned on circular arcs having a chord of fixed length rotating around its middle point (CART) and the second is the RT de fined on a set of circles passing through a fixed point of the plane (CRT). These circular Radon transforms arise from the modeling of di erent modalities in Compton scattering tomography (CST). Inversions via circular harmonic decomposition are used for image reconstructions in CST. Simulation results show the efficiency and interest of this inversion method in imaging science.
%X We work out a novel image reconstruction algorithm based on the circular harmonic decomposition applied on a new modality of Compton scattering tomography. Numerical results and their accuracy of reconstruction prove the feasibility and the interest of this approach in the medical imaging field and the nondestructive testing while proposing an alternative to those of conventional tomography.
%X Conventional CT is used in many fields such as non-destructive testing, biomedical imaging, environmental monitoring, etc. Then a detector rotates in space to collect primary radiation transmitted by an object under investigation. In this case, the Compton effect behaves as a noise interfering with image quality, and therefore, a correction of Compton scattering is imposed. Recently, a new concept for imaging, which uses scattered radiation as an agent for imaging, has been advocated. The camera now collects scattered photons according to their energy and it has been shown that the image reconstruction is possible from the complete set of scattered photons. In this paper, we propose a new modality of tomography Compton, akin to standard X-ray tomography, in the sense that it operates in transmission, but uses the scattered radiation. Modeling of image formation is based on a new class of transformation of Radon defined on circular arcs (CART). We thus obtain a method for imaging reconstruction from the inversion of the CART in the space of harmonic components which is akin to the well-known filtered back-projection algorithm.
%X Applications executed on embedded systems require dynamicity and flexibility according to user and environment needs. Dynamically reconfigurable architecture could satisfy these requirements but needs efficient mechanisms to be managed efficiently. In this paper, we propose a dedicated application modeling technique that helps to establish a predictive scheduling approach to manage a dynamically reconfigurable architecture named OLLAF. OLLAF is designed to support an operating system that deals with complex embedded applications. This model will be used for a predictive scheduling based on an early estimation of our application dynamicity. A vision system of a mobile robot application has been used to validate the presented model and scheduling approach. We have demonstrated that with our modeling we can realize an efficient predictive scheduling on a robot vision application with a mean error of 6.5%.
%X RÉSUMÉ : Dans cet article nous évaluons l'apport des architecturesconnexionnistes aux rétines électroniques en nous basant sur une méthode originale d'évaluation et de prédiction des performances que nous avons développée. Nous validons cette méthode sur une classe de réseaux connexionnistes, et sur une architecture existante. Nous proposons une architecture de rétine électronique basée sur une architecture connexionniste programmable et de ce fait adaptée à une large classe de situations. ABSTRACT : In this article we study the impact of connexionnist architectures on electronic retina thanks to an original performanceevaluation/predictionmethod.We validate this method with a specific connexionnist network class and an existing architecture. Finally we propose a versatile electronic retina based on a programmable connexionnist architecture.
%X A new non-invasive low cost method for accelerations measurement in biped locomotion mechanisms is presented. The aim is to measure all the joint accelerations of the locomotion system during the walking that is difficult because of the non-accessibility of several joint axes. Our measurement approach is non-direct using two triaxial accelerometers aligned on the main axis of each body, that ensures measurement of five accelerations per joint, two angular and three linear. By considering negligible the vertical rotation of a leg in the absolute frame during walking, the vertical rotational acceleration of the hip is measurable by two accelerometers aligned along the sagittal axis. For tibia and thigh, the accelerometers are aligned along vertical axes. The calculation of accelerations in articulation references is achieved by solving a system of Newton mechanics equations. The theoretical statements were validated experimentally on the robot for vertical flexion-extension movement.
%X The aim of this paper is to consider the adaptation behavior of an electromechanical arm manipulator to the physical interaction of humans. Preliminary experiments to explore the possibility of adaptive interactions between an arm robot and a human without knowledge of the forces are investigated. A simple and efficient control adaptation of the system is implemented at the level of the electrical drive.
%X Research in robot locomotion can be separated into few groups. The dominated research is based on a high level modeling approach, like ZMP control that incorporate inverse kinematics techniques, which suffers from the problem of high dimensionality, delays, and the requirement of perfect knowledge of the robot and environment. Taken inspiration from neuroscience, with the attention on biological inspired locomotion for robot control [1]. These controllers do not require perfect knowledge of the robot's dynamics as with classical strategies and have shown great promise in terms of robustness, simplicity and adaptivity. Biological inspired locomotion controllers are based on simple circuits that are built from sensor-neurons, motor neurons, and inter-neurons [2] [3]. Neurophysiological studies associate rhythmic movements with the oscillation activity of a particular type of neurons, called neural oscillators [4] [5]. These oscillators can produce rhythmic activity without sensory inputs and even without any central inputs. But the sensory information is indispensable for walking because it allows shaping of the rhythmic patterns in order to interact with the environment [6]. However, sensory information is mainly used to adapt the controller in the event of changes and perturbations. Neurophysiologists have shown that biological controllers like Central Pattern Generators (CPG) have adaptation properties due to neural plasticity mechanism [4] [7]. With inspiration from neurobiology, Ijspeert et al. proposed different models for rhythmic movements control [3]. The neural reflexive walking controller, proposed by F. W¨org¨otter is one of the few that have been tested on a real bipedal robot [2]. Our work aims to produce a robust biological inspired neural controller for biped walking, based on CPG with a rhythmic neuron proposed by Rowat and Selverston [5]. We show therefore how to adapt against an external perturbation force by phase resetting or by behavior adapting. This paper is organized as following. Section 2 presents the principles of the neural controller based on the model of rhythmic neurons, which is able to generate CPG-like patterns. The three layers of the CPG used in bipedal control are presented. A coupling circuitry for walking is proposed. Next, two approaches to deal with for external perturbation is presented. The last section gives a conclusion and details of further developments.
%X Human robot interaction is a key issue in order to build robots for everyone. The difficulty for people to understand how robots work and how they must be controlled will be one of the mains limit for broad robotics. In this paper, we study a new way of interacting with robots without needing to understand how robots work or to give them explicit instructions. This work is based on psychological data showing that synchronization and rhythm are very important features for pleasant interaction. We propose a biologically inspired architecture using rhythm detection to build an internal reward for learning. After showing the results of keyboard interactions, we present and discuss the results of real human-robots (Aibo and Nao) interactions. We show that our minimalist control architecture allows the discovery and learning of arbitrary sensorimotor associations games with expert users. With non-expert users, we show that using only the rhythm information is not sufficient for learning all the associations due to the different strategies used by the human. Nevertheless, this last experiment shows that the rhythm is still allowing the discovery of sub-sets of associations, being one of the promising signal of tomorrow social applications.
%X xxx
%X Radon transforms on piecewise smooth curves in R2 are rather unfamiliar and have not been so far widely investigated. In this paper we consider three types of Radon transforms defined on a pair of half-lines in the shape of a V, with a fixed axis direction. These three Radon transforms arise from recently suggested tomographic procedures. Our main result consists in obtaining their analytic inverse formulas, which may serve as mathematical foundation for new imaging systems in engineering and physics.
%X Dans cet article, une technique d'optimisation du taux de réjection du mode commun (TRMC) d'un amplificateur faible bruit (LNA) différentiel est présentée. Elle consiste principalement à placer des circuits résonnants dans le plan de symétrie permettant d'annuler le gain en mode commun du circuit. Le circuit est conçu en utilisant la technologie OMMIC ED02AH. Les simulations du circuit montrent que le TRMC est supérieur à 30dB sur une bande de fréquences de 6,2GHz autour de 30GHz, un gain différentiel Sdd21 de 11,8dB, un NFdd de 2,7dB avec une adaptation de -31dB pour Sdd11 et -29dB pour Sdd22.
%X xxx
%X xxx
%X xxx
%X xxx
%X xxx
%X xxx
%X In this paper, we consider the source-channel rate allocation for different transmission schemes. We propose a new trellis structure and a new algorithm able to deal with both variable length packet and fixed length packet problems. The trellis description allows to handle any kind of transmission schemes and can therefore be applied to Bit Interleaved Coded Modulations (BICM), parallel channels or Hybrid - Automatic Repeat reQuest.
%X Dans cet article, nous considérons la coexistence d'un utilisateur primaire et d'un utilisateur secondaire implémentant tous deux des systèmes a Retransmissions Incrémentales (IR) sur un canal cognitif. Nous proposons une généralisation d'un protocole d'émission secondaire présentée dans la littérature a des protocoles IR possédant des tailles de blocs variables et un nombre de retransmissions supérieur a 2. Enfin, nous donnons des expressions semi-analytiques des d ́ bits, d ́ bits moyens et probabilités de coupures pour le primaire et secondaire en l'absence de connaissance non-causale sur le lien cognitif.
%X In this paper, we propose, analyze and compare three different methods for opportunistic spectrum sharing access when the primary users implements an Incremental Redundancy (IR) type Hybrid Automatic ReQuest (H-ARQ) protocol. The first method consists in allowing the secondary user to communicate only during the first primary transmission round of the IR H-ARQ protocol. In this scenario, if the the secondary receiver fails to decode its message after the first round, it realizes a successive interference cancellation in the subsequent primary HARQ rounds by listening to the primary user. The second method consists in realizing a perfect interference cancellation at the secondary receiver with causal channel state information. In this method, the secondary user communicates only when the secondary receiver succeeds in decoding the primary message.To improve throughput performance at the secondary, the secondary pair is also considering the use of an IR-HARQ protocol. In a third method, the secondary user communicates following the same rule as in the proposed second method, but implementing an Adaptive Modulation and Coding scheme instead of HARQ. In particular, we show that this last protocol with a small number of interfered slots allows to limit the loss in the primary throughput needed for the secondary user to transmit.
%X Un amplificateur de puissance RF a été conçu en vue d'une étude de la fiabilité des transistors HEMT GaN. Architecturé autour d'un transistor NITRONEX NPTB00050, et fonctionnant en classe B, à la fréquence de 3GHz, cet amplificateur développe une puissance de 40W à 1dB de compression (10dB de gain en puissance). Un rendement en puissance ajoutée de 55% a été obtenu pour ce régime. Deux maquettes ont été éprouvées pendant 1280 heures sous 1dB de compression en régime RF pulsé à une température de semelle de 80°C. Aucune dégradation n'a été constatée parmi les grandeurs suivies (gain, rendement, transconductance, tension de pincement). Des études supplémentaires mettant en oeuvre le transistor sous forte polarisation de drain à une température de châssis de 80°C sont en cours et tendent à montrer une évolution des caractéristiques statiques du transistor. Des analyses structurales par FIB et TEM des transistors dégradés seront entreprises afin de corréler les évolutions de ces caractéristiques avec des modifications physico-chimiques de la structure du transistor.
%X In his seminal work of 1981, Cormack established that Radon transforms defined on two remarkable families of curves in the plane are invertible and admit explicit inversion formulas via circular harmonic decomposition. A sufficient condition for finding larger classes of curves enjoying the same property is given in this paper.We showthat these generalized Cormack's curves are given by the solutions of a nonlinear first-order differential equation, which is invariant under geometric inversion. A derivation of the analytic inverse formula of the corresponding Radon transforms, as well as some of their main properties, are worked out. Interestingly, among these generalized Cormack's curves are circles orthogonal to a circle of fixed radius centered at the origin of coordinates. It is suggested that a novel Compton scatter tomography modality may be modeled by a Radon transform defined on these circles.
%X Radon transforms on piecewise smooth curves in R2 are rather unfamiliar and have not been so far widely investigated. In this paper we consider three types of Radon transforms defined on a pair of half-lines in the shape of a V, with a fixed axis direction. These three Radon transforms arise from recently suggested tomographic procedures. Our main result consists in obtaining their analytic inverse formulas, which may serve as mathematical foundation for new imaging systems in engineering and physics.
%X We generalize a construction of non-binary quantum LDPC codes over $\F_{2^m}$ due to \cite{KHIS11a} and apply it in particular to toric codes. We obtain in this way not only codes with better rates than toric codes but also improve dramatically the performance of standard iterative decoding. Moreover, the new codes obtained in this fashion inherit the distance properties of the underlying toric codes and have therefore a minimum distance which grows as the square root of the length of the code for fixed $m$.
%X This paper addresses the issue of design of low-rate sparse-graph codes with linear minimum distance in the blocklength. First, we define a necessary condition which needs to be satisfied when the linear minimum distance is to be ensured. The condition is formulated in terms of degree-1 and degree-2 variable nodes and of low-weight codewords of the underlying code, and it generalizies results known for turbo codes [8] and LDPC codes. Then, we present a new ensemble of low-rate codes, which itself is a subclass of TLDPC codes [4], [5], and which is designed under this necessary condition. The asymptotic analysis of the ensemble shows that its iterative threshold is situated close to the Shannon limit. In addition to the linear minimum distance property, it has a simple structure and enjoys a low decoding complexity and a fast convergence.
%X We study iterative receivers for joint decoding and channel-state estimation for transmission on block-fading chan- nels of root-LDPC-coded signals. Root-LDPC codes are known to be most performant codes for block-fading channels, as their spacial "root" structure allows to get the full-diversity property. This property ensures a good error decoding performance of root LDPC codes, especially in contrast with the performance standard LDPC codes (having the maximum diversity equal to 1). However, as any channel code, root-LDPC codes also suffer from the diversity loss when the channel state information is not known at the receiver. In this work we propose a joint channel estimation- decoding scheme for root-LDPC codes that helps to overcome this problem and still to have the full-diversity.
%X In this paper, the update complexity of a linear code ensemble (binary or nonbinary) is considered. The update complexity has been proposed in [1] as a measure of the number of updates needed to be done within the bits of a codeword, if one of information bits, encoded in this codeword, has been changed. The update efficiency is a performance measure of distributed storage applications, that naturally use erasure-correction coding. The ensemble maximum complexity and the average complexity are distinguished in this paper. We first propose a simple lower bound on the average update complexity γavg of a code ensemble and further evaluate a general expression for γavg. Finally, it has been shown that one can upper bound the average update complexity for binary LDPC codes, by using the computation tree approach. We show that the code ensembles with polynomial minimum distance growth are not update-efficient, i.e. they have a high update complexity. It seems that only code families with sub- polynomial minimum distance (i.e. logarithmic) are update- efficient.
%X Imaging processes built on the Compton scattering effect have been under continuing investigation since it was first suggested in the 50s. However, despite many innovative contributions, there are still formidable theoretical and technical challenges to overcome. In this paper, we review the state-of-the-art principles of the so-called scattered radiation emission imaging. Basically, it consists of using the cleverly collected scattered radiation from a radiating object to reconstruct its inner structure. Image formation is based on the mathematical concept of compounded conical projection. It entails a Radon transform defined on circular cone surfaces in order to express the scattered radiation flux density on a detecting pixel. We discuss in particular invertible cases of such conical Radon transforms which form a mathematical basis for image reconstruction methods. Numerical simulations performed in two and three space dimensions speak in favor of the viability of this imaging principle and its potential applications in various fields.
%X A new circular-arc Radon transform arising from the mathematical modeling of image formation in a new modality of Compton scattering tomography is introduced. We describe some of its properties and establish its analytic inverse formula. This result demonstrates the feasibility of image reconstruction from Compton scattered radiation in Compton scattering tomography. We also show that it belongs to a larger class of Radon transforms on algebraic curves, which remain invariant under a specific geometric inversion.
%X A new efficient scheme for imaging gamma-emitting objects is advocated in this work. It is elaborated on the recent idea of collecting data, using a detector equipped with a parallel-hole collimator, from Compton scattered photons to reconstruct an object in three-dimensions. This paper examines a working mode without collimation, which should increase its sensitivity and field of view. To simplify the otherwise complex mathematical formulation, we choose to discuss the image formation process in two-dimensions, which can be implemented with a slit collimator. Comparison with the standard collimated case, via the analysis of the shapes of the respective point spread functions (PSF), shows marked improvements and numerical simulation results, obtained using a brain phantom, support the viability and attractiveness of this new imaging modality.
%X In this article, a complete original framework for non supervised statistical region based active contour segmentation is proposed. More precisely, the method is based on the maximization of alphadivergences between non paramterically estimated probability density functions (PDF) of the inner and outer regions defined by the evolving curve. In this paper, we define the variational context associated to distance maximization in the particular case of alphadivergence and we also provide the complete derivation of the partial differential equation leading the segmentation. Results on synthetic data (corrupted with a high level of Gaussian and Poisonian noises) but also on clinical images (X-ray images) show that the proposed non supervised approach improves classical approach of that kind.
%X The paper presents the design of an agile frequency synthesizer for an UWB transmission in the 3-11 GHz spectrum. The design is presented top-down, starting from a complete structure, the implemented basic structure, the schematic and the layout considerations and the testing. The circuits are implemented in 0.35 µm SiGe BiCMOS technology process AMS_S35D4 in order to validate the required performances.
%X Orthogonal Frequency Division Multiple Access (OFDMA) is very sensitive to the carrier frequency offsets (CFO) which results in intercarrier interference (ICI). In OFDMA carrier allocation is used to improve system performance. In this paper we analyze the effect of carrier allocation schemes on the CFO induced interference in an uplink OFDMA system without channel state information (CSI). We also propose a critical value of CFO for block carrier allocation scheme above which the system's performance shows significant degradation.
%X In this paper, we investigate the effect of subcarrier allocation on the CFO for uplink OFDMA systems. Carriers are allocated to the users in order to get maximum throughput by making use of the channel frequency diversity. But in systems with CFO, while allocating the carriers to the users, attention must be paid to the ICI resulting due to CFO. In this paper we propose a carrier allocation scheme that provides a good compromise between the throughput maximization and robustness to the CFO induced ICI for systems with and without channel state information (CSI).
%X Cet article présente les résultats de l'étude de faisabilité d'un synthétiseur de fréquences agile " multibandes " pour le standard de transmission MBOFDM. A partir d'une structure complète, une brique de base contenant tous les éléments du synthétiseur est choisie et conçue avec la technologie BiCMOS AMS_S35D4. Les résultats post-layout sont donnés pour deux fréquences centrales générées.
%X The IRIM group is a consortium of French teams work- ing on Multimedia Indexing and Retrieval. This paper describes its participation to the TRECVID 2011 se- mantic indexing and instance search tasks. For the semantic indexing task, our approach uses a six-stages processing pipelines for computing scores for the likeli- hood of a video shot to contain a target concept. These scores are then used for producing a ranked list of im- ages or shots that are the most likely to contain the tar- get concept. The pipeline is composed of the following steps: descriptor extraction, descriptor optimization, classification, fusion of descriptor variants, higher-level fusion, and re-ranking. We evaluated a number of dif- ferent descriptors and tried different fusion strategies. The best IRIM run has a Mean Inferred Average Pre- cision of 0.1387, which ranked us 5th out of 19 partic- ipants. For the instance search task, we we used both object based query and frame based query. We formu- lated the query in standard way as comparison of visual signatures either of object with parts of DB frames or as a comparison of visual signatures of query and DB frames. To produce visual signatures we also used two apporaches: the first one is the baseline Bag-Of-Visual- Words (BOVW) model based on SURF interest point descriptor; the second approach is a Bag-Of-Regions (BOR) model that extends the traditional notion of BOVW vocabulary not only to keypoint-based descrip- tors but to region based descriptors.
%X This book represents a sample of recent contributions of researchers all around the world in the field of image restoration. The book consists of 15 chapters organized in three main sections (Theory, Applications, Interdisciplinarity). Topics cover some different aspects of the theory of image restoration, but this book is also an occasion to highlight some new topics of research related to the emergence of some original imaging devices. From this arise some real challenging problems related to image reconstruction/restoration that open the way to some new fundamental scientific questions closely related with the world we interact with.
%X This paper revisits earlier work on rate distortion behavior of sparse sources, namely it highlights the fact that a graphical sparsity characterization proposed in \cite{Weidmann:00b} is a \emph{Lorenz curve}, a tool for summarizing income inequality that has been used by economists for over a century. The Lorenz curve associated to a memoryless source can be used to obtain upper bounds on the distortion rate function, thus characterizing source compressibility. It is shown that an order relation on Lorenz curves induces an analogous relation on distortion rate upper bounds. This can be used to characterize the compressibility of certain parametric families of source distributions, for which an order on the parameters induces an order on Lorenz curves.
%X In this paper we present a biologically-inspired model of spatio-temporal learning in the hippocampus and prefrontal cortex which can be used in tasks requiring the behavior of the robot to be constrained by sensory and temporal information. In this model chains of sensory events are learned and associated with motor actions. The temporality of these sequences is also learned and can be used to predict the timing of upcoming events. The neural network acts as a novelty detector and can modulate the behavior of the robot in case its actions do not have the expected consequences. The system is used to solve two different robotic navigation tasks involving an alternation between random exploration, goal-directed navigation and waiting periods of various lengths.
%X The research in content-based indexing and retrieval of visual information such as images and video has become one of the most populated directions in the vast area of information technologies. Social networks such as YouTube, Facebook, FileMobile, and DailyMotion host and supply facilities for accessing a tremendous amount of professional and user generated data. The areas of societal activity, such as, video protection and security, also generate thousands and thousands of terabytes of visual content. This book presents the most recent results and important trends in visual information indexing and retrieval. It is intended for young researchers, as well as, professionals looking for an algorithmic solution to a problem.
%X Examination of the whole gastrointestinal tract represents a challenge for endoscopists due to its length and inaccessibility using natural orifices. Moreover, radiologic techniques are relatively insensitive for diminutive, flat, infiltrative, or inflammatory lesions of the small bowel. Since 1994, video capsules (VCEs) have been developed to allow direct examination of this inaccessible part of the gastrointestinal tract and to help doctors to find the cause of symptoms such as stomach pain, disease of Crohn, diarrhoea, weight loss, rectal bleeding, and anaemia. The Pillcam© video capsule designed by Given Imaging Company is the most popular of them. This autonomous embedded system allows acquiring about fifty thousand images of gastrointestinal tract during more than twelve hours of an analysis. However, acquired images are of low resolution and imaging conditions could be very poor as the movements and the speed of the VCE as illumination are not entirely controlled all along the transit of the capsule through the whole gastrointestinal tract. As a consequence segmentation of VCE images is a real challenging problem: first of all specialists need a flexible segmentation tool that makes delineations of very different structures (polyps, metastasis... to name a few) possible and second, the proposed method should be efficient even in difficult imaging conditions. In this article, an adapted segmentation method based on statistical active contours for VCE segmentation is proposed and tested on various types of gastrointestinal structures.
%X In this article, we show that the non-ideal Radio-Frequency (RF) front-ends have to be corrected in order to contribute in a Green radio development. In fact, the effects of typical RF imperfections, like nonlinearities, carrier frequency offsets, and IQ imbalances, can be compensated for, when digital correction algorithms are applied. Such algorithms enable Green applications (e.g., Orthogonal Frequency Division Multiple Access for the uplink) despite a restrictive RF imperfection, or allow a less constrained hardware design, which reduces the chip area and the number of components (Green design) or facilitate the reduction of spectral pollution and of power consumption (Green transmission). So, we propose to implement these correction methods to compensate for the damaging effects of RF imperfections in mind of a Green issue.
%X In the framework of cell structure characterization for predictive oncology, we pro- pose in this paper an unsupervised statistical region based active contour approach in- tegrating an original fractional entropy measure for single channel actin tagged fluo- rescence confocal microscopy image segmentation. Following description of statistical based active contour segmentation and the mathematical definition of the proposed frac- tional entropy descriptor, we demonstrate comparative segmentation results between the proposed approach and standard Shannon's entropy obtained for nuclei segmentation. We show that the unsupervised proposed statistical based approach integrating the frac- tional entropy measure leads to very satisfactory segmentation of the cell nuclei from which shape characterization can be subsequently used for the therapy progress assess- ment.
%X We propose a novel algorithm for learning a geometric com- bination of Gaussian kernel jointly with a SVM classifier. This problem is the product counterpart of MKL, with restriction to Gaussian kernels. Our algorithm finds a local solution by alternating a Quasi-Newton gradi- ent descent over the kernels and a classical SVM solver over the instances. We show promising results on well known data sets which suggest the soundness of the approach.
%X In this paper, we discuss methods for building detection and reconstruction from aerial imagery. These methods are intended for the analysis of urban and suburban areas and have been applied to images of different resolutions (between 1 m and 10 cm per pixel). Various algorithms for image matching have been investigated, including hierarchical processing and new correlation schemes that have interesting properties for building recognition and building feature grouping. Cooperative combination of 2-D (monocular) and 3-D (stereoscopic) information allows the complete representation of the observed scene and particularly the detection of man-made raised structures such as buildings. A performance evaluation on simulation-based images has been considered in comparison with the corresponding ground truth reference. Our work illustrates that mid-resolution methods cannot be directly applied to high-resolution images. Classical algorithms must be adapted and new techniques have been defined to carry out dense urban area reconstruction.
%X Cet ouvrage est consacré essentiellement à la segmentation des images, mais il aborde aussi la majorité des grands problèmes du traitement des images : de la représentation des signaux, de leur description, des outils théoriques de leur traitement (morphologie mathématique, champs de Markov, graphes d'adjacence, classification et bien d'autres que j'oublie). C'est un traité qui sera précieux à plusieurs niveaux : pour l'étudiant qui y trouvera les bases indispensables à sa formation, pour le chercheur et l'enseignant qui compléteront leur culture et leurs références, pour le thématicien en télédétection, en sciences biomédicales ou dans d'autres domaines utilisateurs d'images, comme source d'inspiration pour automatiser ses traitements.
%X We address the issue of mining frequent conjunctive queries in a relational database, a problem known to be intractable even for conjunctive queries over a single table. In this article, we show that mining frequent projection-selection-join queries becomes tractable if joins are performed along keys and foreign keys, in a database satisfying functional and inclusion dependencies, under certain restrictions. We note that these restrictions cover most practical cases, including databases operating over star schemas, snow-flake schemas and constellation schemas. In our approach, we define an equivalence relation over queries using a pre-ordering with respect to which the support is shown to be anti-monotonic. We propose a level-wise algorithm for computing all frequent queries by exploiting the fact that equivalent queries have the same support. We report on experiments showing that, in our context, mining frequent projection-selection-join queries is indeed tractable, even for large data sets.
%X We present an approach for mining frequent conjunctive in arbitrary relational data\-bases. Our pattern class is the simple, but appealing subclass of simple conjunctive queries. Our algorithm, called Conqueror+, is capable of detecting previously unknown functional and inclusion dependencies that hold on the database relations as well as on joins of relations. These newly detected dependencies are then used to prune redundant queries. We propose an efficient database-oriented implementation of our algorithm using SQL, and provide several promising experimental results.
%X Le passage de la radiodiffusion analogique (AM et FM) à l'ère du numérique va entraîner une modification importante des contenus diffusés. L'utilisation des techniques d'indexation audio dans les médias de radiodiffusion commerciale deviendra primordiale pour diverses applications, comme la radio à la demande et la pige musicale. Actuellement, les systèmes de surveillance commerciaux sont des installations massives, en raison du grand nombre de canaux à traiter, et nécessitent la présence d'un opérateur par flux annoté. Les applications grands publiques de la radio à la demande et d'indexation audio offriront de nouvelles possibilités telles que la programmation en fonction de critères (genre musical, évitement de publicités, etc.), ou des informations en temps réel sur les programmes l'ensemble des stations. Pour ces applications, la démodulation de toutes les stations en parallèle est nécessaire. La radio logicielle permet de répondre à ce défi. Initialement orientée vers les radiocommunications, les radars et la gestion opportuniste du spectre, la radio logicielle est maintenant utilisée pour les applications commerciales de la radio aussi. De part son procédé d'acquisition/numérisation au plus près de l'antenne, il est possible d'acquérir l'ensemble des canaux diffusés et de les traiter numériquement pour en restituer des informations sur les flux. L'article présente le développement d'un prototype de "navigateur hertzien" pour la bande FM, utilisant une architecture de récepteur défini par logiciel pour la démodulation simultanée de toutes stations, et un algorithme de reconnaissance des média porté sur un GPU.
%X This paper addresses the design of joint source-channel variable-length codes with maximal free distance for given codeword lengths. While previous design methods are mainly based on bounds on the free distance of the code, the proposed algorithm exploits an exact characterization of the free distance. The code optimization is cast in the framework of mixed-integer linear programming and allows to tackle practical alphabet sizes in reasonable computing time.
%X This paper proposes branch-and-prune algorithms for searching prefix-free joint source-channel codebooks with maximal free distance for given codeword lengths. For that purpose, it introduces improved techniques to bound the free distance of variable-length codes.
%X We aim at equipping the humanoid robot NAO with the capacity of performing expressive communicative gestures while telling a story. Given a set of intentions and emotions to convey, our system selects the corresponding gestures from a gestural database, called lexicon. Then it calculates the gestures to be expressive and plans their timing to be synchronized with speech. After that the gestures are instantiated as robot joint values and sent to the robot in order to execute the hand-arm movements. The robot has certain physical constraints to be addressed such as the limits of movement space and joint speed. This article presents our ongoing work on a gesture model generating co-verbal gestures for the robot while taking into account these constraints.
%X Cet article présente une méthode de segmentation par contours actifs basés histogramme intégrant comme mesure de similarité la famille particulière des alpha-divergences. L'intérêt principal de cette méthode réside (i) dans la flexibilité des alpha-divergences dont la métrique intrinsèque peut-être paramétrisée via la valeur de alpha et donc adaptée aux distributions statistiques des régions de l'image à segmenter ; et (ii) dans la capacité unificatrice de cette mesure statistique vis-à-vis des distances classiquement utilisées dans ce contexte (Kullback- Leibler, Hellinger...). Nous abordons l'étude de cette mesure statistique tout d'abord d'un point de vue supervisé pour lequel le processus itératif de segmentation se déduit de la minimisation de l'alpha -divergence entre la densité de probabilité courante et une référence définie manuellement. Puis nous nous focalisons sur le point de vue non supervisé qui permet de se dédouaner de l'étape de définition des références par le biais d'une maximisation de distance entre les densités de probabilités intérieure et extérieure au contour. Par ailleurs, nous proposons une démarche d'optimisation de l'évolution du paramètre alpha conjointe au processus d'extrémisation de la divergence, permettant d'adapter itérativement la divergence à la statistique des données considérées. Au niveau expérimental, nous proposons une étude comparée des différentes approches de segmentations : en premier lieu, sur des images synthétiques bruitées et texturées, puis, sur des images naturelles. Enfin, nous focalisons notre étude sur différentes applications issues des domaines biomédicaux (microscopie confocale cellulaire) et médicaux (radiographie X) dans le contexte de l'aide au diagnotic. Dans chacun des cas, une discussion sur l'apport des alpha-divergences est proposée.
%X This paper describes a bio-inspired architectural approach to design highly adaptive and reconfigurable systems in the context of mobile robotics. The aim is to design the hardware architecture of an intelligent controller for a robot that exhibits several behaviors such as landscape learning, obstacle avoidance, path planning, sensory-motor control. The concerned robot evolves in an indoor unknown environment and uses the visual informations coming from its input sensors to reach its goal. The Embodied Computing approach presented in this paper is employed in this context to integrate the reconfiguration management as a part of the behavior of the global system. Thus the controller will be able to self-organize its reconfigurable processing elements in order to adapt its architecture to the environment and to the robot actions. We propose in this paper a hardware implementation of the approach based on artificial neural networks. We present the results of our first prototype onto the last technology of FPGA.
%X This paper presents a generic model called TGV for efficient evaluation of XQuery in a heterogeneous distributed system. XQuery is a rich and so a complex language that allows users to express a large scale of queries over XML documents. This expressiveness makes difficulties to obtain an exclusive internal representation within a system. To this purpose, models based on Tree Patterns have been proposed: TPQ [ameryahia01], generalized by the GTP [chen2003]. However, they do not capture well all the expressivity of XQuery, cannot handle mediation problems, and do not support extensible optimisation and sources information.. We present a tree pattern-based model called TGV that - integrates the whole functionalities of XQuery - uses an intuitive representation that provides a global visualization of the request in a mediation context - provides a framework for extensible optimization using a rules definition model - take into account all knowledges useful for the query evaluation (cost model, accuracy, etc.) This work has been implemented in the XLive system \cite {dangngoc2005} based on a full-XML architecture.
%X Ces dernières années ont mis en évidence la croissance et la grande diversité des informations électroniques accessibles sur le web. C'est ainsi que des systèmes d'intégration de données tels que des mé\-dia\-teurs ont été conçus pour intégrer ces données distribuées et hétéro\-gènes dans une vue uniforme. Pour faciliter l'intégration des données à travers dif\-fé\-rents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'interrogation pour XML qui s'est imposé pour les systèmes basés sur XML. Ainsi XQuery est employé sur des systèmes de médiation pour concevoir des vues définies sur plusieurs sources. Pour optimiser l'évaluation de requêtes, les vues sont matérialisées. La difficulté est de maintenir incrémentalement des vues matérialisées lors de la mise à jour des sources, car dans le contexte de sources web, très peu d'informations sont fournies par les sources. Les méthodes habituellement proposées ne peuvent pas être appliquées. Cet article étudie comment mettre à jour des vues matérialisées XML sur des sources web, au sein d'une architecture de médiation.
%X This paper proposes a generic cost framework for query optimization in an XML-based mediation system called XLive, which integrates distributed, heterogeneous and autonomous data sources. Our approach relies on cost annotation on an XQuery logical representation called Tree Graph View (TGV). A generic cost communication language is used to give an XML-based uniform format for cost communication within the XLive system. This cost framework is suitable for various search strategies to choose the best execution plan for the sake of minimizing the execution cost.
%X XQuery is a powerful language defined by the W3C to query XML documents. Its query functionalities and its expressiveness satisfy the major needs of both the database community and the text and documents community. As an inconvenient, the grammar used to define XQuery is thus very complex and leads to several equivalent query expressions for one same query. This complexity often discourages XQuery-based software developers and designers and leads to incomplete XQuery handling. Works have been done in [DPX04] and especially in [Che04] to reduce equiv- alent forms of XQuery expressions into identified "canonical forms". How- ever, these works do not cover the whole XQuery specification. We propose in this paper to extend these works in order to canonize the whole untyped XQuery specification.
%X The pervasive Internet and the massive deployment of sensor devices have lead to a huge heterogeneous distributed system connecting millions of data sources and customers together \cite {franklin2001-challenge}. On the one hand, mediation systems \cite {baru1999,bda2005} using XML as an exchange language have been proposed to federate data accross distributed heterogeneous data sources. On the other hand, work \cite {madden-adhoc,abadi2005,bonnet2001-toward,nath2003-irisnet} have been done to integrate data from sensors. The challenge is now to integrate data coming from both "classical" data (DBMS, Web sites, XML files) and "dynamic" data (sensors) in the context of an ad-hoc network, and finally, to adapt queries and result to match the client profile. We propose to use the TGV model \cite {ibis2007,dasfaa2007} as a mobile agent to query sources across devices (sources and terminal) in the context of a rescue coordination system. This work is integrated in the PADAWAN project.
%X Efficient evaluation of XML Query Languages has become a crucial issue for XML exchanges and integration. Tree Pattern (Sihem et al., 2002; Jagadish et al., 2001; Chen et al., 2003) are now well admitted for represen- ting XML Queries and a model -called TGV (Travers, 2006; Travers et al., 2006; Travers et al., 2007c)- has extended the Tree Pattern representation in order to make it more intuitive, respect full XQuery specification and got support to be manipulated, optimized and then evaluated. For optimization, a search strategy is needed. It consists in generating equivalent execution plan using exten- sible rules and estimate cost of plan to find the better one. We propose the specification of extensible rules that can be used in heterogeneous environment, supporting XML and manipulating Tree Patterns.
%X The pervasive Internet and the massive deployment of sensor devices have lead to a huge heterogeneous distributed system connecting millions of data sources and customers together \cite {franklin2001-challenge}. On the one hand, mediation systems \cite {baru1999,bda2005} using XML as an exchange language have been proposed to federate data accross distributed heterogeneous data sources. On the other hand, work \cite {madden-adhoc,abadi2005,bonnet2001-toward,nath2003-irisnet} have been done to integrate data from sensors. The challenge is now to integrate data coming from both "classical" data (DBMS, Web sites, XML files) and "dynamic" data (sensors) in the context of an ad-hoc network, and finally, to adapt queries and result to match the client profile. We propose to use the TGV model \cite {ibis2007,dasfaa2007} as a mobile agent to query sources across devices (sources and terminal) in the context of a rescue coordination system. This work is integrated in the PADAWAN project.
%X Tree Pattern Queries[sihem2002,jagadish2001] are now well admitted for modeling parts of XML Queries. Actual works only focus on a small subpart of XQuery specifications and are not well adapted for evaluation in a distributed heterogeneous environment. In this paper, we propose the TGV (Tree Graph View) model for XQuery processing. The TGV model extends the Tree Pattern representation in order to make it intuitive, has support for full untyped-XQuery queries, and for optimization and evaluation. Several types of Tree Pattern are manipulated to handle all XQuery requirements. Links between Tree Patterns are called hyperlinks in order to apply transformations on results. The TGV has been implemented in a mediator system called XLive.
%X This paper presents an efficient evaluation of XQuery in a heterogeneous distributed system. XQuery(W3C, 2005) is a rich and so a complex language. Its syntax allows us to express a large scale of queries over XML documents. We have extended (Chen et al., 2003) proposal to rewrite XQuery expressions in "canonical XQuery" in order to support the full XQuery specification. The XQuery expressiveness makes difficulties to obtain an exclusive internal representation within a system. Models based on Tree Patterns have been proposed, and we have extended the tree pattern model to a model called TGV that (a) integrates the whole functionalities of XQuery (b) uses an intuitive representation that provides a global visualization of the request in a mediation context and (c) provides a support for optimization and for cost information. Our paper is based on the XLive mediation system. XLive integrates sources in a uniform view. It is a running research vehicle designed at PRiSM Laboratory for assessing the integration system at every stage of the process starting from sources extraction to the user interface and is already used in several projects.
%X Providing services by integrating information available in web resources is one of the main goals of a mediation architecture. In this paper, we consider the standard wrapper-mediator architecture under the following hypothesis: $(i)$ the information exchanged between wrappers and the mediator consists in XML documents, $(ii)$ wrappers have limited resources, and $(iii)$ to answer queries even if sources are not available, materialized XML views are stored at the mediator level. In this setting, we focus on the problem of maintaining materialized XML views, when the sources change. In our context, wrappers send the updated document without providing any information about the type and the localization of the update in the document. Then, the problems we address are, first, identifying the updates, and, second, updating the view in such a way that accesses to the sources are restricted. Our approach is based on the XAlgebra, which allows to consider XQuery requests on XML documents as relational tables. Moreover, our solution uses identifier annotations for XAlgebra and a {\it diff} function.
%X In this paper, we investigate the behaviors of the Belief Propagation algorithm considered as a dynamic system. In the context of LDPC (Low Den- sity Parity-Check) codes, we use the noise power of the transmission channel as a potentiometer to evaluate the different motions that the BP can follow. The computations of dynamic quantifiers as the bifurcation diagram, the Lyapunov ex- ponent and the reconstructed trajectory enable to bring out four main behaviors. In addition, we propose a novel measure that is the hyperspheres method, which provides the knowledge of the time evolution of the attractor size. The information collected from these different quantifiers helps to better understand the BP evolu- tion and to focus on the noise power values for which the BP suffers from chaos.
%X In this study, we investigate the Belief Propagation (BP) algorithm proposed in [1] as an inference method to estimate the joint probability distribution of a given Bayesian network. Many works have raised up the use of BP in numerous applications as in image processing, computer vision, neural network, statistical physics and channel coding.(...) This study helps to classify four distinguished dynamical be- haviors of the BP along the SNR values. It confirms that the BP is widely infuenced by the topology of the considered LDPC code. The estimators, ei- ther well-known or new, provide qualitative and quantitative descriptions of the algorithm, especially for the complex dynamics.
%X The Belief Propagation (BP) is an inference algorithm used to estimate marginal probability distributions for any Markov Random Field (MRF). In the realm of Low-Density Parity-Check (LDPC) codes that can be represented by MRF called Tanner graphs, the BP is used as a decoding algorithm to estimate the states of bits sent through a noisy channel. Known to be optimal when the Tanner graph is a tree, the BP suffers from suboptimality when the Tanner graph has a loop-like topology. Furthermore, combinations of loops, namely the trapping sets, are particularly harmful for the decoding. To circumvent this problem were proposed other algorithms, like the Generalized Belief Propagation (GBP) that comes from statistical physics. This algorithm allows to absorb topological structures inside new nodes called regions. An advantage is that the resulting graph, the region graph, is not unique then according to its construction this region graph is a media for the GBP that can provide more accurate estimates than the BP. In this paper, we propose novel constructions of the region graph for the famous Tanner code of length N = 155 by making use of the trapping sets as basis for the regions.
%X This paper describes a bio-inspired architectural approach to design highly adaptive and reconfigurable systems in the context of mobile robotics. The purpose is to design the hardware architecture of an intelligent controller for a robot that exhibits several behaviors such as landscape learning, obstacle avoidance, path planning, sensorimotor control. The Embodied Computing approach presented is employed in this context to in- tegrate the reconfiguration management as a part of the behavior of the global system. We propose a hardware implementation of the approach based on artificial neural networks.
%X The paper presents an experiment aiming at studying how an autonomous robot can learn to recognize and classify objects through human-robot interaction. In this experiment, a minimal architecture merging navigation, object recognition, visual tracking, emotional values attribution and human-robot interaction is tested at the Quai Branly museum for 10 days and 4 hours a day. In this experiment, the capability of our architecture to control a mobile robot in a crowded environment with fragile artworks has been validated.
%X In the present study, we propose a model of multimodal place cells merging visual and proprioceptive primitives. First we will briefly present our previous sensory-motor architecture, highlighting limitations of a visual-only based system. Then we will introduce a new model of proprioceptive localization, giving rise to the so-called grid cells, wich are congruent with neurobiological studies made on rodent. Finally we will show how a simple conditionning rule between both modalities can outperform visual-only driven models by producing ro- bust multimodal place cells. Experiments show that this model enhances robot localization and also allows to solve some benchmark problems for real life robotics applications.
%X This paper revisits earlier work on the achievable rate-region for the coded side-information problem. For specific source distributions we provide computable extreme rate points. As opposed to previous works, we present short and concise proofs and additional rate points below the time-sharing line of previously known rate points. Our results are based on a formulation as an optimization problem.
%X The conflict between cooperation in distributed state estimation and the resulting leakage of private state information (competitive privacy) is studied for an interconnected two regional transmission organizations (RTOs) model of the grid. Using an information theoretic rate-distortion-leakage (RDL) tradeoff model, each RTO communicates at a rate chosen to optimize an objective function that is dependent on two opposing quantities: a rate-distortion based pricing function that encourages cooperation, and a leakage function that impedes it. It is shown that strictly non-zero pricing incentives are required to achieve non-trivial target distortions.
%X We analyze the problem of finding the optimal signal covariance matrix for MIMO MAC by using an approach based on "exponential learning", a novel optimization method which applies more generally to (quasi-)convex problems defined over sets of positive-definite matrices (with or without trace constraints). If the channels are static, the system users converge to a power allocation profile which attains the sum capacity of the channel exponentially fast (in practice, within a few iterations); otherwise, if the channels fluctuate stochastically over time (following e.g. a stationary ergodic process), users converge to a power profile which attains their ergodic sum capacity instead. An important feature of the algorithm is that its speed can be controlled by tuning the users' learning rate; correspondingly, the algorithm converges within a few iterations even when the number of users and/or antennas per user in the system is large.
%X The strategic deployment of small cells, overlaid on existing wireless infrastructure is foreseen as a key technologies enabling wireless operators to offer a plethora of innovative technologies to their customers. This, in turn, introduces novel technical challenges arising from the selfish behavior of the customers who ultimately seek to select their preferred wireless service while optimizing the quality-price trade-off of this choice. Consequently, wireless providers have to select the optimal pricing policies which maximize their revenues while anticipating the customers' behavior. In this paper, we provide a game-theoretic model suitable for modeling and analyzing the complex interactions between providers and customers in heterogeneous wireless networks. The proposed game is based on a hybrid model that combines a Stackelberg formulation in which the service provider acts as a leader and the customers, acting as followers, engage in a non-cooperative Wardrop game. For the customers' game, we show the existence and give the analytic expression of the Wardrop equilibria irrespective of the number of providers, services offered and quality of services functions. Then, we propose and study different best response based dynamics at the customers' level. We show that these dynamics converge to the equilibrium if only a portion of the customers are allowed to change strategy at each instant. For characterizing the equilibrium of the Stackelberg game, we propose a novel distributed algorithm that can be adopted by the providers and customers in a practical network. Numerical results show that providers are able to reasonably predict the customers' behavior by evaluating the outcome of the so called Krasnoselskij algorithm, and, subsequently optimize their price.
%X We revisit, in this paper, the well-studied Hawk and Dove game within a dynamic framework. A non-standard evolutionary game approach is taken, in which the starting point of the modeling is the dynamic evolution of the population as a function of the strategies used, instead of a fitness based model (in which the fitness functions determine the evolution). This work is motivated by the discussion in the book of Thomas L. Vincent co-authored with J. Brown [7] in which they raise (on page 73) the puzzling question of whom should one consider to be the players: the individuals or the populations?
%X When a robot is brought into a new environment, it has a very limited knowledge of what surrounds it and what it can do. Either to navigate in the world, or to interact with humans, the robot must be able to learn complex states, using input information from sensors. For navigation task, visual information are commonly used for localisation. Other signals are usually employed: ultrasounds, lasers and path integration are as many data that can be taken into account. For human-robot interactions the proprioceptive information, like the values of the articulations and the grip state, are additional degrees in the system that can to be enrolled in the analysis. All those signals have different dynamics, and the system must manage all those differences. To do so, a solution is to introduce different learning levels. An architecture able to create complex multimodal contexts, to recognise them and to use them afterwords in higher level strategies would give the capacity to resolve such situations. In this paper, a model has been introduced for complex categorisation problems, patterning and chunk learning. It is used in a complex architecture to create multimodal states for a cognitive map and to resolve ambiguities. Tests were performed in both a simulated navigation task and in a complex arm manipulation and localisation experiment.
%X Broadcast radio is a rich but underexploited source of multimedia content. To make this available to users, it will be indispensable to develop new types of navigators capable of searching the large quantities of information contained in the radio bands. The article introduces a prototype of a new software radio enabled broadcast media navigator implemented on an FPGA, which is able to demodulate simultaneously all channel in the FM band and perform audio indexing upon them, ultimately using a Graphics Processing Unit.
%X Broadcast radio is a rich yet underexploited source of multimedia content. To make this content available to users, it will be indispensable to develop new types of navigators capable of searching the large quantities of information contained in the radio bands. The article introduces a prototype of a new software radio enabled broadcast media navigator implemented on a Field Programmable Gate Array and multi-core processor, which is able to demodulate simultaneously all channel in the FM band and perform a real time classification of the musical genre.
%X Non-rigid 3D shape retrieval has become an active and important research topic in content-based 3D object retrieval. The aim of this paper is to measure and compare the performance of state-of-the-art methods for non-rigid 3D shape retrieval. The paper develops a new benchmark consisting of 600 non- rigid 3D watertight meshes, which are equally classified into 30 categories, to carry out experiments for 11 different algorithms, whose retrieval accuracies are evaluated using six commonly utilized measures.
%X As psychologists considered synchrony as an important parameter for social interaction, we hypothesize that in the case of social interaction, people focus their attention on regions of interest where the visual stimuli are synchronized with their inner dynamics. Then, we assume that a mechanism able to detect synchrony between internal dynamics of a robot and external visual stimuli can be used as a starting point for human robot interaction. Inspired by human psychological and neurobiological data, we propose a synchrony based neural network architecture capable of selecting the robot interaction partner and of locating Focus of Attention.
%X With technology and artificial intelligence advancements, the notion of professional service robots has emerged. Consequently, robots must share their physical and social space with human beings. How can robots select a partner among many interactants and how can they focus their attention on regions of interest? As psychologists consider synchrony as an important parameter for social interaction, we hypothesize that in the case of social interaction, people focus their attention on regions of interest where the visual stimuli are synchronized with their inner dynamics. Then, we assume that a mechanism able to detect synchrony between internal dynamics of a robot and external visual stimuli (optical flow) can be used as a starting point for human robot interaction. This kind of mechanism can also be involved in more complex tasks of interaction such as partner selection. Inspired by human psychological and neurobiological data, we propose a synchrony-based neural network architecture capable of selecting the robot partner and of locating focus of attention.
%X Robots are poised to fill a growing number of roles in todays society. In future, we could have robots expected to behave as companion in our home, offices etc. Moving to social robotics imply to address several issues related to human-robot interactions for instance, how the robot can develop an attentional mechanism and select an interacting agent among several interactants. We took our inspiration from neurobiological and psychological studies suggesting that synchrony is an essential parameter for social interaction. We assumed that synchrony detection could be used for intitiating human robot interaction. We present a neural network architecture able to focus the attention of the robot and to select an interacting partner on the basis of synchrony detection.
%X Following the concepts of bio-inspired robotic and a constructivist approach we present here integrated robotic control architectures resulting from a close feedback loop between experiments on animals and robots. Robust control architectures for mobile robot navigation in both indoor and outdoor a priori unknown environment are developped. This also leads to a better understanding of the mechanisms by which the brain processes spatial information. From a computational neuroscience view-point, our control architectures are based on a functional model of hippocampo-cortical interactions implicated when rats solve complex navigation tasks. After a short review of previous models, we highlight the difficulty to scale these control architectures to large environment. We propose to overcome these limitations with a new bio-robotic architecture modeling grid cells.
%X In the present study, we propose a model of multimodal place cells merging visual and proprioceptive primitives. We will briefly introduce a new model of proprioceptive localization, giving rise to the so-called grid cells [Hafting2005], wich are congruent with neurobiological studies made on rodent. Then we show how a simple conditionning rule between both modalities can outperform visual-only driven models. Experiments show that this model enhances robot localization and allows to solve some benchmark problems for real life robotics applications.
%X In this paper, the asymptotic iterative performance of block-Markov, sparse-graph codes over the binary erasure relay channel is investigated. Note that the full-duplex relay channel is a particular case of the considered model. We obtain two interesting results: a) the block-Markov structure does not improve the asymptotic performance of good erasure-correcting sparse-graph codes; b) under certain conditions, it does however improve the asymptotic performance of good erasure-detecting (i.e. bad erasure-correcting) sparse-graph codes.
%X This paper studies coding schemes for the $q$-ary symmetric channel based on binary low-density parity-check (LDPC) codes that work for any alphabet size $q=2^m$, $m\in\mathbb{N}$, thus complementing some recently proposed packet-based schemes requiring large $q$. First, theoretical optimality of a simple layered scheme is shown, then a practical coding scheme based on a simple modification of standard binary LDPC decoding is proposed. The decoder is derived from first principles and using a factor-graph representation of a front-end that maps $q$-ary symbols to groups of $m$ bits connected to a binary code. The front-end can be processed with a complexity that is linear in $m=\log_2 q$. An extrinsic information transfer chart analysis is carried out and used for code optimization. Finally, it is shown how the same decoder structure can also be applied to a larger class of $q$-ary channels.
%X In this paper, we study the impact of the cognitive map's adaptation in the context of multi-robot system. This map governs the emergence of non-trivial behaviors and structures at both individual and social levels. In particular, we show that adding a simple imitation and deposit behavior allows the cognitive robots to adapt themselves in unknown environment to solve different navigation tasks. We show that in our architecture the individual discoveries in each robot (i.e., goals) can have an effect at the population level, which induce then a new learning at the individual level and reciprocally, from the individual to the population level. We performed a series of experimentations with robots and simulated agents to validate our system.
%X In this paper, we present the interest of low level imitation strategy on individual and population levels in the field of Multi-Robot System. Particularly, we show that adding a simple imitation capability to our bio-inspired architecture boosts the ability of individual cognitive map building. Taking into account the notion of imitative behavior, we also show that the individual discoveries in each robot (i.e. goals) could have an effect on population level and therfore it induces a new learning capability at the individual level. To analyze and validate our hypothesis, a series of experiments have been performed with and without a low level imitation strategy.
%X This demonstration presents a digital media retrieval system for searching large categories in different media databases. The core of our system is an interactive online classification based on user labeling. The classification is obtained with a statistical learning method: kernels for similarity represen- tation and SVM (Support Vector Machine) using binary user annotations. RETIN applies also an active learning strategy for proposing documents to the user for labeling. The sys- tem can deal with images, 3D objects and videos and other media can be added to. A graphical user interface allows easy browsing of different media, simple and user-friendly interaction and fast retrieval.
%X In this paper, we propose a novel algorithm to design multi- class kernels based on an iterative combination of weak kernels in a schema inspired from the boosting framework. Our solution has a complexity lin- ear with the training set size. We evaluate our method for classification on a toy example by integrating our multi-class kernel into a kNN clas- sifier and comparing our results with a reference iterative kernel design method. We also evaluate our method for image categorization by con- sidering a classic image database and comparing our boosted linear kernel combination with the direct linear combination of all features in a linear SVM.
%X In this paper, we propose a compact image signa- ture based on VLAT. Our method integrates spatial in- formation while significantly reducing the size of orig- inal VLAT by using two pojection steps. we carry out experiments showing our approach is competitive with state of the art signatures.
%X In this paper, we are interested in resource allocation strategies for wireless time-hopping ultra-wide band (TH-UWB) communications with multiple rate capabilities between users. Multiple rates are achieved by assigning different processing gains, i.e. $N_f$, to users. For this purpose, the multiple-access interference (MAI) variance accounting for multi-rate is needed. It is a challenging task due to the lack of a suitable closed-form expression for the MAI variance in a multi-rate context. We further study the multi-rate resource allocation problem in uplink TH-UWB systems for which an optimal search cannot be envisaged due to the exponential complexity induced. Our contribution lies in three-fold: i) A new intercode correlation expression accounting for multi-rate communications is derived, and the variance of the MAI averaging over the codes is obtained. ii) The multi-rate resource allocation problem is tackled by relaxing the integer constraint on the processing gains and modeled via a signomial programming problem. iii) Based on this, a branch and bound (BB) algorithm is derived for the allocation of the processing gains in TH-UWB systems. We also propose a really simple heuristic with linear complexity for the $N_f$ allocation. We show that the algorithm proposed outperforms the BB algorithm in average throughput and average starvation rate.
%X Seeing is not just done through the eyes, it involves the integration of other modalities such as auditory, proprio- ceptive and tactile information, to locate objets, persons and also the limbs. We hypothesize that the neural mechanism of gain-field modulation, which is found to process coordinate transform between modalities in the superior colliculus and in the parietal area, plays a key role to build such unified perceptual world. In experiments with a head-neck-eye's robot with a camera and microphones, we study how gain-field modulation in neural networks can serve for transcribing one modality's reference frame into another one (e.g., audio signals into eyes' coordinate). It follows that each modality influences the estimations of the position of a stimulus (multimodal enhancement). This can be used in example for mapping sound signals into retina coordinates for audio-visual speech perception.
%X En allant à rebours de l'approche " penser, c'est calculer ", qui a été l'axe principal de la recherche en intelligence artificielle jusque dans les années 1980, " La Révolution de l'intelligence du corps développe l'idée que l'intelligence a besoin d'un corps " pour interagir avec l'environnement, selon le contexte et l'instant. Cette proposition a priori simple -- mais qui est une nouvelle façon de voir l'intelligence -- a des implications fortes pour qui veut comprendre les organismes vivants dans leur complexité, mais aussi pour qui veut concevoir des robots plus proches des organismes vivants dans leur façon de fonctionner. A partir d'exemples pris en neurobiologie, en biomécanique, dans les sciences cognitives et les sciences de développement, ce livre explore les principes et les mécanismes qui permettent de définir cette nouvelle approche de l'intelligence : ceux du système d'apprentissage de l'homme et de l'enfant -- le plus évolué au monde --, ainsi que les développements récents empruntés par la nouvelle intelligence artificielle pour créer des systèmes intelligents qui leur ressemblent. Cette vision novatrice de l'intelligence incarnée dans un corps nous donne aussi certaines clés pour comprendre l'ambiguïté de la mémoire, l'interdépendance de la perception et de l'action, la coordination entre le corps et le centre nerveux, l'auto-organisation dans le développement et dans les processus génétiques, et l'inter-subjectivité dans les interactions sociales.
%X We introduce a generic approach for improving the guaranteed error correction capability of regular low-density parity check codes. The method relies on operating (in serial or in parallel) a set of ﬁnite alphabet iterative decoders. The message passing update rules are judiciously chosen to ensure that decoders have different dynamics on a speciﬁc ﬁnite-length code. The idea is that for the Binary Symmetric Channel, if some error pattern cannot be corrected by one particular decoder, there exists in the set of decoders, another decoder which can correct this pattern. We show how to select a plurality of message update rules so that the set of decoders can collectively correct error patterns on the dominant trapping sets. We also show that a set of decoders with dynamic re-initializations can approach the performance of maximum likelihood decoding for ﬁnite-length regular column-weight three codes.
%X In this paper, we propose a system based on non- binary Low-Density Parity-Check (LDPC) codes to communicate efﬁciently over the multiple-relay fading channels, with a simple joint decoding strategy at the receiver end. The particularity of our approach is to rely on non-binary LDPC codes at the source, coupled with multiplicative non-binary local codes at the relays, such that the joint decoding complexity is not increased compared to a system without relays, while preserving the coding gain brought by the re-encoding of the sequence at the relays. We show by simulations on simple conﬁgurations that this cooperative scheme is superior to other techniques proposed in the literature, and close to the Gaussian relay channel capacity, even at moderate codeword lengths.
%X In this paper we analyze the performance of several bit-interleaving strategies applied to Non-Binary Low-Density Parity-Check (LDPC) codes over the Rayleigh fading channel. The technique of bit-interleaving used over fading channel introduces diversity which could provide important gains in terms of frame error probability and detection. This paper demonstrates the importance of the way of im- plementing the bit-interleaving, and proposes a design of an optimized bit-interleaver inspired from the Progressive Edge Growth algorithm. This optimization algorithm depends on the topological structure of a given LDPC code and can also be applied to any degree distribution and code realization. In particular, we focus on non-binary LDPC codes based on graph with constant symbol-node connection dv = 2. These regular (2, dc)-NB-LDPC codes demonstrate best performance, thanks to their large girths and improved decoding thresholds growing with the order of Finite Field. Simulations show excellent results of the proposed interleaving technique compared to the random interleaver as well as to the system without interleaver. Index Terms − Non-binary LDPC codes, bit-interleaver, Rayleigh fading channel, Tanner Graph.
%X In the context of channel coding, the Generalized Belief Propagation (GBP) is an iterative algorithm used to recover the transmission bits sent through a noisy channel. To ensure a reliable transmission, we apply a map on the bits, that is called a code. This code induces artiﬁcial correlations between the bits to send, and it can be modeled by a graph whose nodes are the bits and the edges are the correlations. This graph, called Tanner graph, is used for most of the decoding algorithms like Belief Propagation or Gallager-B. The GBP is based on a non unic transformation of the Tanner graph into a so called region-graph. A clear advantage of the GBP over the other algorithms is the freedom in the construction of this graph. In this article, we explain a particular construction for speciﬁc graph topologies that involves relevant performance of the GBP. Moreover, we investigate the behavior of the GBP considered as a dynamic system in order to understand the way it evolves in terms of the time and in terms of the noise power of the channel. To this end we make use of classical measures and we introduce a new measure called the hyperspheres method that enables to know the size of the attractors.
%X This paper continues a previous work on non-binary cluster-LDPC codes. Such codes are deﬁned by locally dense parity-check matrices, with (possibly rectangular) dense clusters of bits, but which are cluster-wise sparse. We derive a lower bound on the minimum distance of non-binary cluster-LDPC codes that is based on the topological properties of the underlying bipartite graph. We also propose an optimization procedure, which allows designing ﬁnite length codes with large minimum distance, even in the extreme case of codes deﬁned by ultra-sparse graphs, i.e. graphs with all symbol-nodes of degree dv = 2. Furthermore, we provide asymptotic thresholds of ensembles of non-binary cluster-LDPC codes, which are computed exactly under the Belief Propagation decoding, and upper-bounded under the Maximum a Posteriori (MAP) decoding. We show that the MAP-threshold upper bounds, which are conjunctured to be tight, quickly approach the channel capacity, which conﬁrms the excellent minimal distance properties of non-binary cluster- LDPC codes.
%X Finite alphabet iterative decoders (FAIDs) for LDPC codes were recently shown to be capable of surpassing the Belief Propagation (BP) decoder in the error ﬂoor region on the Binary Symmetric channel (BSC). More recently, the technique of decimation which involves ﬁxing the values of certain bits during decoding, was proposed for FAIDs in order to make them more amenable to analysis while maintaining their good performance. In this paper, we show how decimation can be used adaptively to further enhance the guaranteed error correction capability of FAIDs that are already good on a given code. The new adaptive decimation scheme proposed has marginally added complexity but can signiﬁcantly improve the slope of the error ﬂoor performance of a particular FAID. We describe the adaptive decimation scheme particularly for 7-level FAIDs which propagate only 3-bit messages and provide numerical results for column-weight three codes. Analysis suggests that the failures of the new decoders are linked to stopping sets of the code.
%X In this paper, we propose a ﬂexible coding scheme that achieves full diversity over the block fading channel. The particularity of our approach is to rely on non-binary LDPC codes coupled with multiplicative non-binary codes, so that to easily adapt the coding rate to the number of fading blocks. A simple combining strategy is used at the receiver end before the iterative decoding. As a consequence, the decoding complexity is the same, irrespective of the number of fading blocks, while the proposed technique brings an effective coding gain.
%X In this paper, we propose a cooperative coding scheme to communicate efﬁciently over multiple-relay fading channels. The particularity of our approach is to rely on non-binary LDPC codes at the source, coupled with non-binary repeti- tion codes at the relays. A simple joint decoding strategy is used at the receiver end, so that the decoding complexity is not increased compared to a system without relays, while pre- serving the coding gain brought by re-encoding the signal at the relays. We show by simulations that the proposed scheme allows maintaining a constant gap to the outage probability of the cooperative system, irrespectively of the number of relays.
%X This paper considers an iterative algorithm called the Interval-Passing Algorithm (IPA) which is used to reconstruct non-negative real signals using binary measurement matrices in compressed sensing (CS). The failures of the algorithm on stopping sets, also non-decodable conﬁgurations in iterative decoding of LDPC codes over the binary erasure channel (BEC), shows a connection between iterative reconstruction algorithm in CS and iterative decoding of LDPC codes over the BEC. In this paper, a stopping-set based approach is used to analyze the recovery of the IPA. We show that a smallest stopping set is not necessarily a smallest conﬁguration on which the IPA fails and provide sufﬁcient conditions under which the IPA recovers a sparse signal whose non-zero values lie on a subset of a stopping set. Reconstruction performance of the IPA using IEEE 802.16e LDPC measurement matrices are provided to show the effect of the stopping sets in the performance of the IPA.
%X We introduce a generic approach, called FAID diversity, for improving the error correction capability of regular low-density parity check codes, beyond the belief propagation performance. The method relies on operating a set of ﬁnite alpha- bet iterative decoders (FAID). The message-passing update rules are interpreted as discrete dynamical systems, and are judiciously chosen to ensure that decoders have different dynamics on a speciﬁc ﬁnite-length code. An algorithm is proposed which uses random jumps in the iterative message passing trajectories, such that the system is not trapped in periodic attractors. We show by simulations that the FAID diversity approach with random jumps has the potential of approaching the performance of maximum- likelihood decoding for ﬁnite-length regular, column-weight three codes.
%X The Belief Propagation (BP) is an inference algorithm used to estimate marginal probability distributions for any Markov Random Field (MRF). In the realm of Low-Density Parity-Check (LDPC) codes that can be represented by MRF called Tanner graphs, the BP is used as a decoding algorithm to estimate the states of bits sent through a noisy channel. Known to be optimal when the Tanner graph is a tree, the BP suffers from suboptimality when the Tanner graph has a loop-like topology. Furthermore, combinations of loops, namely the trapping sets, are particularly harmful for the decoding. To circumvent this problem were proposed other algorithms, like the Generalized Belief Propagation (GBP) that comes from statistical physics. This algorithm allows to absorb topological structures inside new nodes called regions. An advantage is that the resulting graph, the region graph, is not unique then according to its construction this region graph is a media for the GBP that can provide more accurate estimates than the BP. In this paper, we propose novel constructions of the region graph for the famous Tanner code of length N = 155 by making use of the trapping sets as basis for the regions.
%X We propose nonbinary LDPC codes concatenated with multiplicative repetition codes. By multiplicatively repeating the (2,3)-regular nonbinary LDPC mother code of rate 1/3, we con- struct rate-compatible codes of lower rates . Surprisingly, such simple low-rate nonbinary LDPC codes out- perform the best low-rate binary LDPC codes so far. Moreover, we propose the decoding algorithm for the proposed codes, which can be decoded with almost the same computational complexity as that of the mother code.
%X We study fountain codes transmitted over the binary-input symmetric-output channel. For channels with small capacity, receivers in fountain coding systems needs to collects many channel outputs to recover information bits. Since a collected channel output yields a check node in the decoding Tanner graph, the channel with small ca- pacity leads to large decoding complexity. In this paper, we introduce a novel fountain coding scheme with non-binary LDPC codes. The decoding complexity of the proposed fountain code does not depend on the channel. Numerical experiments show that the proposed codes exhibit better performance than conventional fountain codes, especially for moderate number of information bits.
%X This paper addresses the spectrum-sharing for wireless communication with a licensee-primary and a secondary users interfering with each other. We propose a new practical power control policy for the secondary user, under outage performance requirements for both users, with partial knowledge of channel state information
%X This article presents a new embeddable method for polyp detection in endoscopic video images and wireless capsule endoscopic images based on physician approach. This approach is twofold: the first step consists in a geometric approach to characterize the polyp's geometric features as size or shape. The second, a texture approach based on a learning process of texture features of polyps using the use of the co-occurrence matrix and their related statistics. For classification, we propose the boosting method which allows us to generate a strong classifier and also the possibility of permanent learning. The performance of the learning approach over a database of 300 images, is characterized by a sensibility of 90,6%, a specificity of 95,0% and a false detection rate of 4,8%. All the different steps of the algorithm were meticulously chosen to facilitate the future hardware implementation of the treatment on the capsule for wireless endoscopy.
%X Most of the robot behaviors are based on visual sensing and perception. This paper describes a smart camera composed of a full-hardware vision architecture coupled with an embedded camera sensor. The hardware architecture corresponds to low-level visual perception processes. The integration of such a system onto the robot enables not only to accelerate the visual processing till real-time behavior but also to compress the data-flow at the output of the camera. The results obtained during indoor robotic missions show an important reduction factor of data communication.
%X Recent FPGAs allow the design of efficient and complex Heterogeneous Systems-on-Chip (HSoC). Namely, these systems are composed of several processors, hardware accelerators as well as communication media between all these components. Performances provided by HSoCs make them really interesting for data-flow applications, especially image processing applications. The use of this kind of architecture provides good performances but the drawback is an increase of the programming complexity. This complexity is due to the heterogeneous deployment of the application on the platform. Some functions are implemented in software to run on a processor, whereas other functions are implemented in hardware to run in a reconfigurable partition of the FPGA. This article aims to define a programming model based on the Synchronous Data-Flow model, in order to abstract the heterogeneity of the implementation and to leverage the communication issue between software and hardware actors.
%X Numerous inference problems in statistical physics, computer vision or error-correcting coding theory consist in approximating the marginal probability distributions on Markov Random Fields (MRF). The Belief Propagation (BP) is an accurate solution that is optimal if the MRF is loop free and suboptimal otherwise. In the context of error-correcting coding theory, any Low-Density Parity-Check (LDPC) code has a graphical representation, the Tanner graph, which is a particular MRF. It is used as a media for the BP algortithm to correct the bits, damaged by a noisy channel, by estimating their probability distributions. Though loops and combination thereof in the Tanner graph prevent the BP from being optimal, especially harmful topological structures called the trapping- sets. The BP has been extended to the Generalized Belief Propagation (GBP). This message-passing algorithm runs on a non unique mapping of the Tanner graph, namely the region- graph, such that its nodes are gatherings of the Tanner graph nodes. Then it appears the possibility to decrease the loops effect, making the GBP more accurate than the BP. In this article, we expose a novel region graph construction suited to the Tanner code, an LDPC code whose Tanner graph is entirely covered by trapping-sets. Furthermore, we investigate the dynamic behavior of the GBP compared with that of the BP to understand its evolution in terms of the Signal-to-Noise Ratio (SNR). To this end we make use of classical estimators and we introduce a new one called the hyperspheres method.
%X In this article, we present two studies that pave the way towards a mobile implementation of the WAAVES certified medical image compression encoder. On the algorithmic side, we compared three techniques to increase the compression rate. The obtained results show a significant bit-rate reduction, around 40% with respect to the WAAVES encoder, while keeping the same visual quality. On the architectural side, we describe the HW/SW co-design of an architecture implemented in a FPGA platform. By using code profiling, critical portions of the code were identified, then two methods for hardware acceleration were used to implement the critical part of the coder. The tests were done on a StratixIVGX230 FPGA and the results showed that HW/SW co-design could achieve up to 20x performance gain in the critical portion. The combination of these results demonstrates the feasibility of a mobile implementation of the WAAVES certified medical image coder suitable for e-health applications.
%X We generalize a construction of non-binary quantum LDPC codes over F2m due to [KHIK11] and apply it in particular to toric codes. We obtain in this way not only codes with better rates than toric codes but also improve dramatically the performance of standard iterative decoding. Moreover, the new codes obtained in this fashion inherit the distance properties of the underlying toric codes and have therefore a minimum distance which grows as the square root of the length of the code for fixed m.
%X In this paper, the asymptotic iterative performance of block-Markov, sparse-graph codes over the binary erasure relay channel is investigated. Note that the full-duplex relay channel is a particular case of the considered model. We obtain two interesting results: a) the block-Markov structure does not improve the asymptotic performance of good erasure-correcting sparse-graph codes; b) under certain conditions, it does however improve the asymptotic performance of good erasure-detecting (i.e. bad erasure-correcting) sparse-graph codes.
%X This paper deals with the design of low-rate sparse-graph codes, having a linear minimum distance d_{min} in the blocklength n. Its main contributions are: a) a necessary condition on a general family of sparse-graph codes with linear d_{min}; b) a justification of having degree-1 bits in the low-rate code structure; c) a new, efficient ensemble of low-rate sparse-graph codes with bits of degree 1, designed so that the necessary condition (a) is satisfied.
%X We propose here a new construction of spatially coupled quantum LDPC codes using a small amount of entangled qubit pairs shared between the encoder and the decoder which improves quite significantly all other constructions of quantum LDPC codes and turbo codes with the same rate.
%X The IRIM group is a consortium of French teams work- ing on Multimedia Indexing and Retrieval. This paper describes its participation to the TRECVID 2012 se- mantic indexing and instance search tasks. For the semantic indexing task, our approach uses a six-stages processing pipelines for computing scores for the likeli- hood of a video shot to contain a target concept. These scores are then used for producing a ranked list of im- ages or shots that are the most likely to contain the tar- get concept. The pipeline is composed of the following steps: descriptor extraction, descriptor optimization, classi cation, fusion of descriptor variants, higher-level fusion, and re-ranking. We evaluated a number of dif- ferent descriptors and tried di erent fusion strategies. The best IRIM run has a Mean Inferred Average Pre- cision of 0.2378, which ranked us 4th out of 16 partici- pants. For the instance search task, our approach uses two steps. First individual methods of participants are used to compute similrity between an example image of in- stance and keyframes of a video clip. Then a two-step fusion method is used to combine these individual re- sults and obtain a score for the likelihood of an instance to appear in a video clip. These scores are used to ob- tain a ranked list of clips the most likely to contain the queried instance. The best IRIM run has a MAP of 0.1192, which ranked us 29th on 79 fully automatic runs.
%X In OFDMA, the total available subcarriers are assigned to different users for simultaneous transmission. In case of an unsynchronized uplink OFDMA, each user has a different carrier frequency offset (CFO), which results in loss of orthogonality among subcarriers and causes severe intercarrier interference. In this paper, a self-successive interference canceller (self-SIC) algorithm is proposed for uplink OFDMA systems to mitigate both the self-intercarrier interference and multiuser interference resulting from carrier frequency offsets. We show that for a carrier allocation with larger blocksizes, the proposed compensation algorithm significantly reduces the implementation complexity without performance degradation.
%X This paper presents our approach to select relevant sequences from raw videos in order to generate summaries to Trecvid 2008 BBC Rush Task. Our system is composed of two major steps: First, the system detects \semantic" shot boundaries and keeps only non-redundant shots; then, the system esti- mates average motion for each shot, as a criterion of amount of information, to better share out the duration of the sum- mary between remaining shots. The rst step is based on a fast near-duplicate retrieval using Locality Sensitive Hashing (LSH) which provides results in few seconds (if we do not take into account decoding and encoding processes). The evaluation of Trecvid shows very promising results, since we ranked 17th over 43 runs, regarding redundancy measure (RE), and 18th for object and event inclusion (IN). These balanced results (most of best teams for the rst criterion are among the latest for the second one) show that our method o ers a quite good trade-o between false negatives (IN) and false positives (RE).
%X In content based image retrieval, the success of any distance-based indexing scheme depends critically on the quality of the chosen distance metric. We propose in this paper a kernel-based similarity approach working on sets of vectors to represent images. We introduce a method for fast approximate similarity search in large image databases with our kernel-based similarity metric. We evaluate our algorithm on image retrieval task and show it to be accurate and faster than linear scanning.
%X In the last two years, web-based applications us- ing street-level images have been developing fast. In that context, privacy preservation is an unavoidable issue. We present in this paper a multi-boosting based approach to detect pedestrians in high resolution panoramics in order to blur their faces. This task is quite complex since these features vary in size, shape, color, and often are partially occluded, sometimes be- hind windows or inside cars, etc. Our strategy is thus based on the combination of two existing boosting algo- rithms detecting faces [1] and bodies [2] with a skin tone detection algorithm we developed. The results are quite encouraging for such an unconstrained data: 86:2% of true positives and an average of 2 false positive de- tections per image (2.1 MPixels). This combination solution provides much more robust results than each detection algorithm performed independently.
%X In video surveillance systems, when dealing with dynamic complex scenes, processing the information coming from multiple cameras and fusing them into a comprehensible environment is a challenging task. This work addresses the issue of providing a global and reliable representation of the monitored environment aiming at enhancing the perception and minimizing the operator's effort. The proposed system Virtu4D is based on 3D computer vision and virtual reality techniques and takes benefit from both the "real" and the "virtual" worlds offering a unique perception of the scene. This paper presents a short overview of the framework along with the different components of the design space: Video Model Layout, Video Processing and Immersive Model Generation. The final interface gathers the 2D information in the 3D context but also offers a complete 3D representation of the dynamic environment allowing a free intuitive 3D navigation.
%X Classical models of emotions consider either the communicational aspect of emotions (for instance the emotions conveyed by the facial expressions) or the second order control necessary for survival purpose when the autonomy of the system is an issue. Here, we show the interdependence of communication and meta-control aspects of emotion. We propose the idea that emotions must be understood as a dynamical system linking two controllers: one devoted to social interactions (i.e. communication aspects) and another one devoted to the interactions within the physical world (i.e metacontrol of a more classical controller). Illustrations will be provided from applications involving navigation among di erent goal places according to di erent internal drives or object grasping and avoidance.
%X This paper presents a method to virtualize the communications into a distributed heterogeneous embedded Multiprocessor Systems-on-Chip (MPSoC) platform containing reconfigurable hardware computing units. We propose a new concept of middleware, implemented in software and in hardware to provide the designer a single programming interface. The middleware offers some mechanisms like the accesses to distant operating system (OS) services and the interprocess communication. It abstracts both implementation and mapping. The embedded application then executes regardless of where or how processes are implemented. We are currently validating the concept on a real-time image processing application.
%X Actual dynamic applications, executed on real-time systems, have the tendency to be built on dynamically reconfigurable hardware devices. These applications require high performance and flexibility towards user and environment needs. To perform these application requirements, efficient mechanisms to manage hardware device must exist. In this paper we target OLLAF as a dynamically reconfigurable architecture which is designed to support complex and flexible applications. In order to deal with all of the dynamic aspects of such systems, we describe a predictive scheduling allowing an early estimation of our application dynamicity. A vision system of a mobile robot and an application of 3D synthesis images were served to validate the presented scheduling approach.
%X Modeling techniques are used to solve a variety of practical problems related to processing and scheduling in several domains like manufacturing and embedded systems. In such flexible and dynamic environments, there are complex constraints and a variety of unexpected disruptions. Hence, scheduling remains a complex and time-consuming process and the need for accurate models for optimizing this process is increasing. This paper deals with dynamically reconfigurable architectures which are designed to support complex and flexible applications. It focuses on defining a solution approach for modeling such applications where there is significant uncertainty in the duration, resource requirements and number of tasks to be executed.
%X This article describes a real-time architecture for Hough Transform based on CORDIC algorithm and gradient for straight lines detection. The CORDIC algorithm allows the use of simple operators such as adders and shift registers whereas the use of gradients reduces considerably the computation quantity. The proposed real-time architecture can be easily ¯tted into FPGA (Field Programmable Gate Array) re- con¯gurable devices since the present performance increase of this technology allows the implementation of complex applications while real-time constraints for Hough Transform based lines detection are respected for most of the video transmission standards. This approach can be easily applied for other shapes detection.
%X In this paper, we design explicit codes for strong coordination in two-node networks. Specifically, we consider a two-node network in which the action imposed by nature is binary and uniform, and the action to coordinate is obtained via a symmetric discrete memoryless channel. By observing that polar codes are useful for channel resolvability over binary symmetric channels, we prove that nested polar codes achieve a subset of the strong coordination capacity region, and therefore provide a constructive and low complexity solution for strong coordination.
%X The closest vector problem (CVP) and shortest (nonzero) vector problem (SVP) are the core algorithmic problems on Euclidean lattices. They are central to the applications of lattices in many problems of communications and cryptography. Kannan's \emph{embedding technique} is a powerful technique for solving the approximate CVP, yet its remarkable practical performance is not well understood. In this paper, the embedding technique is analyzed from a bounded distance decoding} (BDD) viewpoint. We present two complementary analyses of the embedding technique: We establish a reduction from BDD to Hermite SVP (via unique SVP), which can be used along with any Hermite SVP solver (including, among others, the Lenstra, Lenstra and Lovasz (LLL) algorithm), and show that, in the special case of LLL, it performs at least as well as Babai's nearest plane algorithm (LLL-aided SIC). The former analysis helps to explain the folklore practical observation that unique SVP is easier than standard approximate SVP. It is proven that when the LLL algorithm is employed, the embedding technique can solve the CVP provided that the noise norm is smaller than a decoding radius lambda_1/(2 gamma), where lambda_1 is the minimum distance of the lattice, and gamma is of the order O(2^(n/4)). This substantially improves the previously best known correct decoding bound of the order O(2^n). Focusing on the applications of BDD to decoding of multiple-input multiple-output (MIMO) systems, we also prove that BDD of the regularized lattice is optimal in terms of the diversity-multiplexing gain tradeoff (DMT), and propose practical variants of embedding decoding which require no knowledge of the minimum distance of the lattice and/or further improve the error performance.
%X This paper describes a novelmethod for shape representation and robust image segmentation. The proposed method combines two well known methodologies, namely, statistical shape models and active contours implemented in level set framework. The shape detection is achieved by maximizing a posterior function that consists of a prior shape probability model and image likelihood function conditioned on shapes. The statistical shape model is built as a result of a learning process based on nonparametric probability estimation in a PCA reduced feature space formed by the Legendre moments of training silhouette images. A greedy strategy is applied to optimize the proposed cost function by iteratively evolving an implicit active contour in the image space and subsequent constrained optimization of the evolved shape in the reduced shape feature space. Experimental results presented in the paper demonstrate that the proposed method, contrary to many other active contour segmentation methods, is highly resilient to severe random and structural noise that could be present in the data.
%X This article deals with the social behavior modeling of a particular forest pest insect: the bark beetle. This ant-like insect has been responsible for the devastation of acres of pines trees in North America since 2005. Any tactic of forest pest management requiring prediction of pest population change over time and/or space, a realistic modeling of beetle colonies behavior would be a real benefit. The originality of this work is to propose a reactive Multi-Agent- System integrating physical diffusion phenomena. The main idea is to take into account the natural vanishing of the trail markers emitted both by decomposing trees (ethanol) and the agents that have found a source of food (pheromone). The proposed experiments show, on the one hand, that the MAS-PDE modeling leads to a realistic global behavior of the colony when considering a usual foraging scenario and, on the other hand, that, when compared with a simple reactive agent, the proposed model has a faster convergence to the asymptotic usual expected "S-shape" behavior of the agents' colony.
%X We propose in this paper a low-cost method of Electrical Impedance Tomography (EIT) data acquisition from soft conductive fabric for the design of robots artificial skin. We use a simple multiplexer/demultiplexer circuit for retrieving the resistance field from the pair-wised electrical current injected and the output voltage measured from the conductive fabric. A microcontroller controls the current injection and voltage output patterns and the analog-to-numeric conversion from the tactile material. After explaining the EIT method, we present the electronics corresponding to the data acquisition and we analyze the material property. Results show that we can acquire and localize in real time spatial patterns of the tactile contact.
%X In evolutionary robotics, plastic neural network models proved to be promising for evolving adaptive behaviors. In particular, neurocontrollers incorporating hebbian synapses have been shown to be useful for implementing conflicting sub-behaviors. Numerous interesting complex tasks assume such flexibility. However, those evolved controllers often exhibit behavioral instability, as simulation time is extended beyond the short limit used during evolution. In this paper, we propose constrained plastic models inspired by neural homeostasis phenomena, in order to evolve flexible and stable pattern generators for single-legged locomotion. Comparative results show that constrained controllers perform better than unconstrained ones in both terms of evolvability and behavioral stability. Functional analyses of the best evolved controller unveil the adaptivity, robustness and homeostasis arising from the statically constrained plasticity. Interestingly, homeostasis evolved implicitly without relying on any active homeostatic mechanisms and is implemented through hebbian plasticity, usually considered destabilizing
%X Cloud computing provides access to "infinite" storage and computing resources, offering promising perspectives for many applications (medicine, nuclear physics, meteorology, etc.). However, this new paradigm requires rethinking of database management principles in order to allow deployment on scalable and easy to access infrastructures, applying a pay-as-you-go model. This position paper introduces building blocks to provide cost-aware semantic caching. To this end, we first introduce cost models for data management in the cloud, then we present a semantic caching framework providing finely tuned caches for different data analysis systems. This semantic caching framework is then discussed in the context of our previous work on rewriting rules and cache management for OLAP queries. Finally, it discusses the problem of query evaluation in the cloud in presence of semantic caches as a multi-criteria optimization problem.
%X In this paper we present algorithms for recommender systems. Our algorithms rely on a semantic relevance measure and a social network analysis measure to partially explore the network using depth-fi rst search and breath-fi rst search strategies. We apply these algorithms to a real data set and we compare them with item-based collaborative fi ltering and hybrid recommendation algorithms. Our experiments show that our algorithms outperform existing recommendation algorithms, while providing good precision and F-measure results.
%X We present an information system supporting the diagnosis and treatment of rice diseases. The heart of the system is a software mediator allowing farmers to formulate distant queries regarding a disease and responding to farmer queries by recommending a treatment of the disease if one exists. The disease diagnosis and the recommended treatment are based on expert knowledge stored in the mediator database. The processing of farmer queries may involve direct access to the mediator by farmers, online communication between the mediator and the experts, or direct dialog between the farmer and an expert. Our information model is generic in the sense that it can be used to support the needs of farmers in a variety of similar environments, with minor changes. The system presented in this paper is currently under development as part of a Franco-Thai project and aims to assist farmers in the quick diagnosis of rice diseases and their treatment.
%X In this paper we present two recommendation algorithms, called Node-Edge-Based and Node-Based recommendation algorithms. These algorithms are designed to recommend items to users connected via social network. Our algorithms are based on three main features: a social network analysis measure (degree centrality), the graph searching algorithm (Depth First Search algorithm), and the semantic similarity measure (which measures the closeness between the input item and users). We apply these algorithms to a real dataset (Amazon dataset) and we compare them with item based collaborative filtering and hybrid recommendation algorithms. Our results show good precision as well as in a good performance in terms of runtime. Moreover, Node-Edge-Based and Node-Based algorithms search a small part of the dataset, compared to item-based and hybrid recommendation algorithms.
%X It is now common practice to address queries to Distributed Heterogeneous Information Systems (DHIS). In such a setting, the issue of query optimization becomes crucial, and more complex than in centralized homogeneous approaches. Indeed, the optimization processing must be as flexible as possible so as to apply to different database models, and integrate different cost models. In this paper, we present a generic framework for query optimization in the context of DHISs, with the goal of facilitating the implementation of efficient query optimizers. To this end, we identify all necessary components for building such a query optimizer and we define the basic functions that have to be implemented. Moreover, we report on experiments showing that our approach allows for an efficient query optimization in the context of DHISs.
%X Dans le contexte de la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement liés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. Dans cet article, nous proposons un Framework basé sur des itemsets pour la construction de variables à partir des tables secondaires. L'informativité de ces nouvelles variables est évaluée dans le cadre de la classification supervisée au moyen d'un critère régularisé qui vise à éviter le surapprentissage. Pour ce faire, nous introduisons un espace de modèles basés sur des itemsets dans la table secondaire ainsi qu'une estimation de la densité conditionnelle des variables construites correspondantes. Une distribution a priori est définie sur cet espace de modèles, pour obtenir ainsi un critère sans paramètres permettant d'évaluer la pertinence des variables construites. Des expérimentations préliminaires montrent la pertinence de l'approche.
%X Le département des sciences informatiques de l'université de Cergy-Pontoise (UCP) [3] s'est doté d'une salle dédiée à l'enseignement des réseaux IP. Cet enseignement est effectué de Bac+3 à Bac+5 (L3 MPI, licence professionnelle " Réseaux et sécurité ", Master professionnel 1ère et 2ème année spécialité " Systèmes Intelligents et Communicants " parcours " Réseaux et sécurité ") sur des modules que nous classerons dans les catégories (1) " IP fondamental " pour la formation de base (adressage, routage, protocoles ethernet, IP, TCP, ICMP, UDP et quelques applications essentielles (DNS, Mail, Web) et des notions de sécurité (2) " IP avancé " pour des protocoles, architectures et applications plus avancés et (3) " IP professionnel " pour la mise en place des notions précédentes sur des équipements et de logiciels spécifiques utilisés en milieu industriel. Cet article donne le résultat de huit ans d'évolution de la salle réseau pour s'adapter aux nouvelles formations et aux nouveaux besoins professionnels, testée sur plus de 600 étudiants et utilisée par une dizaine de formateurs. Nous y décrirons : la salle réseau mise à disposition des enseignements réseaux au département des sciences informatiques de l'UCP ; la pédagogie que nous utilisons pour l'enseignement des réseaux permettant aux étudiants la compréhension de tous les aspects réseaux (concept, protocoles, mise en place) de façon opérationnelle.; le retour d'expérience sur l'organisation de cette salle modulaire et extensible. Nous donnerons ainsi la manière de construire rapidement une salle semblable pour un coût financier minime.
%X Dans cette étude, notre objectif est d'observer l'impact du couplage électromagnétique d'un signal " perturbateur " type horloge sur un signal utile bas niveau au sein d'une même carte à travers une simulation de taux d'erreur binaire. Nous présentons notamment des résultats de dégradation en faisant varier certains paramètres tels que l'isolation entre les deux signaux et la position du parasite dans la bande du signal utile.
%X Cet article présente les résultats de mesures effectuées sur une réalisation BiCMOS d'une partie d'un synthétiseur de fréquences agile dans la bande 3,1-10,5GHz. Les caractéristiques des fréquences générées ainsi que les paramètres des blocs conçus testés séparément sont analysés. La faisabilité du synthétiseur complet est obtenue à partir d'une analyse système grâce aux résultats des mesures effectuées sur les blocs élémentaires.
%X Purpose: This paper presents a new embeddable method for polyp detec- tions in both endoscopic video images and wireless capsule endoscopic images - WCE. Methods: This approach consists rst in extracting possible polyps within the image using some geometric considerations about the shape of the polyps, and then in re n- ing the obtained regions of interest by a boosting-based method using texture features. The performances of the method have been evaluated on a large dataset of 300 polyps, and 1200 non-polyps images. Results: The results show interesting performances of the boosting-based classi cation with a sensitivity of 91%, a speci city of 95% and a false detection rate of 4.8%. These performances are similar to those obtained by Bernal et al. on the same dataset, but the proposed approach reduces the complexity of the hardware implementation for embedded use in WCE or videoendoscope.
%X Heterogeneous Reconfigurable Systems-on-Chip (HRSoC) contain as their name suggests, heterogeneous processing elements in a single chip. Namely, several processors, hardware accelerators as well as communication networks between all these components. In order to leverage the programming complexity of this kind of platform, applications are described with software threads, running on processors, and hardware threads, running on FPGA partitions. Combining techniques such as dynamic and partial reconfiguration and partial readback with the knowledge of the bitstream structure offer the ability to target several partitions using a unique configuration file. Such a feature permits to save critical memory resources. In this article, we propose to tackle the issue of designing fully independent partitions, and especially to avoid the routing conflicts which can occur when using the standard Xilinx FPGA design flow. To achieve the relocation process successfully, we propose a new design flow dedicated to the module relocation, using the standard tools and based on the Isolation Design Flow (IDF), a special flow provided by Xilinx for secure FPGA applications.
%X This paper addresses the challenging problem of scene classification in street-view georeferenced images of urban environments. More precisely, the goal of this task is semantic image classification, consisting in predicting in a given image, the presence or absence of a pre-defined class (e.g. shops, vegetation, etc.). The approach is based on the BOSSA representation, which enriches the Bag of Words (BoW) model, in conjunction with the Spatial Pyramid Matching scheme and kernel-based machine learning techniques. The proposed method handles problems that arise in large scale urban environments due to acquisition conditions (static and dynamic objects/pedestrians) combined with the continuous acquisition of data along the vehicle's direction, the varying light conditions and strong occlusions (due to the presence of trees, traffic signs, cars, etc.) giving rise to high intra-class variability. Experiments were conducted on a large dataset of high resolution images collected from two main avenues from the 12th district in Paris and the approach shows promising results.
%X Abstract--The Radon transform (RT ) on straight lines deals as mathematical foundation for conventional tomographic imaging (e.g. X-ray scanner, Single Photon Emission Computed Tomography (SPECT), Positron Emission Tomography (PET)) which use only one physical phenomenon, i.e. either transmission or emission of radiation. An imaging concept exploiting jointly two phenomena leads to a Radon transform deﬁned on a geometrical support different from straight lines. In this paper, we propose a new two-dimensional X-ray imaging based on the coupling between transmission and reﬂection. Its modeling leads to a Radon transform deﬁned on a pair of half-lines forming a vertical letter V, called V-line RT (V -RT ). Moreover we establish the analytic inverse formula of this new transform, which forms the mathematical basis for image reconstruction. Through simulations, image formation and reconstruction results show the feasibility of this new imaging. The main advantage is the use of an one-dimensional detector which does not rotate for two-dimensional image reconstruction.
%X Recent machine learning techniques have demonstrated their capability for identifying image categories using image features. Among these techniques, Support Vector Machines (SVM) present good results for example in Pascal Voc challenge 2011 [8], particularly when they are associated with a kernel function [28, 35]. However, nowadays image categorization task is very challenging owing to the sizes of benchmark datasets and the number of categories to be classified. In such a context, lot of effort has to be put in the design of the kernel functions and underlying semantic features. In the following of the paper we call semantic features the features describing the (semantic) content of an image. In this paper, we propose a framework to learn an effective kernel function using the Boosting paradigm to linearly combine weak kernels. We then use a SVM with this kernel to categorize image databases. More specifically, this method create embedding functions to map images in a Hilbert space where they are better classified. Furthermore, our algorithm benefits from boosting process to learn this kernel with a complexity linear with the size of the training set. Experiments are carried out on popular benchmarks and databases to show the properties and behavior of the proposed method. On the PASCAL VOC2006 database, we compare our method to simple early fusion, and on the Oxford Flowers databases we show that our method outperforms the best Multiple Kernel Learning (MKL) techniques of the literature.
%X The use of remote sensing images (RSIs) as a source of information in agribusiness applications is very common. In those applications, it is fundamental to identify and understand trends and patterns in space occupation. However, the identification and recognition of crop regions in remote sensing images are not trivial tasks yet. In high-resolution image analysis and recognition, many of the problems are related to the representation scale of the data, and to both the size and the representativeness of the training set. In this paper, we propose a method for interactive classification of remote sensing images considering multiscale segmentation. Our aim is to improve the selection of training samples using the features from the most appropriate scales of representation. We use a boosting-based active learning strategy to select regions at various scales for user's relevance feedback. The idea is to select the regions that are closer to the border that separates both target classes: relevant and non-relevant regions. Experimental results showed that the combination of scales produces better results than isolated scales in a relevance feedback process. Furthermore, the interactive method achieved good results with few user interactions. The proposed method needs only a small portion of the training set to build classifiers that are as strong as the ones generated by a supervised method that uses the whole training set.
%X In this paper, we introduce a novel image signature effective in both image retrieval and image classification. Our approach is based on the aggregation of tensor products of discriminant local features, named VLAT (vector of locally aggregated tensors). We also introduce techniques for the packing and the fast comparison of VLATs. We present connections between VLAT and methods like kernel on bags and Fisher vectors. Finally, we show the ability of our method to be effective for two different retrieval problems, thanks to experiments carried out on similarity search and classification datasets.
%X 'analyse statistique implicative (ASI) est une méthode d'analyse de données non symétrique, conçue par Régis Gras il y a plus de trente ans. À travers thèses, articles de revues, livres et colloques, elle a été développée et l'est encore par lui, par des doctorants ou avec la collaboration d'équipes de recherche universitaires en France et à l'étranger. Puisant ses origines épistémologiques en didactique des mathématiques, son point de départ est un questionnement sur la complexité de l'apprentissage-enseignement des mathématiques. À visée prédictive, elle conduit en ef fet, dans un premier temps, à la modélisation et une mesure de qualité des relations implicatives entre deux observations comportementales a et b, du type " si on observe a alors on a aussi tendance à observer b, et ceci de manière statistiquement significative ". Par la suite, son extension progressive, toujours en réponse à des attentes applicatives, a permis d'extraire de données croisant variables et sujets une structure dynamique et imagée de l'ensemble de variables en jeu, de les conjoindre, d'en élargir les types observés - binaire, numérique, intervalle, flou, vectoriel, rang- puis de construire une structure originale duale entre ces deux ensembles. Cet ouvrage a pour objectif d'en rappeler la genèse unificatrice, de dresser un panorama récent des concepts, des modèles, des méthodes, des images graphiques et des applications de l'ASI. Les 33 chapitres qui le composent sont regroupés en 3 parties principales dont la dynamique est la suivante : * La Partie 1 constitue une sorte de cours, à la manière d'un manuel qui balaierait l'ensemble des concepts formant le coeur de l'ASI, augmenté depuis la première édition et illustré par des exemples didactiques numériques ou graphiques ; * en écho à des problématiques concrètes, la Partie 2 traite de compléments, d'extensions théoriques et l'informatisation de ces concepts qui en montrent le caractère stimulant, innovant et fécond en data mining ; * La Partie 3 présente une sélection d'applications, la plupart nouvelles, qui illustrent les apports de la démarche développée par l'ASI dans des domaines variés, à la recherche de pistes causales en didactiques, pédagogie, psychologie, sociologie, bio-informatique, histoire de l'art, etc. Cette nouvelle édition, à laquelle ont contribué plus de 30 auteurs de disciplines diverses, est préfacée par le Professeur Gilbert SAPORTA
%X This paper presents experimental results on the learning of synchrony between human and a robot arm when they interact in a handshaking situation. The controller is based on two coupled nonlinear neurons (central pattern generator, CPG) that are able to learn to oscillate at different frequencies depending on a biological inspired learning mechanism. This mechanism uses interaction force measured in the robot joint as a learning signal. Results show that the CPG learns and follows the human rhythm easily and then enjoy it.
%X Since Piaget, it is well accepded that higher level cognitive functions are settled on low level sensori-motor associations. In this paper, we will show that different kinds of interactive behaviors can emerge according to the kind of proprioceptive function available in a given sensori-motor system. We will study two different examples. In the first one, an internal proprioceptive signal is avaible for the learning of the visuo-motor coordination between an arm and a camera. An imitation behavior can emerge when the robot 's eye focuses on the hand of the experimenter instead of its own hand. The imitative behavior results from the error minimization between the visual signal and the proprioceptive signal. Here, the imitation results from the perception ambiguity: the robot mistakes its hand with the experimenter hand! In the second example, a robot head has to recognize the facial expression of the human caregiver. Yet, the robot has no visual feedback of its own facial expression. The human expressive resonance will allows the robot to select the visual features relevant for a particual facial expression. As a result, after few minutes of interactions, the robot can imitates the facial expression of the human partner. We will show the different proprioceptive signals used in both examples can be seen as bootstrap mechanisms for more complex interactions. Applied as a crude model of the human, we will propose that these mechanisms play an important role in the process of individuation.
%X This demo will show a prototype of a new software radio enabled broadcast media navigator implemented on an FPGA and quad-core processor, which is able to demodulate simultaneously all channel in the FM band and perform a real time classification of the musical genre. This prototype represents the elementary component of a navigator capable of searching the large quantities of information contained in the radio bands.
%X The paper describes a Software Defined Radio architecture that simultaneously demodulates all radio stations in the Frequency Modulated (FM) band, and is intended as a preprocessor for content and metadata indexing applications. The system, which contains an overlap-add type channelizing filterbank and a massively parallel frequency demodulation block, is implemented in a single Field- Programmable Gate Array, thus offering the possibility of replacing more traditional media indexing installations containing tens of individual receivers. It is believed to be the first single chip full-band channelization system intended for consumer broadcast media indexing applications. Chip resource utilization details and experimental results from the developed system are also presented.
%X This paper addresses the problem of large scale image repre- sentation for object recognition and classification. Our work deals with the problem of optimizing the classification accu- racy and the dimensionality of the image representation. We propose to iteratively select sets of projections from an ex- ternal dataset, using Bagging and feature selection thanks to SVM normals. Features are selected using weights of SVM normals in orthogonalized sets of projections. The Bagging strategy is employed to improve the results and provide more stable selection. The overall algorithm linearly scales with the size of features, and thus is able to process the large state- of-the-art image representation. Given Spatial Fisher Vectors as input, our method consistently improves the classification accuracy for smaller vector dimensionality, as demonstrated by our results on the popular and challenging PASCAL VOC 2007 benchmark.
%X Dans le souci de permettre le passage à l'échelle des méthodes d'indexation par le contenu devant l'explosion de la quantité de contenu disponible, nous présentons un nouveau système décentralisé qui permet l'indexa- tion conjointe de multiples bases de données multimédia aux contenus potentiellement hétérogènes. Nous montrons que cette approche, qui repose sur le calcul décentralisé d'un dictionnaire visuel, fournit des résultats de recherche équivalents aux approches centralisées tout en garantis- sant un passage à l'échelle aisé.
%X In this paper we propose to tackle human actions indexing by introducing a new local motion descriptor. Our proposed descriptor is based on two modeling, a spatial model and a temporal model. The spatial model is computed by projection of optical flow onto bivari- ate orthogonal polynomials. Then, the time evolution of spatial coefficients is modeled with a one dimension polynomial basis. To perform the action classification, we extend recent still image signatures using local de- scriptors to our proposal and combine them with linear SVM classifiers. The experiments are carried out on the well known KTH dataset and on the more challeng- ing Hollywood2 action classification dataset and show promising results.
%X During the last decade, a significant attention has been paid, by the computer vision and the computer graphics communities, to three dimensional (3D) object retrieval. Shape retrieval methods can be divided into three main steps: the shape descriptors extraction, the shape signatures and their associated similarity measures, and the machine learning relevance functions. While the first and the last points have vastly been addressed in recent years, in this paper, we focus on the second point; presenting a new 3D object retrieval method using a new coding/pooling technique and powerful 3D shape descriptors extracted from 2D views. For a given 3D shape, the approach extracts a very large and dense set of local descriptors. From these descriptors, we build a new shape signature by aggregating tensor products of visual descriptors. The similarity between 3D models can then be efficiently computed with a simple dot product. We further improve the compactness and discrimination power of the descriptor using local Principal Component Analysis on each cluster of descriptors. Experiments on the SHREC 2012 and the McGill benchmarks show that our approach outperforms the state-of-the-art techniques, including other BoF methods, both in compactness of the representation and in the retrieval performance.
%X The paper presents a novel Optical Network on Chip (ONoC) relying on the multi-level optical layer design paradigm and called "OMNoC". The proposed ONoC relies on multi-level microring resonator allowing efficient light coupling between superposed waveguides. Such microring resonator avoids using waveguide crossing, which contribute to reduce propagation losses. Preliminary experimental results demonstrate the potential of multi-level optical layer for reducing power consumption and increasing scalability in the proposed ONoC.
%X We propose an unsupervised statistical region based active contour approach integrating an original fractional entropy measure for image segmentation with a particular application to single channel actin tagged fluorescence confocal microscopy image segmentation. Following description of statistical based active contour segmentation and the mathematical definition of the proposed fractional entropy descriptor, we demonstrate comparative segmentation results between the proposed approach and standard Shannon's entropy on synthetic and natural images. We also show that the proposed unsupervised statistical based approach, integrating the fractional entropy measure, leads to very satisfactory segmentation of the cell nuclei from which shape characterization can be calculated.
%X We consider the Interval-Passing Algorithm (IPA), an iterative reconstruction algorithm for reconstruction of non-negative sparse real-valued signals from noise-free measurements. We ﬁrst generalize the IPA by relaxing the original constraint that the measurement matrix must be binary. The new algorithm operates on any non-negative sparse measurement matrix. We give a performance comparison of the generalized IPA with the reconstruction algorithms based on 1) linear programming and 2) veriﬁcation decoding. Then we identify signals not recoverable by the IPA on a given measurement matrix, and show that these signals are related to stopping sets responsible to failures of iterative decoding algorithms on the binary erasure channel (BEC). Contrary to the results of the iterative decoding on the BEC, the smallest stopping set of a measurement matrix is not the smallest conﬁguration on which the IPA fails. We analyze the recovery of sparse signals on subsets of stopping sets via the IPA and provide sufﬁcient conditions on the exact recovery of sparse signals. Reconstruction performance of the IPA using the IEEE 802.16e LDPC codes as measurement matrices are given to show the effect of stopping sets in the performance of the IPA.
%X This work concentrates on the study of inverse determinant sums, which arise from the union bound on the error probability, as a tool for designing and analyzing algebraic space-time block codes. A general framework to study these sums is established, and the connection between asymptotic growth of inverse determinant sums and the diversity-multiplexing gain trade-off is investigated. It is proven that the growth of the inverse determinant sum of a division algebra-based space-time code is completely determined by the growth of the unit group. This reduces the inverse determinant sum analysis to studying certain asymptotic integrals in Lie groups. Using recent methods from ergodic theory, a complete classification of the inverse determinant sums of the most well known algebraic space-time codes is provided. The approach reveals an interesting and tight relation between diversity-multiplexing gain trade-off and point counting in Lie groups.
%X This paper presents a new embeddable method for polyp detections in Wireless Capsule Endoscopic - WCE images. this approach con- sists first of extracting candidate polyps within the image using geo- metric considerations about related shape, and second, in classifying (polyp/non-polyp) obtained candidates by a boosting-based method using texture features. The proposed approach has been designed in accordance with the hardware constraints related to FPGA imple- mentation for integration within WCE imaging device. The classification performance of the method have been evaluated on a large dataset of 300 polyps, and 1200 non-polyps images. Experiments show interesting and promising performance: the boosting-based classification is characterized by a sensitivity of 91%, a specificity of 95% and a false detection rate of 4.8%, the detection rate of the over- all processing chain being of 68%. The performance of the boosting- based classification are in accordance with the most recent reference on this particular topic using the same dataset. Building of a dedicated WCE image database should permit the improvement of the global detection rate.
%X Compton scattering tomography (CST) is an alternative imaging process which reconstructs, slice by slice, the electron density of an object by collecting radiation emitted by an external source and scattered throughout the object. A new CST modality, has been proposed based on a Radon Transform over a family of circular arcs but no attenuation factor was taken into account. Reconstruction of the unknown electron density is achieved by the inversion of the corresponding circular-arcs Radon transform (CART). In this work, medium attenuation factor is considered in the modelling and we propose an iterative correction algorithm to compensate this issue.Thus the feasibility of this new CST modality under realistic working conditions is demonstrated.
%X In this paper, we investigate the behaviors of the Belief Propagation algorithm considered as a dynamic system. In the context of LDPC (Low Density Parity-Check) codes, we use the noise power of the transmission channel as a potentiometer to evaluate the different motions that the BP can follow. The computations of dynamic quantifiers as the bifurcation diagram, the Lyapunov exponent and the reconstructed trajectory enable to bring out four main behaviors. In addition, we propose a novel measure that is the hyperspheres method, which provides the knowledge of the time evolution of the attractor size. The information collected from these different quantifiers helps to better understand the BP evolution and to focus on the noise power values for which the BP suffers from chaos.
%X After a short review of properties of biological place cells, mainly found in the hippocampal region of rodents, and a brief presentation of a biologically inspired navigation architecture relying on these cells, we will show how contextual information could facilitate scale changes to large environments. We thus present a simple model of spatial context allowing to both reduce noise effects on place cells (in biological model) and increase its computational performance.
%X Allowing a robot to autonomously navigate wide and unknown environments not only requires a set of robust strategies to cope with miscellaneous situations, but also needs mechanisms of self-assessment for guiding learning and monitoring strategies. Monitoring strategies requires feedbacks on the behavior's quality, from a given fitness system, to take correct decisions. In this work, we focus on how violations of expectations of such fitness system can be detected. Following an incremental and bio-mimetic approach, we first present two different sensorimotor strategies our robot can use to navigate: a Place Cells based strategy and a road following strategy. Then, we present a neural architecture that may be able to evaluate both navigation strategies. This model is based on an online novelty detection algorithm using a neural predictor. This neural predictor learns contingencies between sensations and actions, giving the expected sensation based from the previous perception. Prediction error, coming from surprising events, provides a direct measure of the quality of the underlying sensorimotor contingencies involved. We propose that this model might be a key structure toward self-assessment. We made several experiments that can account for such properties for both strategies.
%X In this work, we focus on how an emotional controller can be used to modulate robot's behaviors. We present a generic neural architecture, based on an online novelty detection algorithm, that may be able to self-evaluate any sensori-motor strategies. We show how a simple mechanism based on the prediction progress allows the system to monitor its strategies and communicate its disability in deadlock situations. We made several experiments that can account for such properties for two different behaviors (road following and place cells based navigation) in a simulated environment
%X L'une des solutions potentielles pour améliorer ou éventuellement remplacer les réseaux d'interconnexions au sein des systèmes sur puce est l'utilisation des interconnexions RF. Dans ce papier, nous traitons de l'impact de la ligne de transmission sur la qualité du signal y circulant. En effet, la variation de l'atténuation que subit le signal, liée entre autres à l'effet de peau et la légère désadaptation de la ligne, va influencer directement le rapport signal sur bruit au niveau de chacune des voies en réception du réseau.
%X JKernelMachines is a Java library for learning with kernels. It is primarily designed to deal with custom kernels that are not easily found in standard libraries, such as kernels on structured data. These types of kernels are often used in computer vision or bioinformatics applications. We provide several kernels leading to state of the art classification performances in computer vision, as well as various kernels on sets. The main focus of the library is to be easily extended with new kernels. Standard SVM optimization algorithms are available, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL), and a recently published algorithm to learn powered products of similarities (Product Kernel Learning).
%X In this paper, we present an efficient 3D object retrieval method invariant to scale, orientation and pose. Our approach is based on the dense extraction of discriminative local descriptors extracted from 2D views. We aggregate the descriptors into a single vector signature using tensor products. The similarity between 3D models can then be efficiently computed with a simple dot product. Experiments on the SHREC12 commonly-used benchmark demonstrate that our approach obtains superior performance in searching for generic shapes.
%X The main issues of web scale image retrieval are to achieve a good accuracy while retaining low computational time and memory footprint. In this paper, we propose a compact image signature by aggregating tensors of visual descriptors. Efficient aggregation is achieved by preprocessing the descriptors. Compactness is achieved by projection and quantization of the signatures. We compare our method to other efficient signatures on a 1 million images dataset, and show the soundness of the approach.
%X The question whether newborns possess inborn social skills is a long debate in developmental psychology. Fetal behavioral and anatomical observations show evidences for the control of eye movements and facial behaviors during the third trimester of pregnancy whereas specific sub-cortical areas, like the superior colliculus (SC) and the striatum appear to be functionally mature to support these behaviors. These observations suggest that the newborn is potentially mature for developing minimal social skills. In this manuscript, we propose that the mechanism of sensory alignment observed in SC is particularly important for enabling the social skills observed at birth such as facial preference and facial mimicry. In a computational simulation of the maturing superior colliculus connected to a simulated facial tissue of a fetus, we model how the incoming tactile information is used to direct visual attention toward faces. We suggest that the unisensory superficial visual layer (eye-centered) in SC and the deep somatopic layer (face-centered) in SC are combined into an intermediate layer for visuo-tactile integration and that multimodal alignment in this third layer allows newborns to detect faces and to mimic. After the learning stage through hebbian reinforcement learning, we show that the intermediate layer develop visuo-tactile sensory alignment. We observe emergent properties of the global network such as sensitivity to he orientation of face-like patterns and detection of facial expressions.
%X Newborns present impressive developmental changes during the first year in almost all domains marked by memory categorization and variability. We propose that one important actor of this developmental shift is the gradual influence of the cholinergic system in the cortico-hippocampal circuits. Based on neurological observations and developmental studies, we model how the neuromodulator acetylcholine could be gradually released in the hippocampal system from the fetal period till the first year to support the detection of novel ..
%X Autonomy and self-improvment capabilities are still challenging in the fields of robotics. Allowing a robot to autonomously navigate wide and unknown environments not only requires a set of robust strategies to cope with miscellaneous situations, but also needs mechanisms of self-assesment for guiding learning and monitoring strategies. Monitoring strategies requires feedbacks on the behavior's quality, from a given fitness system, to take correct decisions. In this work, we focuse on how an emotional controller can be used to modulate robot behaviors. Following an incremental and constructivist approach, we present a generic neural architecture, based on an online novelty detection algorithm, that may be able to self-evaluate any sensori-motor strategies. This architecture learns contingencies between sensations and actions, giving the expected sensation based from the previous perception. Prediction error, coming from surprising events, provides a direct measure of the quality of the underlying sensori-motor contingencies involved. We show how a simple emotional controller based on the prediction progress allows the system to monitor its strategies to solve complex navigation tasks and communicate its disability in deadlock situations. We propose that this model could be a key structure toward self-monitoring. We made several experiments that can account for such properties for different behaviors (road following and Place Cells based navigation).
%X With the increasing usage of 3D models, the need for efficient shape retrieval techniques became mandatory. In this chapter, several approaches for 3D retrieval and shape recognition are presented. Invariant descriptor issues for 3D static mesh are exposed, before addressing action and face recognition in 3D videos. Then classical 3D-search engine performance evaluation tools are thoroughly explained. Finally, examples of significant applications using 3D retrieval techniques are given.
%X In our previous work we took inspiration from human psychological and neurological data which suggest that synchrony is an important parameter for human-human interaction. We proposed synchrony as a way of interacting and presented a synchrony-based architecture capable of selecting the human partner and of locating the focus of attention. We extend here our approach to people recognition to track the synchronized partner in the context of an autonomous mobile robot. If an agent interacts with a frequency close to the robot's dynamic our architecture selects this agent as an interacting partner and focus on him. Therefor, a shape recognition model learns the shape of the interactant. Combining synchrony and shape strategies, we obtained a bio-inspired robust and efficient architecture to automatically initiate the interaction with a selected partner and to locate and track this preferred partner in real indoor environment using kinematic (synchrony) and form (shape) pathways.
%X Inspired by studies of interpersonal coordinations, we assumed that unintentional synchrony is a fundamental parameter to initiate and maintain Human Machine interactions. We developed a neural model allowing a robot to synchronize its behavior depending on human movement frequency, and thus to choose this partner on the basis of synchrony detection between its own learned dynamic and the visual stimuli induced by the human. To confirm or deny our assumptions we presented here a psychological study to measure unintentional synchronization during Unidirectional and Bidirectional Human Robot Interaction using our previously proposed model for initiating the interaction and focusing the robot attention on a selected partner. The experimental results demonstrated that bidirectional intuitive interaction leading to possible unintentional synchronization is primordial to obtain natural human robot interactions using minimal cognitive load (unintentional behavior).
%X This contribution introduces a robust and content oriented detector of interest zones for defect localization in oil pipeline intelligent inspection achieving good perfomances toward complexity ratio. The method self-processes the multidimensional data collected by a pipeline inspection device equipped with many ultrasonic sensors (up to 512). It introduces a new content oriented usage of the EM algorithm, adapted to fit the very peculiar nature of the data to first isolate candidates zone, followed with a segmentation step to both get fine contours of defects and reject false alarms. Obtained performances in terms of specificity and sensibility show that the proposed approach is compatible with a routine utilization by specialists.
%X Purpose: Wireless capsule endoscopy (WCE) is commonly used for noninvasive gas- trointestinal tract evaluation, including the detection of mucosal polyps. A new embed- dable method for polyp detection in wireless capsule endoscopic images was developed and tested. Methods: First, possible polyps within the image were extracted using geometric shape features. Next, the candidate regions of interest were evaluated with a boosting-based method using textural features. Each step was carefully chosen to accommodate hard- ware implementation constraints. The method's performance was evaluated on WCE datasets including 300 images with polyps and 1200 images without polyps. Hardware implementation of the proposed approach was evaluated to quantitatively demonstrate the feasibility of such integration into the WCE itself. Results: The boosting-based polyp classi cation demonstrated a sensitivity of 91.0%, a speci city of 95.2% and a false detection rate of 4.8%. This performance is close to that reported recently in systems developed for an on-line analysis of video colonoscopy images. Conclusion: A new method for polyp detection in videoendoscopic WCE examinations was developed using boosting-based approach. This method achieved good classi ca- tion performance and can be implemented in situ with embedded hardware.
%X Le département des sciences informatiques de l'université de Cergy-Pontoise (UCP) [3] s'est doté d'une salle dédiée à l'enseignement des réseaux IP. Cet enseignement est effectué de Bac+3 à Bac+5 : L3 MPI, licence professionnelle " Réseaux et sécurité ", Master professionnel 1ère et 2ème année spécialité " Systèmes Intelligents et Communicants " (master ISIM SIC Pro) parcours " Réseaux et sécurité " (RS). L'enseignement réseaux est dispensé sous forme d'une dizaine de modules que nous classerons dans les catégories (1) " IP fondamental " pour la formation de base (adressage, routage, protocoles ethernet, IP, TCP, ICMP, UDP et quelques applications essentielles (DNS, Mail, Web) et des notions de sécurité (2) " IP avancé " pour des protocoles, architectures et applications plus avancés et (3) " IP professionnel " pour la mise en place des notions précédentes sur des équipements et de logiciels spécifiques utilisés en milieu industriel. Cet article synthétise huit ans d'évolution de la salle réseau pour s'adapter aux nouvelles formations et besoins professionnels, testée sur plus de 600 étudiants et utilisée par une dizaine de formateurs. Nous y décrirons : la salle réseau mise à disposition des enseignements réseaux au département des sciences informatiques de l'UCP ; la pédagogie que nous utilisons pour l'enseignement des réseaux permettant aux étudiants la compréhension de tous les aspects réseaux (concept, protocoles, mise en place) de façon opérationnelle.; le retour d'expérience sur l'organisation de cette salle modulaire et extensible. Nous décrirons comment construire rapidement une salle semblable pour un coût financier minime.
%X Navigation tasks are based on two approaches: place- action directed or goal directed. In the goal directed ones, a reward is generally given to the system when the goal is reached. Many models are able to predict the reward in a simple cases. In this paper, we present an architecture for complex conditioning.
%X Elastic shape analysis on non-linear Riemannian manifolds provides an efficient and elegant way for simultaneous comparison and registration of non-rigid shapes. In such formulation, shapes become points on some high dimensional shape space. A geodesic between two points corresponds to the optimal deformation needed to register one shape onto another. The length of the geodesic provides a proper metric for shape comparison. However, the computation of geodesics, and therefore the metric, is computationally very expensive as it involves a search over the space of all possible rotations and re- parameterization. This problem is even more important in shape retrieval scenarios where the query shape is compared to every element in the collection to search. In this paper, we propose a new procedure for metric approximation using the framework of kernel functions. We will demonstrate that this provides a fast approximation of the metric while preserving its invariance properties.
%X Many methods have been recently proposed to deal with the large amount of data provided by high- resolution remote sensing technologies. Several of these methods rely on the use of image segmentation algorithms for delineating target objects. However, a common issue in geographic object-based applications is the definition of the appropriate data representation scale, a problem that can be addressed by exploiting multiscale segmentation. The use of multiple scales, however, raises new challenges related to the definition of effective and efficient mechanisms for extracting features. In this paper, we address the problem of extracting histogram-based features from a hierarchy of regions for multiscale classification. The strategy, called H-Propagation, exploits the existing relation- ships among regions in a hierarchy to iteratively prop- agate features along multiple scales. The proposed method speeds up the feature extraction process and yields good results when compared with global low- level extraction approaches.
%X This paper presents a new tensor motion descriptor only using optical flow and HOG3D information: no interest points are extracted and it is not based on a visual dictionary. We propose a new aggregation technique based on tensors. This is a double aggregation of tensor descriptors. The first one represents motion by using polynomial coefficients which approximates the optical flow. The other represents the accumulated data of all histograms of gradients of the video. The descriptor is evaluated by a classification of KTH, UCF11 and Hollywood2 datasets, using a SVM classifier. Our method reaches 93.2% of recognition rate with KTH, comparable to the best local ap- proaches. For the UCF11 and Hollywood2 datasets, our recognition achieves fairly competitive results compared to local and learning based approaches. Keywords: Global motion descriptor, optical flow, histogram of gradients, action recognition
%X Wireless capsule endoscopy (WCE) is commonly used for noninvasive gastrointestinal tract evaluation, including the identification of polyps. In this paper, a new multimodal embeddable method for polyp detection and classification in wireless capsule endoscopic images was developed and tested. The multimodal wireless capsule used both 2D and 3D data to identify possible polyps and to deliver cancerous information of the polyps based on 3D geometric features. Possible polyps within the image (2D) were extracted using simple geometric shape features and, in a second step, the candidate regions of interest (ROI) were evaluated with a boosting-based method using textural features. Once the 2D identification of polyps has been performed, the two-class ("malignant" or "begnin") classification of the polyps is achieved using the 3D parameters computed from the preselected ROI using an active stereo vision system. At this stage, a Support Vector Machine (SVM) classifier is used to proceed to the final classification and to make possible a pre diagnosis. The new proposed multimodal approach based on 2D - 3D feature extraction improves WCE capabilities to identify and classify polyps: The boosting-based polyp classification demonstrated a sensitivity of 91%, a specificity of 95% and a false detection rate of 4.8% on a database composed of 300 hundred positive examples and 1200 negative ones; Considering the 3D performance, a large scale demonstrator was evaluated and tested to perform in vitro experiments on an ad hoc polyp database. The performance of the 3D approach achieved a correct classification rate (malignant or benin) of approximately 95%.
%X The paper provides simple lower and upper bounds on block and bit performances of spatially-coupled LDPC (SC-LDPC) codes over a particular model of the block erasure channel. As expected, the spatial coupling structure helps in the correction of bursty erasures, and the decoding performance of SC-LDPC codes improves if the coupling parameter increases.
%X We propose here a new construction of quantum codes combining an improved version of a family of spatially coupled quantum LDPC codes, suggested in [1], with a family of error reducing turbo-codes of [2]. This new construction displays outstanding performances under iterative decoding for noise levels very close to the hashing bound, without needing qubits, protected from noise as in [1].
%X This work considers normalized inverse determinant sums as a tool for analyzing the performance of division algebra based space-time codes for multiple antenna wireless systems. A general union bound based code design criterion is obtained as a main result. In our previous work, the behavior of inverse determinant sums was analyzed using point counting techniques for Lie groups; it was shown that the asymptotic growth exponents of these sums correctly describe the diversity-multiplexing gain trade-off of the space-time code for some multiplexing gain ranges. This paper focuses on the constant terms of the inverse determinant sums, which capture the coding gain behavior. Pursuing the Lie group approach, a tighter asymptotic bound is derived, allowing to compute the constant terms for several classes of space-time codes appearing in the literature. The resulting design criterion suggests that the performance of division algebra based codes depends on several fundamental algebraic invariants of the underlying algebra.
%X In this paper, we analyze the optimal blockwise subcarrier allocation scheme in coded Orthogonal Frequency Division Multiple Access (OFDMA) uplink systems without channel state information at the transmitter side. Due to the Discrete Fourier Transform, the subcarrier gains in OFDMA systems are correlated and this correlation plays a determinant role since it causes a degradation of the transmission performance. As a consequence, there exist some subcarrier allocation schemes which are preferable to achieve higher diversity gains. We propose the minimization of an inter-carrier sum-correlation function defined as the sum of correlations of each subcarrier with respect to the others in the same allocation scheme. We show the robustness of the proposed blockwise allocation policy to the effect of carrier frequency offsets (CFO). Moreover, we propose the block-size which provides the best trade-off between diversity and CFO for practical values of CFO. Finally, numerical simulations validating our results are presented.
%X Autonomy and self-improvement capabilities are still challenging in the fields of robotics and machine learning. Allowing a robot to autonomously navigate in wide and unknown environments not only requires a repertoire of robust strategies to cope with miscellaneous situations, but also needs mechanisms of self-assessment for guiding learning and for monitoring strategies. Monitoring strategies requires feedbacks on the behavior's quality, from a given fitness system in order to take correct decisions. In this work, we focus on how a second-order controller can be used to (1) manage behaviors according to the situation and (2) seek for human interactions to improve skills. Following an incremental and constructivist approach, we present a generic neural architecture, based on an online novelty detection algorithm that may be able to self-evaluate any sensory-motor strategies. This architecture learns contingencies between sensations and actions, giving the expected sensation from the previous perception. Prediction error, coming from surprising events, provides a measure of the quality of the underlying sensory-motor contingencies. We show how a simple second-order controller (emotional system) based on the prediction progress allows the system to regulate its behavior to solve complex navigation tasks and also succeeds in asking for help if it detects dead-lock situations. We propose that this model could be a key structure toward self-assessment and autonomy. We made several experiments that can account for such properties for two different strategies (road following and place cells based navigation) in different situations.
%X The design of a resource allocation mechanism is becoming a major challenge with many-core SoC architectures. We deﬁne a hardware controller in which a grid of processing elements (PEs) will support a set of neuro-cognitive processes in order to drive a robot in different tasks. We propose an original artiﬁcial neural network (NN) named DMAD-SOM for "Distributed Multiplicative Activity Dependent Self Organizing Map" inspired by Neural Fields (NF) equations that have shown self-organizing behaviors and can be suitable for this purpose. This model is shortly described here. It can be used to take allocation decisions locally, taking in account the state of the whole system through the emergent behavior of the network. This paper describes the distributed DMAD-SOM model and focuses mainly on its implementation onto FPGA.
%X Dans ce papier, nous proposons un nouvel algorithme de décodage itératif pour le problème de codage de source avec information adjacente compressée. L'information adjacente est compressée par un quantificateur qui génère un index. Plutôt que d'utiliser un seul représentant pour l'information adjacente (la reconstruction correspondante à l'index), nous allons projeter une solution intermédiaire du décodage de la source sur la cellule de Voronoï correspondante à l'index reçu. Grâce à cette projection, nous espérons rapprocher l'information adjacente du mot source recherché, accélérant ainsi le décodage itératif de la source. Des simulations utilisant un code LDPC pour la branche principale et une quantification sur treillis pour l'information adjacente montrent que pour un nombre d'itérations de décodage fixé, notre algorithme permet d'augmenter le nombre de mots correctement décodés.
%X This paper presents a new iterative decoding algorithm for the source coding with coded side information problem. Side information (SI) is compressed to an index by a many-to-one (quantization) function. Instead of using the reconstruction corresponding to the quantization index as a single representative SI word to aid the main decoder, one can modify it by projecting an intermediate estimate of the source word onto the Voronoi cell associated to the SI index. The hope is that the projection brings the representative SI word closer to the source word, and thus accelerates iterative decoding. Simulations using LDPC syndrome coding in the main branch and trellis-coded quantization in the SI branch show that for a fixed number of decoder iterations, this method indeed increases the number of correctly decoded source words. In fact, the decoding threshold is shifted, which may be attributed to a partial compensation of the suboptimality of the quantizer.
%X This paper presents an overview of popular retrieval tech- niques based on machine learning for content based multimedia retrieval. Furthermore, we also propose to highlight current gaps and required im- provement in this context. We first introduce common retrieval problems, and the usual models and assumptions made on multimedia data. Thanks to these assumptions, techniques based on machine learning can be used in many application cases. In this scope, we present popular methods for indexing multimedia data, like the ones based on the training of visual dic- tionaries. Then, we present supervised techniques that use labeled data to train and design retrieval components. We show how this last topic could benefit from many improvement from the machine learning community. Finally, this paper presents interesting perspective and new paradigms for multimedia retrieval based on machine learning.
%X In this paper, we address the problem of scheduling workflow applications on heterogeneous computing systems like cloud computing infrastructures. In general, the cloud workflow scheduling is a complex optimization problem which requires considering different criteria so as to meet a large number of QoS (Quality of Service) requirements. Traditional research in workflow scheduling mainly focuses on the optimization constrained by time or cost without paying attention to energy consumption. The main contribution of this study is to propose a new approach for multi-objective workflow scheduling in clouds, and present the hybrid PSO algorithm to optimize the scheduling performance. Our method is based on the Dynamic Voltage and Frequency Scaling (DVFS) technique to minimize energy consumption. This technique allows processors to operate in different voltage supply levels by sacrificing clock frequencies. This multiple voltage involves a compromise between the quality of schedules and energy. Simulation results on synthetic and real-world scientific applications highlight the robust performance of the proposed approach.
%X In this letter we propose a compressive sensing scheme for the mixture estimation problem in spectroscopy. We show that by applying an appropriate measurement matrix on the chemical mixture spectrum, we obtain an overall measurement matrix which is sparse. This enables the use of a low-complexity iterative reconstruction algorithm, called the interval-passing algorithm, to estimate the concentration of each chemical present in the mixture. Simulation results for the proportion of correct reconstructions show that chemical mixtures with a large number of chemicals present can be recovered.
%X The iterative decoding threshold of low-density parity-check (LDPC) codes over the binary erasure channel (BEC) fulfills an upper bound depending only on the variable and check nodes with minimum distance 2. This bound is a consequence of the stability condition, and is here referred to as stability bound. In this paper, a stability bound over the BEC is developed for doubly-generalized LDPC codes, where variable and check nodes can be generic linear block codes, assuming maximum a posteriori erasure correction at each node. It is proved that also in this generalized context the bound depends only on the variable and check component codes with minimum distance 2. A condition is also developed, namely, the derivative matching condition, under which the bound is achieved with equality. The stability bound leads to consider single parity-check codes used as variable nodes as an appealing option to overcome common problems created by generalized check nodes.
%X In this paper, a method for the asymptotic analysis of generalized low-density parity-check (GLDPC) codes and doubly generalized low-density parity-check (D-GLDPC) codes over the binary erasure channel (BEC), based on extrinsic information transfer (EXIT) chart, is described. This method overcomes the problem consisting of the impossibility to evaluate the EXIT function for the check or variable component codes, in situations where the information functions or split information functions for component codes are unknown. According to the proposed technique, GLDPC codes and D-GLDPC codes where the generalized check and variable component codes are random codes with minimum distance at least 2, are considered. A technique is then developed which finds the EXIT chart for the overall GLDPC or D-GLDPC code, by evaluating the expected EXIT function for each check and variable component code. This technique is finally combined with the differential evolution algorithm in order to generate some good GLDPC and D-GLDPC edge distributions. Numerical results of long, random codes, are presented which confirm the effectiveness of the proposed approach. They also reveal that D-GLDPC codes can outperform standard LDPC codes and GLDPC codes in terms of both waterfall performance and error floor.
%X In this paper, the design of doubly generalized low-density parity-check (DGLDPC) codes is proposed. This approach generalizes the structure of LDPC codes at both check and variable nodes. The performance of DGLDPC codes over the AWGN channel is analyzed using EXIT charts. Combined with differential evolution optimization, this analysis provides thresholds for DGLDPC codes that are better than that of LDPC and GLDPC codes with the same maximum variable degree. These theoretical thresholds are verified via simulations. Furthermore DGLDPC codes exhibit a lower error floor compared with their LDPC and GLDPC counterparts.
%X An error-burst correcting algorithm is developed based on a circulant parity-check matrix of a cyclic code. The proposed algorithm is more efficient than error trapping if the code rate is less than about 2/3. It is shown that for any (n, k) cyclic code, there is an n Ã n circulant parity-check matrix such that the algorithm, applied to this matrix, corrects error bursts of lengths up to the error-burst correction limit of the cyclic code. This same matrix can be used to efficiently correct erasure bursts of lengths up to n - k. The error-burst correction capabilities of a class of cyclic low-density parity-check (LDPC) codes constructed from finite geometries are also considered.
%X In this paper, we present a generalization of the ordered statistics decoding (OSD) techniques for the class of intersymbol interference (ISI) channels, and show decoding results for the extended Bose-Chaudhuri-Hocquenghem (eBCH) [128,64,22] code and the [255,239,17] Reed-Solomon (RS) binary image, over the PR2 partial response channel. Using the generalized OSD technique, we go on to generalize the Box-andMatch Algorithm (BMA) to the class of ISI channels. The BMA is an enhancement of OSD, and prior work has shown it to provide significant performance gain over OSD for memoryless additive white Gaussian noise (AWGN) channels. We present decoding results of the BMA for ISI channels, for the same eBCH and RS (binary image) codes, and PR2 channel. Our results show that the BMA (generalized for ISI channels) is superior to the OSD in terms of its performance/complexity trade-off. More specifically, the BMA may be tuned such that both algorithms have similar complexity, whereby the BMA still outperforms the OSD by a significant margin.
%X In this paper, a low-density parity-check (LDPC) coded recording system is investigated, for which the run-length-limited (RLL) constraint is satisfied by deliberate flipping at the write side and by estimating the flipped bits at the read side. Two approaches are proposed for enhancing the error performance of such a system. The first approach is to alleviate the negative effect of incorrect estimation of the flipped bits by adjusting the soft information. The second approach is to increase the likelihood of the correct detection of flipped bits by designing a flipped-bit detection algorithm that utilizes both the RLL constraint and the parity-check constraint of the LDPC code. These two approaches can be combined to obtain significant improvement in performance over previously proposed methods.
%X Nous proposons d'utiliser la technique des champs de Markov triplet (CMT) pour effectuer une segmentation non supervisée de la région périoculaire d'images considérées comme difficiles dans le cadre de la reconnaissance d'iris. Les résultats obtenus confirment l'intérêt de l'utilisation des CMTs par rapport aux champs de Markov caché (CMCs) et aux traditionnelles approches basées sur les gradients pour la segmentation d'iris. Nous avons en effet montré que la précision des cercles de normalisation est nettement améliorée en utilisant le modèle CMT malgré la qualité dégradée des images. Ces résultats sont prometteurs pour une intégration de ces approches dans les systèmes de vérification de l'iris.
%X Dans cet article, nous considérons un système de communication multi-utilisateurs mettant en oeuvre une couche physique ultra-large bande (ULB) impulsionnelle à répartition par code. Nous nous intéressons tout d'abord à l'expression de la variance de l'interférence d'accès multiple (IAM) lorsque les utilisateurs ont des durées symboles différentes grâce à un nombre de trames, $N_f$, variable selon les utilisateurs. Nous nous intéressons ensuite au problème de la maximisation du débit global en affectant un nombre de trames différent pour chaque utilisateur sous contrainte de qualité de service (QoS) hétérogène pour certaines classes d'utilisateurs. Nous proposons une heuristique à complexité linéaire avec le nombre d'utilisateurs pour l'allocation du nombre de trames et évaluons ses performances par rapport à deux algorithmes de références.
%X During development, infants learn to differentiate their motor behaviors relative to various contexts by exploring and identifying the correct structures of causes and effects that they can perform; these structures of actions are called task sets or internal models. The ability to detect the structure of new actions, to learn them and to select on the fly the proper one given the current task set is one great leap in infants cognition. This behavior is an important component of the child's ability of learning-to-learn, a mechanism akin to the one of intrinsic motivation that is argued to drive cognitive development. Accordingly, we propose to model a dual system based on (1) the learning of new task sets and on (2) their evaluation relative to their uncertainty and prediction error. The architecture is designed as a two-level-based neural system for context-dependent behavior (the first system) and task exploration and exploitation (the second system). In our model, the task sets are learned separately by reinforcement learning in the first network after their evaluation and selection in the second one. We perform two different experimental setups to show the sensorimotor mapping and switching between tasks, a first one in a neural simulation for modeling cognitive tasks and a second one with an arm-robot for motor task learning and switching. We show that the interplay of several intrinsic mechanisms drive the rapid formation of the neural populations with respect to novel task sets.
%X In this work, we study how emotional interactions with a social partner can bootstrap increasingly complex behaviors such as social referencing. Our idea is that social referencing as well as facial expression recognition can emerge from a simple sensory-motor system involving emotional stimuli. Without knowing that the other is an agent, the robot is able to learn some complex tasks if the human partner has some "empathy" or at least "resonate" with the robot head (low level emotional resonance). Hence, we advocate the idea that social referencing can be bootstrapped from a simple sensory-motor system not dedicated to social interactions.
%X La recherche de points d'intérêt dans les couples stéréoscopiques d'images végétales revêt une importance dans le calcul du profil 3D d'un couvert végétal et en particulier du profil des hauteurs des plantes qui s'y trouvent. Cette méthodologie permet d'éviter de reconstruire en 3D tous les points de la scène mais fournit une bonne statistique sur les hauteurs des végétaux présents dans le couvert en ayant des temps de calcul plus raisonnables. Pour cela, l'article proposé fait le point sur les détecteurs de points d'intérêt les plus usités et fournit une nouvelle méthode de détection de points d'intérêt fondée sur l'algorithme LBP étendu aux images couleur et aux scènes complexes (univers non polyèdriques). La méthode proposée donne de meilleurs résultats que les détecteurs de Moravec et d'Harris ainsi que le détecteur SIFT dans la localisation et la répartition des points clés. Des essais en cours sur la mise en correspondance stéréoscopiques sont prometteurs.
%X Precision Agriculture is concerned with all sort of within-field variability, spatially and temporally, that reduces the efficacy of agronomic practices applied in a uniform way all over the field. Because of these sources of heterogeneity, uniform management actions strongly reduce the efficiency of the resource input to the crop (i.e., fertilization, water) or for the agrochemicals used for pest control (i.e. herbicide). In particular, weed plants are one of these sources of variability for the crop, as they occur in patches in the field. Detecting the location, size and internal density of these patches, along with identification of main weed species involved, open the way to a site-specific weed control strategy, where only patches of weeds would receive the appropriate herbicide (type and dose). Herein, the first stage of recognition method of vegetal species, the classification of soil and vegetation, is described and is based upon the kernel Fisher discriminant method (KFDM) and on Kernel Principal Analysis (KPCA).
%X In this paper, we examine cognitive radio systems that evolve dynamically over time as a function of changing user and en- vironmental conditions. To take into account the advantages of orthogonal frequency division multiplexing (OFDM) and recent advances in multiple antenna (MIMO) technologies, we consider a full MIMO-OFDM Gaussian cognitive radio system where users with several antennas communicate over multiple non-interfering frequency bands. In this dynamic context, the objective of the network's secondary users (SUs) is to stay as close as possible to their optimum power allo- cation and signal covariance profile as it evolves over time, with only local channel state information at their disposal. To that end, we derive an adaptive spectrum management policy based on the method of matrix exponential learning, and we show that it leads to no regret (i.e. it performs asymptotically as well as any fixed signal distribution, no matter how the system evolves over time). As it turns out, this online learning policy is closely aligned to the direction of change of the users' data rate function, so the system's SUs are able to track their individual optimum signal profile even under rapidly changing conditions.
%X In this paper, we consider the clustering of very large datasets distributed over a network of computational units using a decentralized K-means algorithm. To obtain the same codebook at each node of the network, we use a randomized gossip aggregation protocol where only small messages are ex- changed. We theoretically show the equivalence of the algorithm with a centralized K-means, provided a bound on the number of messages each node has to send is met. We provide experiments showing that the consensus is reached for a number of messages consistent with the bound, but also for a smaller number of messages, albeit with a less smooth evolution of the objective function.
%X In object recognition, the Bag-of-Words model assumes: i) extraction of local descriptors from images, ii) embedding these descriptors by a coder to a given visual vocabulary space which results in so-called mid-level features, iii) extracting statistics from mid-level features with a pooling operator that aggregates occurrences of visual words in images into so-called signatures. As the last step aggregates only occurrences of visual words, it is called as First-order Occurrence Pooling. This paper investigates higher-order approaches. We propose to aggregate over co-occurrences of visual words, derive Bag-of-Words with Second- and Higher-order Occurrence Pooling based on linearisation of so-called Minor Polynomial Kernel, and extend this model to work with adequate pooling operators. For bi- and multi-modal coding, a novel higher-order fusion is derived. We show that the well-known Spatial Pyramid Matching and related methods constitute its special cases. Moreover, we propose Third-order Occurrence Pooling directly on local image descriptors and a novel pooling operator that removes undesired correlation from the image signatures. Finally, Uni- and Bi-modal First-, Second-, and Third-order Occurrence Pooling are evaluated given various coders and pooling operators. The proposed methods are compared to other approaches (e.g. Fisher Vector Encoding) in the same testbed and attain state-of-the-art results.
%X The IRIM group is a consortium of French teams working on Multimedia Indexing and Retrieval. This paper describes its participation to the TRECVID 2013 semantic indexing and instance search tasks. For the semantic indexing task, our approach uses a six-stages processing pipelines for computing scores for the likelihood of a video shot to contain a target concept. These scores are then used for producing a ranked list of images or shots that are the most likely to contain the target concept. The pipeline is composed of the following steps: descriptor extraction, descriptor optimization, classiffication, fusion of descriptor variants, higher-level fusion, and re-ranking. We evaluated a number of different descriptors and tried different fusion strategies. The best IRIM run has a Mean Inferred Average Precision of 0.2796, which ranked us 4th out of 26 participants.
%X This paper shows that learning by imitation leads to a pos- itive effect not only in human behavior but also in the behavior of the autonomous agents (AA) in the eld of self-organized creation deposits. Indeed, for each agent, the individual discoveries (i.e. goals) have an ef- fect on the performance of the population level and therefore they induce a new learning capability at the individual level. Particularly, we show through a set of experiments that adding a simple imitation capability to our bio-inspired architecture allows increasing the ability of agents to share more information and improving the overall performance of the whole system. We will conclude with robotics' experiments which will feature how our approach applies accurately to real life environments.
%X Since swarm intelligence allows self-organization into an unfamiliar environment and adapting behaviors through simple individuals' interactions, we propose to realize a swarm multirobot organization with a fuzzy control. We introduce in this paper a fuzzy system for avoiding the collaboration stagnation and to improve the counter-ant algorithm (CAA). The robots' collaborative behavior is based on a hybrid approach combining the CAA and a fuzzy system learned by MAGAD-BFS (Multi-agent Genetic Algorithm for the Design of Beta Fuzzy System). A series of simulations enables us to discuss and validate both the effectiveness of the hybrid approach to the problem of environment exploration (i.e., for the purpose of cleaning an area) as well as the usefulness of MAGAD-BFS for learning the fuzzy knowledge base while tuning it and reducing its number of rules.
%X In this paper, we focus on the Generalized Belief Propagation (GBP) algorithm to solve trapping sets in Low-Density Parity-Check (LDPC) codes. Trapping sets are topological structures in Tanner graphs of LDPC codes that are not correctly decoded by Belief Propagation (BP), leading to exhibit an error-floor in the bit-error rate. Stemming from statistical physics of spin glasses, GBP consists in passing messages between groups of Tanner graph nodes. Provided a well-suited grouping, this algorithm proves to be a powerful decoder as it may lower harmful topological effects of the Tanner graph. We then propose to use GBP to break trapping sets and create a new decoder to outperform BP and to defeat error-floor.
%X La multiplication du nombre de cœurs de calcul présents sur les puces va de pair avec une augmentation des besoins en communication. C'est pour palier à ce problème que nous présentons dans cette article un réseau d'interconnexion sur puce utilisant la RF. Nous présentons les raisons du choix de la RF par rapport aux autres nouvelles technologies du domaine que sont l'optique et la 3D, l'architecture détaillée de ce réseau et d'une puce le mettant en œuvre ainsi que l'évaluation de sa faisabilité et de ses performances. Un des avantages potentiels de ce réseau d'interconnexion RF est la possibilité de faire du broadcast à faible coût, ce qui ouvre de nouvelles perspectives notamment en terme de gestion de la cohérence mémoire.
%X This paper considers dimensionality reduction in large decentralized networks with limited node-local computing and memory resources and unreliable point-to-point connectivity (e.g peer-to-peer, sensors or ad-hoc mobile networks). We propose an asynchronous decentralized algorithm built on a Gossip consensus protocol that perform Principal Components Analysis (PCA) of data spread over such networks. All nodes obtain the same local basis that span the global principal subspace. Reported experiments show that obtained bases both reach a consensus and accurately estimate the global PCA solution.
%X The next generation of multiprocessor systems on chip requires development and improvement of innovative implementations of optical networks on chip (ONoC). ONoC efficiency mostly relies on crosstalk between waveguides and on optical power loss. Taking into account new integration capabilities in order to achieve the best design compromise minimizing power loss and crosstalk is thus mandatory. In this paper, we propose a novel switch implementation based upon a multi-layer microring resonator, allowing efficient light coupling and switching between superposed waveguides. Such microring resonator avoids waveguide crossing, thus contributing to reduce propagation losses. A transmission loss from -4.8 to -7 dB, crosstalk between -17 and -29 dB, a quality factor about 202 allowing a bandwidth up to 1THz, are demonstrated with a 1.65 µm radius microring. Furthermore, this paper demonstrates that pure curvature loss remains less than -0.4 dB for microring radii less than 2µm thus allowing miniaturization of switch size.
%X In this paper we present an hardware realisation for an image coder used in the SmartEEG project. This collaborative project has the aim of the conception of a multimodal tool for EEG signal to allow transmission of a complete examination of a patient. We show that we can expect good performance on a FPGA board for the time consuming part of this tool that is the image coder.
%X Several descriptors have been proposed in the past for 3D shape analysis, yet none of them achieves best perfor- mance on all shape classes. In this paper we propose a novel method for 3D shape analysis using the covariance matrices of the descriptors rather than the descriptors them- selves. Covariance matrices enable efficient fusion of dif- ferent types of features and modalities. They capture, using the same representation, not only the geometric and the spa- tial properties of a shape region but also the correlation of these properties within the region. Covariance matrices, however, lie on the manifold of Symmetric Positive Defi- nite (SPD) tensors, a special type of Riemannian manifolds, which makes comparison and clustering of such matrices challenging. In this paper we study covariance matrices in their native space and make use of geodesic distances on the manifold as a dissimilarity measure. We demonstrate the performance of this metric on 3D face matching and recognition tasks. We then generalize the Bag of Features paradigm, originally designed in Euclidean spaces, to the Riemannian manifold of SPD matrices. We propose a new clustering procedure that takes into account the geometry of the Riemannian manifold. We evaluate the performance of the proposed Bag of Covariance Matrices framework on 3D shape matching and retrieval applications and demonstrate its superiority compared to descriptor-based techniques.
%X This paper investigates the use of recent visual features based on second-order statistics, as well as new pro- cessing techniques to improve the quality of features. More specifically, we present and evaluate Fisher Vectors (FV), Vec- tors of Locally Aggregated Descriptors (VLAD), and Vectors of Locally Aggregated Tensors (VLAT). These techniques are combined with several normalization techniques, such as power law normalization and orthogonalisation/whitening of descriptor spaces. Results on the UC Merced land use dataset shows the relevance of these new methods for land-use classification, as well as a significant improvement over Bag-of-Words.
%X An OLAP analysis can be defined as an interactive session during which an user launches queries over a data warehouse. The launched queries are often interdependent, and they can be either newly defined queries or they can be existing ones that are browsed and reused. Moreover, in a collaborative environment, queries may be shared among users. This notion of OLAP analysis has never been formally defined. In this paper, we propose a clear definition of this notion, by introducing a model for sharing, browsing and reusing OLAP queries over a data warehouse.
%X This paper presents an FPGA platform for the design and study of network of coupled All-Digital Phase Locked Loops (ADPLLs), destined for clock generation in large synchronous System on Chip (SoC). An implementation of a programmable and reconfigurable 4×4 ADPLL network is described. The paper emphasizes the difference between the FPGA and ASIC-based implementation of such a system, in particular, implementation of digitally controlled oscillators and phase-frequency detector. The FPGA-implemented network allows studying complex phenomena related to coupled ADPLL operation and exploiting stability issues and nonlinear behavior. A dynamic setup mechanism has been proposed for the network, allowing selecting the desirable synchronized state. Experimental results demonstrate the global synchronization of network and performance of the network for different configurations.
%X This paper describes the joint submission of Inria and Xerox to their joint participation to the FGCOMP'2013 challenge. Although the proposed system follows most of the standard Fisher classification pipeline, we describe a few key features and good practices that significantly improve the accuracy when specifically considering fine-grain classification tasks. In particular, we consider the late fusion of two systems both based on Fisher vectors, but for which we choose drastically design choices that make them very complementary. Moreover, we propose a simple yet effective filtering strategy, which significantly boosts the performance for several class domains.
%X This paper investigates the classification of photographic paper tex-tures using visual descriptors. Such classification is called fine grain due to the very low inter-class variability. We propose a novel image representation for photographic paper texture categorization, relying on the incorporation of a powerful local descriptor into an efficient higher-order model deviation where texture is represented by com-puting statistics on the occurrences of specific local visual patterns. We perform an evaluation on two different challenging datasets of photographic paper textures and show such advanced methods in-deed outperforms existing descriptors.
%X Trajectories can be encoded as attraction basin resulting from recruited associations between visually based localization and orientations to follow (low level behaviors). Navigation to different places according to some other multimodal information needs a particular learning. We propose a minimal model explaining such a behavior adaptation from non-verbal interaction with a teacher. Specific contexts can be recruited to prevent the behaviors to activate in cases the interaction showed they were inadequate. Still, the model is compatible with the recruitment of new low level behaviors. The tests done in simulation show the capabilities of the architecture, the limitations regarding the generalization and the learning speed. We also discuss the possible evolutions towards more bio-inspired models.
%X The Stimulus-Response (S-R) theory and Tolman's Cognitive Theory of behavior control both issued from behaviorism in the early XXth century still provide a relevant general framework to account for animal rewardbased adaptive behavior. In this paper, we propose a new paradigm for representing and implementing both the cognitive strategy and the S-R habit strategy within a unitary coding frame.
%X Corrective learning is an interesting paradigm for on-line learning of a trajectory from non-verbal interaction. We propose a model of action selection based on corrective interactions between the robot and a teacher. By using the appropriate correction signal, a minimal solution is to learn specific contexts inhibiting the wrong actions in order to let the appropriate behavior be exhibited in any circumstances. We present a solution to an action selection problem, where a mobile robot has to pick up objects in a given position of the environment.
%X In this paper, we tackle the storage and computational cost of linear projections used in dimensionality reduction for near duplicate image retrieval. We propose a new method based on metric learning with a lower training cost than existing methods. Moreover, by adding a sparsity constraint, we obtain a projection matrix with a low storage and projection cost. We carry out experiments on a well known near duplicate image dataset and show our algorithm behaves correctly. Retrieval performances are shown to be promising when compared to the memory footprint and the projection cost of the obtained sparse matrix.
%X A paradigm shift is apparent in Chip Multiprocessor (CMP) design, as the new performance bottleneck is becoming communication rather than computation. It is widely provisioned that number of cores on a single chip will reach thousands in a decade. Thus, new high rate interconnects such as optical or RF have been proposed by various researchers. However, these interconnect structures fail to provide essential requirements of heterogeneous on-chip traffic; bandwidth reconfigurability and broadcast support with a low complex design. In this paper we investigate the feasibility of a new Orthogonal Frequency Division Multiple Access (OFDMA) RF interconnect for the first time to the best of our knowledge. In addition we provide a novel dynamic bandwidth arbitration and modulation order selection policy, that is designed regarding the bimodal on-chip packets. The proposed approach decreases the average latency up to 3.5 times compared to conventional static approach.
%X Wireless capsule endoscopy (WCE) enables screening of the gastrointestinal tract by a swallowable imaging system. However, contemporary WCE systems have several limitations, which often result in low diagnostic yield. This paper introduces the concept of a next generation WCE system with embedded intelligence aiming to effectively minimize diagnostic errors. The proposed system is based on a novel wirelesslypowered hardware-software architecture integrating reconfigurable components that are optimized in terms of areatime complexity and power consumption. It integrates multispectral and 3D vision modules, and embedded intelligence for video quality control, accurate localization of the capsule and automatic detection of a broad spectrum of abnormalities. The feasibility of the proposed WCE system is qualitatively assessed with respect to the results obtained, and novel research directions are drawn.
%X —For low level behaviors, navigational trajectories can be encoded as attraction basin resulting from associations between visual based localization and directions to follow. The use of other sensory information such as contexts for modifying the behavior needs a specialized learning. In this paper, we propose a minimal model using multimodal contexts, and a mechanism for obtaining a better generalization of the contexts and creation of chunks. We briefly present the bases of the sensory-motor architecture, and explain the neurobiological principal inspiring this model. We also evaluate the proposed improvement on simulated signals and in a robotic navigational experiment.
%X The so-called self–other correspondence problem in imitation demands to find the transformation that maps the motor dynamics of one partner to our own. This requires a general purpose sensorimotor mechanism that transforms an external fixation-point (partner’s shoulder) reference frame to one’s own body-centered reference frame. We propose that the mechanism of gain-modulation observed in parietal neurons may generally serve these types of transformations by binding the sensory signals across the modalities with radial basis functions (tensor products) on the one hand and by permitting the learning of contextual reference frames on the other hand. In a shoulder–elbow robotic experiment, gain-field neurons (GF) intertwine the visuo-motor variables so that their amplitude depends on them all. In situations of modification of the body-centered reference frame, the error detected in the visuo-motor mapping can serve then to learn the transformation between the robot’s current sensorimotor space and the new one. These situations occur for instance when we turn the head on its axis (visual transformation), when we use a tool (body modification), or when we interact with a partner (embodied simulation). Our results defend the idea that the biologically-inspired mechanism of gain modulation found in parietal neurons can serve as a basic structure for achieving nonlinear mapping in spatial tasks as well as in cooperative and social functions.
%X In this paper, we propose a new method for taking into ac-count the spatial information in image categorization. More specifically, we remove the loss of spatial information in Bag of Words related methods by computing the image signature over specific regions selected by object detectors. We propose to select the detectors using Multiple Kernel Learning tech-niques. We carry out experiments on the well known VOC 2007 dataset, and show our semantic pooling obtains promis-ing results.
%X Visual learning with weak supervision is a promising re-search area, since it offers the possibility to build large image datasets at reasonable cost. In this paper, we address the prob-lem of weakly supervised object detection, where the goal is to predict the label of the image using object position as latent variable. We propose a new method that builds upon the La-tent Structural SVM (LSSVM) formalism. Specifically, we introduce an original coarse-to-fine approach that limits the evolution of the latent parameter subspace. This incremental strategy drives the learning towards better solutions, provid-ing a model with increased predictive accuracy. In addition, this leads to a significant speed up during learning and infer-ence compared to standard sliding window methods. Experi-ments carried out on Mammal dataset validate the good per-formances and fast training of the method compared to state-of-the-art works.
%X RESUME : La programmation de jeux vidéo est une motivation souvent donnée par les étudiants désireux de suivre une formation en sciences informatiques. Or implémenter un jeu vidéo est complexe et fait appel à beaucoup de notions dont la plus visible est l'Interface Homme-Machine (IHM). Cette dernière fait souvent obstacle à la proposition d'un sujet de projet portant sur le jeu vidéo. Pourtant, les jeux vidéo font appel aux notions fondamentales (algorith-miques, bases de données, réseaux, intelligence artificielle, etc.) des sciences informatiques. Nous montrons dans cet article que la plupart des jeux vidéo peuvent néanmoins être implémentés avec une IHM minimale qui pourra ensuite être étendue suivant la motivation des étudiants. Dans cet article, nous avons catégorisé les jeux existants suivant leur genre et les avons classés en fonction des compétences visées tout au long de la formation informatique à l'université de Cergy-Pontoise. Nous montrons comment nous avons mise en place des solutions pour encadrer les étudiants dans leur réalisation de projets de jeux vidéo. Les résultats sur 14 ans d'enseignements en projets de jeux vidéo montrent l'effica-cité de cette approche dans l'apprentissage des sciences informatiques.
%X Many methods have been recently proposed to deal with the large amount of data provided by the new remote sensing technologies. Several of those methods rely on the use of segmented regions. However, a common issue in region-based applications is the definition of the appropriate representation scale of the data, a problem usually addressed by exploiting multiple scales of segmentation. The use of multiple scales, however, raises new challenges related to the definition of effective and efficient mechanisms for extracting features. In this paper, we address the problem of extracting features from a hierarchy by proposing two approaches that exploit the existing relationships among regions at different scales. The H-Propagation propagates any histogram-based low-level descriptors. The BoW-Propagation approach uses the bag-of-visual-word model to propagate features along multiple scales. The proposed methods are very efficient as features need to be extracted only at the base of the hierarchy and yield comparable results to low-level extraction approaches.
%X In web-scale image retrieval, the most effective strategy is to ag-gregate local descriptors into a high dimensionality signature and then reduce it to a small dimensionality. Thanks to this strategy, web-scale image databases can be represented with small index and explored using fast visual similarities. However, the computation of this index has a very high complexity, because of the high di-mensionality of signature projectors. In this work, we propose a new efficient method to greatly reduce the signature dimensionality with low computational and storage costs. Our method is based on the linear projection of the signature onto a small subspace using a sparse projection matrix. We report several experimental results on two standard datasets (Inria Holidays and Oxford) and with 100k image distractors. We show that our method reduces both the projec-tors storage cost and the computational cost of projection step while incurring a very slight loss in mAP (mean Average Precision) per-formance of these computed signatures.
%X The need for efficient tools to index and retrieve 3D content becomes even more acute. This paper presents a fully automatic 3D-object retrieval method. It consists of two main steps namely shape signature extraction to describe the shape of objects, and similarity computing to compute similarity between objects. In the first step (signature extraction), we use a shape descriptor called geodesic cords. This descriptor can be seen as a probability distribution sampled from a shape function. In the second step (similarity computing), a global distance, based on belief function theory, is computed between each pairwise of descriptors corresponding respectively to an object query and an object from a given database. Experiments on commonly-used benchmarks demonstrate that our method obtains competitive performance compared to 3D-object retrieval methods from the state-of-the-art.
%X The intent of 3D-models classification is to find categories of similar objects according to their shapes. This task is a challenging and important problem in 3D-mining and shape processing. In this paper, we present a novel method to categorize 3D-objects based on view-based descriptors. The proposed method goes into two stages. The first stage corresponds to the training in which 3D-objects in the same category are processed and a set of representative 2D views is selected, The second stage corresponds to the labelling in which unknown objects are classified using a belief based classifier. The experimental results obtained on the Shrec07 datasets show that the system efficiently performs in categorizing 3D-models.
%X — This paper introduces flexible radio techniques inside integrated circuits in order to tackle the interconnect issue for many-core chips. We propose to take benefits from OFDMA for a RF-interconnect associated to a carrier allocation policy and adaptive modulation. A 20 GHz bandwidth is shared between 32 tilesets made of 32 tiles of 4 cores each, for a 4096 cores chip. We adopt a cognitive radio approach in order to dynamically share 1024 carriers, which avoids inter-cluster communication contention and decreases latency compared to conventional static approaches.
%X — This paper introduces flexible radio techniques inside integrated circuits in order to tackle the interconnect issue for many-core chips. We propose to take benefits from OFDMA for a RF-interconnect associated to a carrier allocation policy and adaptive modulation. A 20 GHz bandwidth is shared between 32 tilesets made of 32 tiles of 4 cores each, for a 4096 cores chip. We adopt a cognitive radio approach in order to dynamically share 1024 carriers, which avoids inter-cluster communication contention and decreases latency compared to conventional static approaches.
%X Local descriptors are the ground layer of recognition feature based systems for still images and video. We propose a new framework to explain local descriptors. This framework is based on the descriptors decomposition in three levels: primitive extraction, primitive coding and code aggregation. With this framework, we are able to explain most of the popular descriptors in the literature such as HOG, HOF, SURF. We propose two new projection methods based on approximation with oscillating functions basis (sinus and Legendre polynomials). Using our framework, we are able to extend usual descriptors by changing the code aggregation or adding new primitive coding method. The experiments are carried out on images (VOC 2007) and videos datasets (KTH, Hollywood2 and UCF11), and achieve equal or better performances than the literature.
%X Visualization of the entire length of the gastrointestinal tract through natural orifices is a challenge for endoscopists. Videoendoscopy is currently the "gold standard" technique for diagnosis of different pathologies of the intestinal tract. Wireless Capsule Endoscopy (WCE) has been developed in the 1990's as an alternative to videoendoscopy to allow direct examination of the gastrointestinal tract without any need for sedation. Nevertheless, the systematic post-examination by the specialist of the 50,000 (for the small bowel) to 150,000 images (for the colon) of a complete acquisition using WCE remains time-consuming and challenging due to the poor quality of WCE images. In this article, a semiautomatic segmentation for analysis of WCE images is proposed. Based on active contour segmentation, the proposed method introduces alpha-divergences, a flexible statistical similarity measure that gives a real flexibility to different types of gastrointestinal pathologies. Results of segmentation using the proposed approach are shown on different types of real-case examinations, from (multi-) polyp(s) segmentation, to radiation enteritis delineation.
%X —In this paper we size a RF intra-chip communications based on Orthogonal Frequency Division Multiple Access (OFDMA) modulation which allows data rate and message recipient reconfiguration. Firstly, we present the advantages of this modulation such as providing flexible and high-speed data transmission. Then, we study the impact of the RF-interconnect channel shape on the transmission in terms of required transmission power. Finally, we present the effect of the channel composed of the line and its multiple access on the transfer of information and we perform a channel equalization to overcome this undesired effect.
%X We present an information system supporting the diagnosis and treatment of rice diseases. The heart of the system is a software mediator allowing farmers to formulate distant queries regarding a disease and responding to farmer queries by recommending a treatment of the disease if one exists. The disease diagnosis and the recommended treatment are based on expert knowledge stored in the mediator database. The processing of farmer queries may involve direct access to the mediator by farmers, online communication between the mediator and the experts, or direct dialog between the farmer and an expert. Our information model is generic in the sense that it can be used to support the needs of farmers in a variety of similar environments, with minor changes. The system presented in this paper is currently under development as part of a Franco-Thai project and aims to assist farmers in the quick diagnosis of rice diseases and their treatment.
%X This paper presents a set of algorithms dedicated to the 3D modeling of historical buildings from a collection of old ar-chitecture plans, including floor plans, elevations and cut-offs. Image processing algorithms help to detect and local-ize main structures of the building from the floor plans (thick and thin walls, openings). The extrusion of the walls allow us to build a first 3D model. We compute height informations and add textures to the model by analyzing the elevation im-ages from the same collection of documents. We applied this pipeline to XVIII th century plans of the Château de Versailles, and show results for two different parts of the Château.
%X Distributed multimedia applications have emerged at an increasing rate during the last decade in several domains (video conferencing, e-health, virtual meeting rooms, etc). This has created several new challenging problems related to the data integration and fragmentation, user-oriented and adaptive interfaces, real time and network performances, etc.In this paper, we focus on the problem of data(base) fragmentation in a multimedia context. We recall in this respect that data fragmentation consists of reducing irrelevant data accesses by grouping data frequently accessed together in dedicated segments. We mainly address the issue of query and predicate implication required in current fragmentation algorithms, and provide a formal approach to identify such implications, in order to partition multimedia data efficiently. Our approach is capable of considering multimedia-based as well as semantic comparisons, both ignored in current studies but required when multimedia data come to play.
%X In this paper we propose to tackle human actions indexing by introducing a new local motion de-scriptor based on a model of the optical flow. We pro-pose to apply a coding step to vector field before the modeling. We use two modeling, a spatial model and a temporal model. The spatial model is computed by pro-jection of optical flow onto bivariate orthogonal poly-nomials. Then, the time evolution of spatial coefficients is modeled with a one dimension polynomial basis. To perform the action classification, we extend recent still image signatures using local descriptors to our proposal and combine them with linear SVM classifiers. The ex-periments are carried out on the well known UCF11 dataset and on the more challenging Hollywood2 ac-tion classification dataset and show promising results.
%X Face to the limitations of the classical computation model, neuromorphic systems are envisaged as an alternative. Interesting properties of the neural systems are its highly parallel computation, and its self-organizing capabilities. In this paper, we study how to bring distribution in the hardware computation of these neural models, especially in the case of self-organizing maps. We propose a bio-inspired hardware substrate in which a grid of neural processing elements will support a set of neurocognitive processes. We describe an original distributed artificial neural network adapted to hardware scalability and provide the results of its implementation as a set of NPUs onto FPGA devices.
%X A recent trend in several robotics tasks is to consider vision as the primary sense to perceive the environment or to interact with humans. Therefore, vision processing becomes a central and challenging matter for the design of real-time control architectures. We follow in this paper a biological inspiration to propose a real-time and embedded control system relying on visual attention to learn specific actions in each place recognized by our robot. Faced with a performance challenge, the attentional model allows to reduce vision processing to a few regions of the visual field. However, the computational complexity of the visual chain remains an issue for a processing system embedded onto an indoor robot. That is why we propose as the first part of our system, a full-hardware architecture prototyped onto reconfigurable devices to detect salient features at the camera frequency. The second part learns continuously these features in order to implement specific robotics tasks. This neural control layer is implemented as embedded software making the robot fully autonomous from a computation point of view. The integration of such a system onto the robot enables not only to accelerate the frame rate of the visual processing, to relieve the control architecture but also to compress the data-flow at the output of the camera, thus reducing communication and energy consumption. We present in this paper the complete embedded sensorimotor architecture and the experimental setup. The presented results demonstrate its real-time behavior in vision-based navigation tasks.
%X The so-called self–other correspondence problem in imitation demands to find the transformation that maps the motor dynamics of one partner to our own. This requires a general purpose sensorimotor mechanism that transforms an external fixation-point (partner’s shoulder) reference frame to one’s own body-centered reference frame. We propose that the mechanism of gain-modulation observed in parietal neurons may generally serve these types of transformations by binding the sensory signals across the modalities with radial basis functions (tensor products) on the one hand and by permitting the learning of contextual reference frames on the other hand. In a shoulder–elbow robotic experiment, gain-field neurons (GF) intertwine the visuo-motor variables so that their amplitude depends on them all. In situations of modification of the body-centered reference frame, the error detected in the visuo-motor mapping can serve then to learn the transformation between the robot’s current sensorimotor space and the new one. These situations occur for instance when we turn the head on its axis (visual transformation), when we use a tool (body modification), or when we interact with a partner (embodied simulation). Our results defend the idea that the biologically-inspired mechanism of gain modulation found in parietal neurons can serve as a basic structure for achieving nonlinear mapping in spatial tasks as well as in cooperative and social functions.
%X Correlation between channel state and source symbol is under investigation for a joint source-channel coding problem. We investigate simultaneously the lossless transmission of information and the empirical coordination of channel inputs with the symbols of source and states. Empirical coordination is achievable if the sequences of source symbols, channel states, channel inputs and channel outputs are jointly typical for a target joint probability distribution. We characterize the joint distributions that are achievable under lossless decoding constraint. The performance of the coordination is evaluated by an objective function. For example, we determine the minimal distortion between symbols of source and channel inputs for lossless decoding. We show that the correlation source/channel state improves the feasibility of the transmission.
%X This paper introduces a method to optimize the configuration of a 3D helmet-mounted antenna array carried by emergency rescuers expected to locate natural disaster survivors. Configuration optimization is based on metrics extracted from the 3D Fisher Information Matrix (FIM) relative to the considered array architecture for single source Direction Of Arrival (DOA) estimation. Comprehensive simulations illustrate the optimization metrics behaviour with regard to survivor's emitter 3D DOA and array configuration. Design constraints related to helmet-mounted antenna requirements reduce the usually exhausting FIM based design optimization to a 2 degrees of freedom exploration of the array DOA estimation performances.
%X This paper presents the BlenderCAVE project, which extends the 3D creation content software Blender and its Game Engine (BGE) to Virtual Reality (VR) applications. Based on a multi-screen non-stereoscopic adaptation of the BGE [Gascon et al., 2010], BlenderCAVE now integrates a complete framework dedicated to Virtual Reality (VR), compatible with the three main Operating Systems for any given VR architecture configuration. It has been developed by audio and VR researchers with support from the Blender Community on LIMSI's state of the art VR platforms. Acting as a Scene Graph, BlenderCAVE handles multi-screen/multi-user tracked stereoscopic rendering through an efficient low-level master/slave synchronization process while controlling spatial audio rendering (ambisonic, multi-user binaural, WFS, etc.) and haptic events through OSC and VRPN protocols. The scene creation process itself is reduced to simple Blender manipulations including basic python programming easily carried out using standard laptops. OSC client and spatial audio rendering methods have thus far been implemented in the Max/MSP Audio Programming Environment.
%X This paper is part of a project concerned with the improvement of audio radiogoniometer design ergonomics and sound aesthetic. It introduces a virtual prototyping implementation of a simple radiogoniometer along with a methodology to assess its ecological validity. Said methodology involves a performance comparison between two different radiogoniometer designs, both implemented as virtual prototypes. While suggested assessment achievement supposes a companion study in a real environment (based on a physical prototype), significant results have already been gathered regarding the impact of the virtual environment on the virtual prototype validity.
%X This article surveys the existing literature on the most widely used factor models employed in the realm of a financial asset pricing field. Through the concrete application of evaluating risks in the hedge fund industry, this article demonstrates that signal processing techniques are an interesting alternative to the selection of factors and can provide more efficient estimation procedure than classical techniques.
%X The Progressive Edge Growth (PEG) algorithm is one of the most widely-used methods for constructing fi nite length LDPC codes. In this paper we consider the PEG algorithm together with a scheduling distribution, which specifi es the order in which edges are established in the graph. The goal is to find a scheduling distribution that yields "the best" performance in terms of decoding overhead, performance metric speci c to erasure codes and widely used for upper-layer forward error correction (UL-FEC). We rigorously formulate this optimization problem, and we show that it can be addressed by using genetic optimization algorithms. We also exhibit PEG codes with optimized scheduling distribution, whose decoding overhead is less than half of the decoding overhead of their classical-PEG counterparts.
%X The aim of the present paper is to study the effects of Hebbian learning in random recurrent neural networks with biological connectivity, i.e. sparse connections and separate populations of excitatory and inhibitory neurons. We furthermore consider that the neuron dynamics may occur at a (shorter) time scale than synaptic plasticity and consider the possibility of learning rules with passive forgetting. We show that the application of such Hebbian learning leads to drastic changes in the network dynamics and structure. In particular, the learning rule contracts the norm of the weight matrix and yields a rapid decay of the dynamics complexity and entropy. In other words, the network is rewired by Hebbian learning into a new synaptic structure that emerges with learning on the basis of the correlations that progressively build up between neurons. We also observe that, within this emerging structure, the strongest synapses organize as a small-world network. The second effect of the decay of the weight matrix spectral radius consists in a rapid contraction of the spectral radius of the Jacobian matrix. This drives the system through the "edge of chaos" where sensitivity to the input pattern is maximal. Taken together, this scenario is remarkably predicted by theoretical arguments derived from dynamical systems and graph theory.
%X Multi-dimensional databases have been designed to provide decision makers with the necessary tools to help them understand their data. Compared to transactional data, this framework is par- ticular as the datasets contain huge volumes of historized and aggregated data defined over a set of dimensions, which can be arranged through multiple levels of granularities. Many tools have been proposed to query the data and navigate through the levels of granularity. However, automatic tools are still missing to mine this type of data, in order to discover regular specific patterns. In this paper, we present a method for mining sequential patterns from multi-dimensional databases, taking at the same time advantage of the different dimensions and levels of granularity, which is original compared to existing work. The necessary definitions and algorithms are extended from regular sequential patterns to this particular case. Experiments are reported, showing the interest of this approach.
%X Sales on the Internet have increased sig- nificantly during the last decade, and so, it is crucial for companies to retain customers on their web site. Among all strategies towards this goal, providing customers with a flexible search tool is a crucial issue. In this paper, we propose an approach, called TIGER, for handling such flexibility automatically. More precisely, if the search criteria of a given query to a relational table or a Web catalog are too restric- tive, our approach computes a new query combining extensions of the criteria. This new query maximizes the quality of the answer, while being as close as possible to the original query. Experiments show that our approach improves the quality of queries, in the sense explained just above.
%X In this paper, we address the issue of mining gradual classification rules. In general, gradual patterns refer to regularities such as ''The older a person, the higher his salary''. Such patterns are extensively and successfully used in command-based systems, especially in fuzzy command applications. However, in such applications, gradual patterns are supposed to be known and/or provided by an expert, which is not always realistic in practice. In this work, we aim at mining from a given training dataset such gradual patterns for the generation of gradual classification rules. Gradual classification rules thus refer to rules where the antecedent is a gradual pattern and the conclusion is a class value.
%X Cette présentation à pour but de présenter les résultats d'un groupe de travail interGDR sur les systèmes embarqués pour la santé.
%X The research work presented in this document addresses two different problems of designing embedded digital architectures for information processing applications. The first axis concerns the study and design of architectural models for channel decoding systems used in digital communication applications. These decoders are based on LDPC (Low Density Parity Check) codes that are currently proposed as error correcting codes in several transmission standards. We are notably interested in the wireless digital video broadcast standard DVB-S2. The hardware implementation of decoding algorithms used in these architectures necessitates a fine tradeoff between the parallelism degree, computation scheduling and available resources. Another study on reducing the complexity of the decoding algorithms working on non binary codes is also presented, and could help to define the associated architecture. The second research axis generalises the problem to the design of highly integrated SoC (System-on-Chip) architectures exhibiting flexibility, adaptability and dynamic hardware reconfiguration capacities. The use of an embedded real-time operating system becomes necessary for managing such architectures. Designing these architectures by using classical design methods would thus fail. The second part of the work concerns new exploration and design methodologies for reconfigurable architectures. The problem of modelling embedded operating systems and the design of SDR (Software-Defined Radio) applications and platforms are presented.
%X Les travaux de recherche dont la synthèse est présentée dans ce document portent sur deux aspects de la conception d'architectures numériques embarquées pour des applications de traitement de l'information. Le premier axe concerne l'étude et la conception de modèles architecturaux pour les décodeurs de canal utilisés dans les communications numériques. Les décodeurs étudiés sont basés sur les codes LDPC (Low Density Parity Check codes) qui, depuis quelques années, sont proposés comme codes correcteurs d'erreurs dans plusieurs normes de transmission. On s'intéresse en particulier à la norme DVB-S2 de radio-diffusion de programmes multimédia. Ces architectures de décodeurs mettent en oeuvre des algorithmes dont les réalisations matérielles reposent sur une adéquation fine entre le taux de parallélisme, l'ordonnancement des calculs et les quantités de ressources nécessaires. Une étude sur la réduction de complexité des algorithmes de décodage LDPC non binaires, préalable à la définition d'une architecture associée est également présentée. Le deuxième axe de recherche étend la problématique aux architectures très fortement intégrées, de type SoC (systèmes sur puces), et qui disposent de capacités de flexibilité, d'adaptabilité et de reconfiguration matérielle dynamique. La présence d'un système d'exploitation temps-réel embarqué devient alors nécessaire pour gérer de telles architectures et rend inadaptées les méthodes classiques de conception. Le deuxième axe des travaux porte sur de nouvelles méthodologies d'exploration et de conception d'architectures reconfigurable. Le cas de la modélisation des systèmes d'exploitation embarqués est abordé ainsi que le cas de la conception des applications et plates-formes pour la radio-logicielle.
%X This thesis deals with the hardware application of the software concepts of middleware and software architecture based on components, containers and connectors within Field-Programmable Gate Arrays (FPGAs). The target application domain is Software Deﬁned Radio (SDR) compliant with the Software Communications Architecture (SCA). With the SCA, software radio applications are broken into functional waveform components to be deployed on heterogeneous and distributed hardware/software radio platforms. These components provide and require abstract software interfaces described using operation signatures in the Uniﬁed Modeling Language (UML) and/or the Interface Deﬁnition Language (IDL) of the Common Object Request Broker Architecture (CORBA) middleware, both standardized by an international software industry consortium called Object management Group (OMG). The portability and reusability needs of these business components require that their abstract interfaces deﬁned at a system level are independent of a software or hardware implementation and can be indifferently translated into a software programming language like C/C++, a system language like SystemC at transaction level (Transaction Level Modeling - TLM), or a hardware description language like VHDL or SystemC at Register Transfer Level (RTL). The interoperability need of SDR components requires transparent communications regardless of their hardware/software implementation and their distribution. These ﬁrst needs were addressed by formalizing mapping rules between abstract components in OMG IDL3 or UML2, signal based hardware components described in VHDL or SystemC RTL, and system components in SystemC TLM. The second requirement was addressed by prototyping a hardware middleware using transparently memory mapping and two message protocols: CORBA General Inter Object Request Broker Protocol (GIOP) and SCA Modem Hardware Abstraction layer (MHAL).
%X Cette thèse s'intéresse à la déclinaison matérielle des concepts logiciels d'intergiciel et d'architecture logicielle à base de composants, conteneurs et connecteurs dans les réseaux de portes programmables in situ (Field-Programmable Gate Array - FPGA). Le domaine d'applications ciblé est la radio déﬁnie logiciellement (Software Deﬁned Radio (SDR)) conforme au standard Software Communications Architecture) (SCA). Avec le SCA, les applications radio sont décomposées en composants fonctionnels, qui sont déployés sur des plateformes radios hétérogènes et distribuées. Ces composants fournissent et requièrent des interfaces logicielles abstraites décrites sous forme de signatures d'opérations dans le langage de modélisation uniﬁé appelé Uniﬁed Modeling language (UML) et/ou le langage de déﬁnition d'interface (Interface Deﬁnition Language - IDL) de l'intergiciel CORBA (Common Object Request Broker Architecture) standardisé par un consortium industriel appelé Object Management Group (OMG). Les besoins de portabilité et de réutilisation de ces composants requièrent que leurs interfaces abstraites déﬁnies au niveau système soient indépendantes d'une implémentation logicielle ou matérielle et puissent être indifféremment traduites dans un langage de programmation logiciel tel que C/C++, un langage système tel que SystemC au niveau transaction (Transaction Level Modeling - TLM), ou un langage de description matériel tel que VHDL ou SystemC au niveau registre (Register Transfer Level - (RTL)). Le besoin d'interopérabilité de ces composants requière des communications transparentes quelques soient leur implémentation logicielle ou matérielle et leur distribution. Ces premiers besoins ont été adressés en formalisant des règles de mise en correspondance entre des composants abstraits en OMG IDL3 ou UML2, des composants matériels à base de signaux en VHDL ou SystemC RTL, et des composants systèmes en SystemC TLM. Le deuxième besoin a été adressé en prototypant un intergiciel matériel utilisant de façon transparente le mapping mémoire et deux protocoles messages: CORBA General Inter-Object Request Broker Protocol (GIOP) et SCA Modem Hardware Abstraction Layer (MHAL).
%X The scope of this thesis is to propose solutions to improve the performances of the CMOS transistor only simulated inductors (TOSI) aiming RF filtering applications. We are interested in TOSI architectures because they prove better performances than the classical gm–C filters, being superior with respect to the number of transistors, power consumption, frequency capability and chip area. Furthermore, TOSI architectures have many potential applications in RF design. In the general context of the multi–standard trend followed by wireless transceivers, TOSI based RF filters may offer the possibility of implementing reconfigurable devices. However, satisfying the telecommunications requirements is not an easy task therefore high order TOSI based filters should be implemented. Consequently, using good second order TOSI cells is a matter of the utmost importance and we propose a novel quality factor tuning principle which offers an almost independent tuning of self resonant frequency and quality factor for simulated inductors. An improved TOSI architecture with increased frequency capability is also reported.
%X In this last decade, the digital revolution results in a massive increase of digital picture quantities. The database sizes grow much faster than the processing capacity of computers. The current search engines which have been conceived for smaller data volumes do not any more allow to perform retrieval in these new corpuses with acceptable response times for users. In this thesis, we developed scalable content-based image search engines. At first, we considered automatic search engines where images are indexed with global histograms. Secondly, we focused on more sophisticated engines allowing to improve the search quality by working with bag of features. At last, we proposed a strategy to reduce the complexity of interactive search engines which allow to iteratively improve results by using labels that the users supply to the system during search sessions.
%X Avec la révolution numérique de cette dernière décennie, la quantité de photos numériques mise à disposition de chacun augmente plus rapidement que la capacité de traitement des ordinateurs. Les outils de recherche actuels ont été conçus pour traiter de faibles volumes de données. Leur complexité ne permet généralement pas d'effectuer des recherches dans des corpus de grande taille avec des temps de calculs acceptables pour les utilisateurs. Dans cette thèse, nous proposons des solutions pour passer à l'échelle les moteurs de recherche d'images par le contenu. Dans un premier temps, nous avons considéré les moteurs de recherche automatique traitant des images indexées sous la forme d'histogrammes globaux. Le passage à l'échelle de ces systèmes est obtenu avec l'introduction d'une nouvelle structure d'index adaptée à ce contexte qui nous permet d'effectuer des recherches de plus proches voisins approximées mais plus efficaces. Dans un second temps, nous nous sommes intéressés à des moteurs plus sophistiqués permettant d'améliorer la qualité de recherche en travaillant avec des index locaux tels que les points d'intérêt. Dans un dernier temps, nous avons proposé une stratégie pour réduire la complexité de calcul des moteurs de recherche interactifs. Ces moteurs permettent d'améliorer les résultats en utilisant des annotations que les utilisateurs fournissent au système lors des sessions de recherche. Notre stratégie permet de sélectionner rapidement les images les plus pertinentes à annoter en optimisant une méthode d'apprentissage actif.
%X Au cours des cinq dernières années, mes thèmes de recherche furent orientés autour des trois axes suivants : •la conception et l'optimisation asymptotique de récepteurs itératifs pour les communications numériques, comme par exemple l'analyse et l'optimisation des codes LDPC ou familles dérivées pour différents types de canaux, turbo-égalisation, décodage source-canal conjoint ; •la conception, le décodage et l'optimisation de codes définis sur les graphes pour les tailles finies, comme par exemple l'optimisation des codes LDPC non binaires à taille finie et certaines familles dérivées ou le décodage itératif non binaire de codes binaires ; •l'allocation de ressources, la conception et l'optimisation de systèmes à composantes itératives pour les canaux sans fil (par exemple système à retransmissions (HARQ) pour canaux sélectifs en fréquence, AMC pour l'ultra-large bande, protection inégale contre les erreurs et allocation de codes correcteurs).
%X This last decade, modeling of 3D city became one of the challenges of multimedia search and an important focus in object recognition. In this thesis we are interested to locate various primitive, especially the windows, in the facades of Paris. At first, we present an analysis of the facades and windows properties. Then we propose an algorithm able to extract automatically window candidates. In a second part, we discuss about extraction and recognition primitives using graph matching of contours. Indeed an image of contours is readable by the human eye, which uses perceptual grouping and makes distinction between entities present in the scene. It is this mechanism that we have tried to replicate. The image is represented as a graph of adjacency of segments of contours, valued by information orientation and proximity to edge segments. For the inexact matching of graphs, we propose several variants of a new similarity based on sets of paths, able to group several contours and robust to scale changes. The similarity between paths takes into account the similarity of sets of segments of contours and the similarity of the regions defined by these paths. The selection of images from a database containing a particular object is done using a KNN or SVM classifier.
%X Cette dernière décennie, la modélisation des villes 3D est devenue l'un des enjeux de la recherche multimédia et un axe important en reconnaissance d'objets. Dans cette thèse nous nous sommes intéressés à localiser différentes primitives, plus particulièrement les fenêtres, dans les façades de Paris. Dans un premier temps, nous présentons une analyse des façades et des différentes propriétés des fenêtres. Nous en déduisons et proposons ensuite un algorithme capable d'extraire automatiquement des hypothèses de fenêtres. Dans une deuxième partie, nous abordons l'extraction et la reconnaissance des primitives à l'aide d'appariement de graphes de contours. En effet une image de contours est lisible par l'oeil humain qui effectue un groupement perceptuel et distingue les entités présentes dans la scène. C'est ce mécanisme que nous avons cherché à reproduire. L'image est représentée sous la forme d'un graphe d'adjacence de segments de contours, valué par des informations d'orientation et de proximité des segments de contours. Pour la mise en correspondance inexacte des graphes, nous proposons plusieurs variantes d'une nouvelle similarité basée sur des ensembles de chemins tracés sur les graphes, capables d'effectuer les groupements des contours et robustes aux changements d'échelle. La similarité entre chemins prend en compte la similarité des ensembles de segments de contours et la similarité des régions définies par ces chemins. La sélection des images d'une base contenant un objet particulier s'effectue à l'aide d'un classifieur SVM ou kppv. La localisation des objets dans l'image utilise un système de vote à partir des chemins sélectionnés par l'algorithme d'appariement.
%X The decreasing cost of stockage and availability of high quality numeric media allow today the building of huge image databases in various scopes. These databases are often under exploited, because of the lack of tools to access the information they contain. In this thesis, we propose a set of learning techniques for image category search, gathered in the RETIN 2 system. Concerning indexing, we propose a quantization method for a very large number of vectors in order to build the image indexes as histograms, and also a kernel approach of similarity. Concerning interactive search, we study several classification methods, and present active learning which aims at selecting the pictures the user should label. We propose several methods in an active learning architecture which deal with the particular characteristics of the CBIR context. At last, we study the problem of the re-use of labels users have given during retrieval sessions. We present the weakly aspect of this learning problem in our context. We propose two methods which optimize the similarity between images in the scope of kernel functions. The system is evaluated on a generalist image database of several thousands of images.
%X La diminution du coût de stockage et la disponibilité de techniques de numérisation de haute qualité permettent aussi aujourd'hui de constituer de très grandes bases d'images dans des domaines variés. Ces bases sont souvent immenses et généralement sous-exploitées faute d'outils pour accéder à l'information qu'elles contiennent. Dans cette thèse, nous proposons un ensemble de techniques d'apprentissage pour la recherche de catégories d'images, regroupées au sein du système RETIN 2. Du point de vue de l'indexation, nous proposons une méthode de quantification d'un très grand nombre de vecteurs pour la construction de signatures sous la forme d'histogrammes, ainsi qu'une approche de la similarité par fonction noyau. Du point de vue de la recherche interactive, nous étudions de nombreuses techniques de classification, et présentons l'apprentissage actif, qui offre un cardre formel pour la sélection des images à faire annoter par l'utilisateur. Nous proposons plusieurs méthodes au sein d'une architecture d'apprentissage actif qui permettent de faire face aux caratéristiques particulières de la recherche interactive d'images. Le dernier point important que nous avons étudié concerne la ré-utilisation des annotations que les utilisateurs fournissent lors des sessions de recherche. Nous présentons le caractère faiblement supervisé de ce problème d'apprentissage dans notre contexte. Nous proposons deux méthodes qui s'appuient sur une modification des similarités entre les images dans le cadre de l'utilisation de fonctions noyaux. Le système est évalué sur une base d'images généraliste de plusieurs milliers d'images.
%X The problem of mining frequent queries in a database has motivated many research efforts during the last two decades. This is so because many interesting patterns, such as association rules, exact or approximative functional dependencies and exact or approxi- mative conditional functional dependencies can be easily retrieved, which is not possible using standard techniques. However, the problem mining frequent queries in a relational database is not easy because, on the one hand, the size of the search space is huge (because encompassing all possible queries that can be addressed to a given database), and on the other hand, testing whether two queries are equivalent (which entails redundant support computations) is NP-Complete. In this thesis, we focus on Projection-Selection-Join (PSJ) queries, assuming that the database is defined over a star schema. In this setting, we define a pre-ordering (q ≤ q′) between queries and we prove the following basic properties : 1. The support measure is anti-monotonic with respect to ≤, and 2. Defining q ≡ q′ if and only if q ≤ q′ and q′ ≤ q, all equivalent queries have the same support. The main contributions of the thesis are, on the one hand to formally study properties of the pre-ordering and the equivalence relation mentioned above, and on the other hand, to prose a level-wise, Apriori like algorithm for the computation of all frequent queries in a relational database defined over a star schema. Moreover, this algorithm has been imple- mented and the reported experiments show that, in our approach, runtime is acceptable, even in the case of large fact tables.
%X Au cours de ces dernières années, le problème de la recherche de requêtes fréquentes dans les bases de données est un problème qui a suscité de nombreuses recherches. En effet, beaucoup de motifs intéressants comme les règles d'association, des dépendances fonction- nelles exactes ou approximatives, des dépendances fonctionnelles conditionnelles exactes ou approximatives peuvent être découverts simplement, contrairement au méthodes clas- siques qui requièrent plusieurs transformations de la base pour extraire de tels motifs. Cependant, le problème de la recherche de requêtes fréquentes dans les bases de données relationnelles est un problème difficile car, d'une part l'espace de recherche est très grand (puisque égal à l'ensemble de toutes les requêtes pouvant être posées sur une base de données), et d'autre part, savoir si deux requêtes sont équivalentes (donc engendrant les calculs de support redondants) est un problème NP-Complet. Dans cette thèse, nous portons notre attention sur les requêtes de type Projection- Selection-Jointure (PSJ), et nous supposons que la base de données est définie selon un schéma étoile. Sous ces hypothèses, nous définissons une relation de pré-ordre (≤) entre les requêtes et nous montrons que : 1. La mesure de support est anti-monotone par rapport à ≤, et 2. En définissant, q ≡ q′ si et seulement si q ≤ q′ et q′ ≤ q, alors toutes les requêtes d'une même classe d'équivalence ont même support. Les principales contributions de cette thèse sont, d'une part d'étudier formellement les propriétés du pré-ordre et de la relation d'équivalence ci-dessus, et d'autre part, de pro- poser un algorithme par niveau de type Apriori pour rechercher l'ensemble des requêtes fréquentes d'une base de données définie sur un schéma étoile. De plus, cet algorithme a été implémenté et les expérimentations que nous avons réalisées montrent que, selon notre approche, le temps de calcul des requêtes fréquentes dans une base de données définie sur un schéma étoile reste acceptable, y compris dans le cas de grandes tables de faits.
%X Graphs are models of representation that can model many type of documents. In this thesis, we focus on their use for research in multimedia databases. We begin by presenting the theory of graphs and around an overview of methods that have been proposed for matching. Then, we are particularly interested in their use for recognition forms and multimedia indexing. In order to respond in the most generic possible different research problems, we propose to work within the framework of kernel functions. This framework allows to separate the problems related to the nature of the documents those introduced by the di fferent types of research. Thus, all our energy is devoted to the design of mapping functions, but bearing in mind that they must meet a number mathematical properties. In this context, we propose new solutions that better meet the speci c graphs from primitive and visual descriptors. We also present algorithms to quickly assess these functions. Finally, we present experiments that highlight these different characteristics and experiences that show advantages of our models with respect to the literature.
%X Les graphes sont des modèles de représentation qui permettent de modéliser un grand nombre de type de documents. Dans cette thèse, nous nous intéressons à leur utilisation pour la recherche dans des bases de données multimédia. Nous commençons par présenter la théorie autour des graphes ainsi qu'un aperçu des méthodes qui ont été proposées pour leur mise en correspondance. Puis, nous nous intéressons plus particulièrement à leur utilisation pour la reconnaissance des formes et l'indexation multimédia. Dans le but de répondre de la manière la plus générique possible aux différents problèmes de recherche, nous proposons de travailler dans le cadre des fonctions noyaux. Ce cadre permet de séparer les problèmes liées à la nature des documents de ceux apportés par les différents types de recherche. Ainsi, toute notre énergie est consacrée à la conception de fonctions de mise en correspondance, mais en gardant à l'esprit qu'elles doivent respecter un certain nombre de propriétés mathématiques. Dans ce cadre, nous proposons de nouvelles solutions qui permettent de mieux répondre aux caractéristiques particulières des graphes issus de primitives et descripteurs visuels. Nous présentons aussi les algorithmes qui permettent d'évaluer rapidement ces fonctions. Enfin, nous présentons des expériences qui mettent en lumière ces différentes caractéristiques, ainsi que des expériences qui montrent les avantages qu'offrent nos modèles vis à vis de la littérature.
%X This thesis proposes a generic framework for query optimization in heterogeneous and distributed environments. We propose a generic source description model (GSD), which allows describing any type of information related to query processing and optimization. With GSD, we can use cost information to calculate the costs of execution plans. Our generic framework for query optimization provides a set of unitary functions used to perform optimization by applying different search strategies. Our experimental results show the accuracy of cost calculus when using GSD, and the flexibility of our generic framework when changing search strategies. Our proposed approach has been implemented and integrated in a data integration product (DVS) licensed by Xcalia - Progress Software Corporation. For queries with many inter-site joins accessing large size data sources, the time used for finding the optimal plan is in the order of 2 seconds, and the execution time of the optimized plan is reduced by 28 times, as compared with the execution time of the non optimized original plan.
%X Dans cette thèse, nous proposons un cadre générique d'optimisation de requêtes dans les environnements hétérogènes répartis. Nous proposons un modèle générique de description de sources (GSD), qui permet de décrire tous les types d'informations liées au traitement et à l'optimisation de requêtes. Avec ce modèle, nous pouvons en particulier obtenir les informations de coût afin de calculer le coût des différents plans d'exécution. Notre cadre générique d'optimisation fournit les fonctions unitaires permettant de mettre en œuvre les procédures d'optimisation en appliquant différentes stratégies de recherche. Nos résultats expérimentaux mettent en évidence la précision du calcul de coût avec le modèle GSD et la flexibilité de notre cadre générique d'optimisation lors du changement de stratégie de recherche. Notre cadre générique d'optimisation a été mis en œuvre et intégré dans un produit d'intégration de données (DVS) commercialisé par l'entreprise Xcalia - Progress Software Corporation. Pour des requêtes contenant beaucoup de jointures inter-site et interrogeant des sources de grand volume, le temps de calcul du plan optimal est de l'ordre de 2 secondes et le temps d'exécution du plan optimal est réduit de 28 fois par rapport au plan initial non optimisé.
%X This thesis takes interest in the mechanisms facilitating the autonomous acquisition of be- haviors in animals and proposes to use these mechanisms in the frame of robotic tasks. Artificial neural networks are used to model cerebral structures, both to understand how these structures work and to design robust and adaptive algorithms for robot control. The work presented here is based on a model of the hippocampus capable of learning the temporal relationship between perceptive events. The neurons performing this learning, called transition cells, can predict which future events the robot could encounter. These transitions support the building of a cognitive map, located in the prefrontal and/or parietal cortex. The map can be learned by a mobile robot exploring an unknown environment and then be used to plan paths in order to reach one or several goals. Apart from their use in building a cognitive map, transition cells are also the basis for the design of a model of reinforcement learning. A biologically plausible neural implementation of the Q-learning algorithm, using transitions, is made by taking inspiration from the basal ganglia. This architecture provides an alternative strategy to the cognitive map planning strategy. The reinforcement learning strategy requires a longer learning period but corresponds more to an au- tomatic low-level behavior. Experiments are carried out with both strategies used in cooperation and lesions of the prefrontal cortex and basal ganglia allow to reproduce experimental results obtained with rats. Transition cells can learn temporally precise relations predicting the exact timing when an event should be perceived. In a model of interactions between the hippocampus and prefrontal cortex, we show how these predictions can explain in-vivo recordings in these cerebral struc- tures, in particular when rat is carrying out a task during which it must remain stationary for 2 seconds on a goal location to obtain a reward. The learning of temporal information about the environment and the behavior of the robot allows the system to detect regularity. On the contrary, the absence of a predicted event can signal a failure in the behavior of the robot, which can be detected and acted upon in order to modulate the failing behavior. Consequently, a fail- ure detection system is developed, taking advantage of the temporal predictions provided by the hippocampus and the interaction between behavior modulation functions in the prefrontal cortex and reinforcement learning in the basal ganglia. Several robotic experiments are conducted, in which the failure signal is used to modulate, immediately at first, the behavior of the robot in order to stop selecting actions which lead to failures and explore other strategies. The signal is then used in a more lasting way by modulating the learning of the associations leading to the selection of an action so that the repeted failures of an action in a particular context lead to the suppression of this association. Finally, after having used the model in the frame of navigation, we demonstrate its general- ization capabilities by using it to control a robotic arm in a trajectory planning task. This work constitutes an important step towards obtaining a generic and unified model allowing the control of various robotic setups and the learning of tasks of different natures.
%X Cette thèse s'intéresse aux mécanismes permettant de faciliter l'acquisition autonome de comportements chez les êtres vivants et propose d'utiliser ces mécanismes dans le cadre de tâches robotiques. Des réseaux de neurones artificiels sont utilisés pour modéliser certaines structures cérébrales, à la fois afin de mieux comprendre le fonctionnement de ces structures dans le cerveau des mammifères et pour obtenir des algorithmes robustes et adaptatifs de contrôle en robotique. Les travaux présentés se basent sur un modèle de l'hippocampe permettant d'apprendre des relations temporelles entre des événements perceptifs. Les neurones qui forment le substrat de cet apprentissage, appelés cellules de transition, permettent de faire des prédictions sur les événements futurs que le robot pourrait rencontrer. Ces transitions servent de support à la con- struction d'une carte cognitive, située dans le cortex préfrontal et/ou pariétal. Cette carte peut être apprise lors de l'exploration d'un environnement inconnu par un robot mobile et ensuite utilisée pour planifier des chemins lui permettant de rejoindre un ou plusieurs buts. Outre leur utilisation pour la construction d'une carte cognitive, les cellules de transition servent de base à la conception d'un modèle d'apprentissage par renforcement. Une implémen- tation neuronale de l'algorithme de Q-learning, utilisant les transitions, est réalisée de manière biologiquement plausible en s'inspirant des ganglions de la base. Cette architecture fournit une stratégie de navigation alternative à la planification par carte cognitive, avec un apprentissage plus lent, et correspondant à une stratégie automatique de bas-niveau. Des expériences où les deux stratégies sont utilisées en coopération sont réalisées et des lésions du cortex préfrontal et des ganglions de la base permettent de reproduire des résultats expérimentaux obtenus chez les rats. Les cellules de transition peuvent apprendre des relations temporelles précises permettant de prédire l'instant où devrait survenir un événement. Dans un modèle des interactions entre l'hippocampe et le cortex préfrontal, nous montrons comment ces prédictions peuvent expliquer certains enregistrements in-vivo dans ces structures cérébrales, notamment lorsqu'un rat réalise une tâche durant laquelle il doit rester immobile pendant 2 secondes sur un lieu but pour obtenir une récompense. L'apprentissage des informations temporelles provenant de l'environnement et du comportement permet de détecter des régularités. A l'opposé, l'absence d'un événe- ment prédit peut signifier un échec du comportement du robot, qui peut être détecté et utilisé pour adapter son comportement en conséquence. Un système de détection de l'échec est alors développé, tirant parti des prédictions temporelles fournies par l'hippocampe et des interactions entre les aspects de modulation comportementale du cortex préfrontal et d'apprentissage par renforcement dans les ganglions de la base. Plusieurs expériences robotiques sont conduites dans lesquelles ce signal est utilisé pour moduler le comportement d'un robot, dans un premier temps de manière immédiate, afin de mettre fin aux actions du robot qui le mènent à un échec et envisager d'autres stratégies. Ce signal est ensuite utilisé de manière plus permanente pour moduler l'apprentissage des associations menant à la sélection d'une action, afin que les échecs répétés d'une action dans un contexte particulier fassent oublier cette association. Finalement, après avoir utilisé le modèle dans le cadre de la navigation, nous montrons ses capacités de généralisation en l'utilisant pour le contrôle d'un bras robotique. Ces travaux constituent une étape importante pour l'obtention d'un modèle unifié et générique permettant le contrôle de plates-formes robotiques variés et pouvant apprendre à résoudre des tâches de natures différentes.
%X The main goal of this thesis is to develop innovative and practicaltools for the reconstruction of buildings from images. The typical input to our workis a set of facade images, building footprints, and coarse 3d models reconstructedfrom aerial images. The main steps include the calibration of the photographs,the registration with the coarse 3d model, the recovery of depth and sematicinformation, and the refinement of the coarse 3d model.To achieve this goal, we use computer vision, pattern recognition and computergraphics techniques. Contributions in this approach are presented on two parts.In the first part, we focused on multiple view reconstruction techniques withthe aim to automatically recover the depth information of facades from a setof uncalibrated photographs. First, we use structure from motion techniques toautomatically calibrate the set of photographs. Then, we propose techniques for theregistration of the sparse reconstruction to a coarse 3d model. Finally, we proposean accelerated multi-view stereo and voxel coloring framework using graphicshardware to produce a textured 3d mesh of a scene from a set of calibrated images.The second part is dedicated to single view reconstruction and its aim is to recoverthe semantic structure of a facade from an ortho-rectified image. The novelty ofthis approach is the use of a stochastic grammar describing an architectural style asa model for facade reconstruction. we combine bottom-up detection with top-downproposals to optimize the facade structure using the Metropolis-Hastings algorithm.
%X L'objectif principal de cette thèse est de développer des outils pour la reconstruction de l'environnement urbain à partir d'images. Les entrées typiques de notre travail est un ensemble d'images de façades, des empreintes au sol de bâtiments, et des modèles 3D reconstruits à partir d'images aériennes. Les principales étapes comprennent le calibrage des images,le recalage avec le modèle 3D, la récupération des informations de profondeur ainsi que la sémantique des façades.Pour atteindre cet objectif, nous utilisons des techniques du domaine de vision par ordinateur, reconnaissance de formes et de l'informatique graphique. Les contributions de notre approche sont présentés en deux parties.Dans la première partie, nous nous sommes concentrés sur des techniques de reconstruction multi-vues dans le but de récupérer automatiquement les informations de profondeur de façades à partir un ensemble des photographies non calibrées. Tout d'abord, nous utilisons la technique structure et mouvement pour calibrer automatiquement l'ensemble des photographies. Ensuite, nous proposons des techniques pour le recalage de la reconstruction avec un modèle 3D. Enfin, nous proposons des techniques de reconstruction 3d dense (stéréo multi-vues et voxel coloring) pour produire un maillage 3D texturé d'une scène d'un ensemble d'images calibrées.La deuxième partie est consacrée à la reconstruction à partir d'une seule vue et son objectif est de récupérer la structure sémantique d'une façade d'une image ortho-rectifiée. La nouveauté de cette approche est l'utilisation d'une grammaire stochastique décrivant un style architectural comme modèle pour la reconstruction de façades. nous combinons un ensemble de détecteurs image avec une méthode d'optimisation globale stochastique en utilisant l'algorithme Metropolis-Hastings.
%X Les travaux présentés dans cette HDR visent à mieux comprendre le fonctionnement des mécanismes d'apprentissage qui sont liés au contrôle moteur bas niveau chez l'humain, pour les modéliser et les intégrer dans les contrôleurs des robots humanoïdes. L'objectif est de rendre ces derniers plus robustes face aux perturbations externes dues à l'environnement, ou à leur interaction avec l'humain (forces externes, glissement, pentes ou irrégularités du sol), ou aux dommages internes, soudains ou progressifs, qu'ils peuvent subir et qui mettent en péril leur mission (usures articulaires, amputation de membres moteurs, pertes sensorielles...). Pour ce faire, les capacités de généralisation offertes par les algorithmes d'apprentissage des réseaux de neurones et les synergies de leurs mécanismes adaptatifs (homéostasie, plasticité neuronale et synaptique) ont été étudiées. Les solutions ont été évaluées en soumettant les robots à des perturbations ou à des dysfonctionnements externes ou internes, lents ou brusques. Des réponses aux questions suivantes ont été apportées: comment la marche adaptative des robots peut elle être produite? Comment peut-elle être contrôlée? Quels types d'architecture permettent à la fois la production de rythmes locomoteurs et le contrôle de la posture? Quels sont les mécanismes adaptatifs sensori-moteurs qui régissent ces architectures pour les marches normales et déficientes des robots? Quelles sont les méthodologies possibles pour modéliser et reproduire ces architectures? Quelles sont les limites de ces approches ?.
%X The problem of learning behaviors on an autonomous robot raises many issues related to motor control, behavior encoding, behavioral strategies and action selection. Using a developmental approach is of particular interest in the context of autonomous robotics. The behavior of the robot is based on low level mechanisms that together can make more complex behaviors emerge. Moreover, the robot has no a priori information about its own physical characteristics or on its environment, it must learn its own sensori-motor dynamic. For instance, I started my thesis by studying a model of low level imitation. From a developmental point of view, imitation is present from birth and accompanies the development of young children under multiple forms. It has a learning function and shows up as an asset in term of performance in time of behaviors acquisition, as well as a communication function playing a role in the bootstrap and the maintenance of nonverbal and natural interactions. Moreover, even if there is not a real intention to imitate, the observation of another agent allows to extract enough information to be able to reproduce the task. Initially, my work consisted in applying and testing a developmental model allowing emergence of low level imitation behaviors on an autonomous robot. This model is built like a homeostatic system which tends to balance its rough perceptive information (movement detection, color detection, angular information from motors of a robotic arm) by its action. Thus, when a human moves his hand in the robot visual field, the perception ambiguity of the robot makes it consider the human hand as its own arm extremity. From the resulting error a immediate imitation behavior emerges. Of course, such a model implies that the robot is initially able to associate the visual positions of its effector with the proprioceptive informations of its motors. Thanks to imitation behavior, the robot makes movements from which it can learn to build more complex behaviors. Then, how to go from a simple movement to a more complex gesture which can imply an object or a place ? I then proposed an architecture allowing a robot to learn a behavior as a complex temporal sequences (with repetition of elements) of movements. Two models allowing to learn sequences have been developed and tested. The first, based on a model of the hippocampus, learns on-line the timing of simple temporal sequences. The second, based on the properties of a dynamic reservoir, learns on-line complex temporal sequences. Based on these works, an architecture learning the timing of a complex temporal sequence has been proposed. The tests in simulation and on actual robot have shown the necessity to add a resynchronization mechanism that allows to find the correct hidden states for starting a complex sequence by an intermediate state. In a third time, my work consisted in studying how two sensori-motor strategies can cohabit in the context of navigation task. The first strategy codes the behavior from spatial informations, then the second uses temporal informations. Both architectures have been independently tested on the same task. Then, both strategies were merged and executed in parallel. Responses of both strategies were merged with the use of dynamical neural filed. A mechanism of "chunking" which represents the instantaneous state of the robot (current place with current action) allows to resynchronize the temporal sequences dynamics. In parallel, a number of programming and design problems about neural networks have appeared. In fact, our networks can be made of many hundreds of thousands of neurons. It becomes hard to execute them on one computational unit. How to design neural architectures with parallel computation, network communication and real time constraints ? Another part of my work consisted in providing tools allowing the design, communication and real time execution of distributed architectures. Finally, in the context of the Feelix Growing European project, I contribute to integrate my work with those of the LASA laboratory of EPFL for the learning of complex behaviors mixing navigation, gesture and object. To conclude, this thesis allowed me to develop new models for learning behaviors - in time and in space, new tools to handle very large neural networks, and to discuss, beyond limitations of the current system, the important elements for an action selection system.
%X La problématique de l'apprentissage de comportements sur un robot autonome soulève de nombreuses questions liées au contrôle moteur, à l'encodage du comportement, aux stratégies comportementales et à la sélection de l'action. Utiliser une approche développementale présente un intérêt tout particulier dans le cadre de la robotique autonome. Le comportement du robot repose sur des mécanismes de bas niveau dont les interactions permettent de faire émerger des comportements plus complexes. Le robot ne possède pas d'informations a priori sur ses caractéristiques physiques ou sur l'environnement, il doit apprendre sa propre dynamique sensori-motrice. J'ai débuté ma thèse par l'étude d'un modèle d'imitation bas niveau. Du point de vue du développement, l'imitation est présente dès la naissance et accompagne, sous de multiples formes, le développement du jeune enfant. Elle présente une fonction d'apprentissage et se révèle alors être un atout en terme de temps d'acquisition de comportements, ainsi qu'une fonction de communication participant à l'amorce et au maintien d'interactions non verbales et naturelles. De plus, même s'il n'y a pas de réelle intention d'imiter, l'observation d'un autre agent permet d'extraire suffisamment d'informations pour être capable de reproduire la tâche. Mon travail a donc dans un premier temps consisté à appliquer et tester un modèle développemental qui permet l'émergence de comportements d'imitation de bas niveau sur un robot autonome. Ce modèle est construit comme un homéostat qui tend à équilibrer par l'action ses informations perceptives frustres (détection du mouvement, détection de couleur, informations sur les angles des articulations d'un bras de robot). Ainsi, lorsqu'un humain bouge sa main dans le champ visuel du robot, l'ambigüité de la perception de ce dernier lui fait confondre la main de l'humain avec l'extrémité de son bras. De l'erreur qui en résulte émerge un comportement d'imitation immédiate des gestes de l'humain par action de l'homéostat. Bien sûr, un tel modèle implique que le robot soit capable d'associer au préalable les positions visuelles de son effecteur avec les informations proprioceptives de ses moteurs. Grace au comportement d'imitation, le robot réalise des mouvements qu'il peut ensuite apprendre pour construire des comportements plus complexes. Comment alors passer d'un simple mouvement à un geste plus complexe pouvant impliquer un objet ou un lieu ? Je propose une architecture qui permet à un robot d'apprendre un comportement sous forme de séquences temporelles complexes (avec répétition d'éléments) de mouvements. Deux modèles différents permettant l'apprentissage de séquences ont été développés et testés. Le premier apprend en ligne le timing de séquences temporelles simples. Ce modèle ne permettant pas d'apprendre des séquences complexes, le second modèle testé repose sur les propriétés d'un réservoir de dynamiques, il apprend en ligne des séquences complexes. A l'issue de ces travaux, une architecture apprenant le timing d'une séquence complexe a été proposée. Les tests en simulation et sur robot ont montré la nécessité d'ajouter un mécanisme de resynchronisation permettant de retrouver les bons états cachés pour permettre d'amorcer une séquence complexe par un état intermédiaire. Dans un troisième temps, mes travaux ont consisté à étudier comment deux stratégies sensorimotrices peuvent cohabiter dans le cadre d'une tâche de navigation. La première stratégie encode le comportement à partir d'informations spatiales alors que la seconde utilise des informations temporelles. Les deux architectures ont été testées indépendamment sur une même tâche. Ces deux stratégies ont ensuite été fusionnées et exécutées en parallèle. La fusion des réponses délivrées par les deux stratégies a été réalisée avec l'utilisation de champs de neurones dynamiques. Un mécanisme de "chunking" représentant l'état instantané du robot (le lieu courant avec l'action courante) permet de resynchroniser les dynamiques des séquences temporelles. En parallèle, un certain nombre de problème de programmation et de conception des réseaux de neurones sont apparus. En effet, nos réseaux peuvent compter plusieurs centaines de milliers de neurones. Il devient alors difficile de les exécuter sur une seule unité de calcul. Comment concevoir des architectures neuronales avec des contraintes de répartition de calcul, de communications réseau et de temps réel ? Une autre partie de mon travail a consisté à apporter des outils permettant la modélisation, la communication et l'exécution en temps réel d'architecture distribuées. Pour finir, dans le cadre du projet européen Feelix Growing, j'ai également participé à l'intégration de mes travaux avec ceux du laboratoire LASA de l'EPFL pour l'apprentissage de comportements complexes mêlant la navigation, le geste et l'objet. En conclusion, cette thèse m'a permis de développer à la fois de nouveaux modèles pour l'apprentissage de comportements - dans le temps et dans l'espace, de nouveaux outils pour maîtriser des réseaux de neurones de très grande taille et de discuter à travers les limitations du système actuel, les éléments importants pour un système de sélection de l'action.
%X In this thesis, we investigate the optimization of Raptor codes for various channels of interest in practical wireless systems. First, we present an analytical asymptotic analy- sis of jointly decoded Raptor codes over a BIAWGN channel. Based on the analysis, we derive an optimization method for the design of eﬃcient output degree distributions. We show that even though Raptor codes are not universal on other channels than the BEC, Raptor code optimized for a given channel capacity also perform well on a wide range of channel capacities when joint decoding is considered. Then, we propose a rate splitting strategy that is eﬃcient for the design of ﬁnite length Raptor codes. We next investigate the extension of the analysis to the uncorrelated Rayleigh-fading chan- nel with perfect channel state information (CSI) at the receiver, and optimize Raptor codes for quasi-static fading channels when CSI is available at the receiver but not at the transmitter. Finally, we show that in presence of imperfect CSI at the receiver, it is possible to improve the performance with no additional complexity, by using an appropriate metric for the computation of the LLR at the output of the channel. In the second part of this thesis, we investigate the construction of eﬃcient ﬁnite length LDPC codes. In particular, we present some improvements for the Progressive Edge- Growth algorithm that allow to construct minimal graphs. The proposed algorithm is used to construct protographs with large girth that perform well under iterative decoding. Moreover, we propose an eﬃcient structured search procedure for the design of quasi-cyclic LDPC codes.
%X In this thesis we present our work in the domain of non-binary decoding algorithm for general classes of non-binary LDPC codes. Binary Low-Density Parity-Check (LDPC) codes were originally presented by Gallager in 1963, and after some fundamental theo- retical advancement, they were considered in standards like DVB-S2, WI-MAX, DSL, W-LAN etc. Later on, Non-Binary LDPC (NB-LDPC) codes were proposed in liter- ature, and showed better performance for small lengths or when used on non-binary channels. However, the advantages of using NB-LDPC codes come with the conse- quence of a heavily increased decoding complexity. For a code defined in GF (q), the complexity is of the order O (q2). Similarly, the memory required for storing messages is of order O (q). Consequently, the implementation of an LDPC-decoder defined over a field order q > 64 becomes practically impossible. The main objective of the thesis is to develop reduced complexity algorithms for non-binary LDPC codes that exhibit excellent performance and are practically implementable. For better decoding per- formance, not only the decoding algorithm is important, but also the structure of the code plays an important role. With this goal in mind, a new family of codes called cluster-NB-LDPC codes was developed and specific improvements of the NB decoder for cluster-NB-LDPC codes were proposed. Our principal result is that we were able to propose decoders for cluster-NB-LDPC codes with reduced complexity compared to usual decoders for NB-LDPC codes on fields, without any performance loss in error correction capability. iv Acknowledgements In the first part of the thesis, we modify the EMS algorithm for cluster codes. We see that the direct implementation of the EMS algorithm to NB cluster-LDPC codes is not a feasible option. There is a loss in performance and an increase in decoding complexity. Therefore, we propose some modification in the procedure, which not only significantly improves the decoding performance but also decreases the decoding complexity. It places the same limits on the number of operations at the check nodes as the EMS algorithm for GF (q)-codes i.e. O (nmlognm), with nm << q. We then propose another method, based on the diversity of cluster codes, to improve the per- formance of the EMS algorithm for cluster codes. It also helps in reducing the overall complexity of the decoder. In the end we compare the decoding performance using this method and analyze the effect on the decoding complexity. In the last part of the chapter, we propose a new direction for the decoding of LDPC codes. It is based on the creation of lists of codewords that are local to the parity check nodes. The list is constructed recursively in a tree structure, which makes it a good candidate for hardware implementation. It is a new method and requires further im- provement. As an initial report, we have obtained good results with less number of computations.
%X Dans cette thèse, nous présentons nos travaux dans le domaine des algorithmes de décodage des codes LDPC non-binaires généralisés. Les codes LDPC binaires ont été initialement proposés par Gallager en 1963, et après quelques avancées théoriques fondamentales, ils ont été proposés dans des standards tels que DVB-S2, WI-MAX, DSL, W-LAN etc. Plus tard, les codes LDPC non-binaires (NB-LDPC) ont été pro- posés dans la littérature, et ont montré une meilleure performance pour de petites tailles de code ou lorsqu'ils sont utilisés sur des canaux non-binaires. Cependant, les avan- tages de l'utilisation de codes NB-LDPC impliquent une augmentation importante de la complexité de décodage. Pour un code défini dans un corps de Galois GF (q), la complexité est d'ordre O (q2). De même, la mémoire requise pour le stockage des messages est d'ordre O (q). Ainsi, l'implémentation d'un décodeur LDPC défini sur un corps de Galois pour q > 64 devient impossible dans la pratique. L'objectif prin- cipal de cette thèse est de développer des algorithmes avec une bonne performance et complexité réduite de sorte qu'ils deviennent implémentables. Pour une performance de décodage optimisée, non seulement l'algorithme est important, mais également la structure du code joue un rôle clé. Avec cet objectif à l'esprit, une nouvelle famille de codes appelés " cluster-NB-LDPC codes " a été élaborée ainsi que des améliorations spécifiques du décodeur non-binaire pour ces codes. Le résultat principal est que nous avons pu proposer des décodeurs pour les codes cluster-NB-LDPC avec une complex- ité réduite par rapport aux décodeurs classiques pour les codes NB-LDPC définis sur les corps de Galois, sans aucune perte de performance dans la capacité de correction vi Résumé d'erreur. Dans la première partie de la thèse, nous avons modifié l'algorithme EMS pour les cluster-codes. La généralisation directe de l'algorithme EMS aux codes cluster-NB- LDPC n'est pas réaliste . Il y a une perte de performance et une augmentation de la complexité. Par conséquent, nous proposons quelques modifications dans la procé- dure, qui non seulement améliore considérablement les performances de décodage, mais diminue également la complexité. Au niveau des noeuds de parité, cet algo- rithme conserve les mêmes limites sur le nombre d'opérations que l'algorithme EMS pour GF (q)-codes, O (nmlognm) avec nm << q. Nous proposons ensuite une autre méthode, basée sur la diversité des codes cluster, afin d'améliorer les performances de l'algorithme EMS pour les codes cluster-LDPC. Il contribue également à réduire la complexité globale du décodeur. Finalement, nous comparons les performances de décodage en utilisant cette méthode et analysons l'effet sur la complexité de décodage. Dans la dernière partie du chapitre, nous proposons une nouvelle direction pour le décodage des codes LDPC. Elle est basée sur la création des listes des mots de code qui correspondent à des noeuds de parité. Les listes sont construite de manière récur- sive dans une structure en arbre, ce qui en fait un bon candidat pour l'implémentation matérielle. Il s'agit d'une méthode nouvelle et doit encore être améliorée mais à pre- miére vue nous avons obtenu de bons résultats avec un nombre réduit d'operations.
%X In this work, we consider the problem of optimization of the training sequence length when a Maximum a posteriori (MAP) detector is used. We consider a transmission over a Single Input Single Output (SISO) frequency selective channel and a transmission over a Multiple-Input Multiple-Output (MIMO) flat fading channel. We study the case where a non-iterative receiver is used and the case where a turbo-detector composed of a MAP detector a MAP decoder is used. The optimal length of the training sequence is found by maximizing an effective Signal-to-Noise Ratio (SNR) taking into account the data throughput loss due to the use of pilot symbols. We then consider an Orthogonal Frequency Division Multiplexing (OFDM). We consider the case where the Frequency Division Duplexing (FDD) mode is used. We jointly optimize the power allocation and the feedback. We also propose low complexity bit loading algorithms with limited feedback for an OFDM system. We emphasize that there are two methods to return the bit allocation from the receiver to the transmitter and precise which method minimizes the number of feedback bits.
%X Dans cette thèse, nous proposons d'optimiser la longueur de la séquence d'apprentissage quand un détecteur basé sur le critère Maximum a posteriori (MAP) est utilisé. Nous considérons une transmission mono-antenne sur un canal sélectif en fréquence et une transmission multi-antennes (MIMO: Multiple-Input Multiple-Output) sur un canal non sélectif en fréquence. Nous étudions le cas d'un récepteur non itératif et le cas d'un turbo-détecteur composé d'un détecteur MAP d'un décodeur MAP. La longueur optimale de la séquence d'apprentissage est trouvée en maximisant un Rapport Signal à Bruit (RSB) utile qui tient compte de la perte en termes de débit due à l'utilisation des symboles pilotes. Nous considérons ensuite un système OFDM (Orthogonal Frequency Division Multiplexing) mono-antenne. Nous considérons le cas où le mode FDD (Frequency Division Duplexing) est utilisé. Nous étudions le problème de l'optimisation conjointe de la voie de retour et de l'allocation des puissances. Nous proposons aussi des algorithmes d'allocation de la modulation à faible complexité utilisant une quantité limitée de ressources sur la voie de retour pour un système OFDM. Nous mettons en évidence deux méthodes pour utiliser la voie de retour et donnons une condition analytique permettant de choisir entre ces deux méthodes.
%X This thesis is dedicated to the resources allocation for the transmission of scalable multimedia data under Quality-of-Service (QoS) constraints on heterogeneous networks. We focus on wire and wireless links (DS-CDMA, OFDMA) with the transmission of images, speech over frequency and non frequency selective channels. Resources from the physical layer are addressed : channel code rates (to protect the data against the degradation of the signal-to-noise ratio SNR), modulation orders, carriers ordering (to convey the layers) and the allocated power. The aim of this report is to allocate these parameters in order to maximize the source rate of the multimedia data under targeted QoS and system payload with a perfect or partial channel knowledge. The QoS is expressed in term of perceived quality from the End To end User and in term of Bit Error Rate per Class from the scalable source encoder. In a such context, we propose some link adaptation schemes whose novelty is to enable the truncation of the data layers. Moreover, these strategies make use of the sensivity to transmission errors and the channel state information to dynamically adapt the protection of the layers (Unequal Error Protection UEP) in accordance with the QoS requirements. These procedures explore multiple resources optimization criteria: the minimization of the system payload and the maximization of the robustness to the channel estimation error. For each one, we perform the optimal allocation (bit loading) of the previous parameters that maximize the source rate while ensuring the constraints of the receiver. We show that these schemes fi t to any communication system and we present the performances and compare them to the State Of The Art procedures.
%X Cette thèse s'intéresse aux problèmes d'allocation de ressources pour la transmission de données multimédia scalables sous contraintes de qualité de service (QoS) sur les réseaux hétérogènes. Les liaisons filaires et sans fil considérées (DS-CDMA, OFDMA) sont appliquées à des services de transmission d'images et de parole sur des canaux à évanouissements lents ou rapides, avec ou sans multitrajets. La QoS de ces réseaux est exprimée en terme de qualité perçue du point de vue de l'utilisateur (couche Application) et en terme de taux d'erreurs binaires (TEB) par classe du point de vue de la transmission (couche Physique). Les ressources étudiées sont : l'allocation des puissances, des ordres de modulation et des porteuses ainsi que les propriétés de protection inégale contre les erreurs (UEP). L'objectif de ce document est d'allouer ces ressources de façon à maximiser le débit source des données multimédia hiérarchisées (sous forme de classes d'importance) en s'appuyant sur une connaissance parfaite ou partielle des canaux de propagation, sous contrainte de performances cibles en réception. Les stratégies d'adaptation de lien que nous présentons se basent sur la possible troncature d'une partie de ces données à transmettre. Elles se fondent également sur le degré de sensibilité et la protection adéquate de chacune de ces classes contre les erreurs liées à la transmission sur le canal, conformément aux exigences de QoS exprimées sur ces dernières. Les schémas de transmission explorent plusieurs critères d'optimisation des ressources : la minimisation de la charge utile du système ainsi que l'optimisation de la robustesse de la transmission aux erreurs d'estimation du canal. Dans ces contextes, nous décrivons l'allocation optimale de sous-porteuses, de modulations, de rendements de code et d'énergie maximisant le débit source de l'utilisateur tout en véri ant les contraintes sur la charge du système et la QoS. Nous montrons que ces schémas d'allocation sont adaptables à de nombreux systèmes de communication et présentent des performances supérieures aux stratégies de l'état de l'art.
%X One of the most prominent issues in the design and implementation of OFDMA based systems is the need for a very fine frequency synchronization due to the fact that OFDMA, like OFDM, is extremely sensitive to carrier frequency offsets (CFO). The task of frequency synchronization becomes more challenging in the uplink OFDMA-based systems where one OFDMA symbol is generated by the contribution of many different users. Our goals include the study and analysis of problems resulting from frequency mismatches provide solution to combat these problems. We first look at the interference resulting from CFOs, resulting from user terminal oscillator mismatch. We demonstrate that one must take into account the cyclic prefix while analyzing interference resulting from CFO. A new analytical expression of the ICI that takes into account the effect of CFO on the cyclic prefix is proposed. Then we focus our attention on analysis of the trade-off between channel frequency diversity and robustness against CFO and show that there exists a contradiction between the two. We propose a trade-off in the form of a Threshold blocksize, to allow a good compromise between the channel diversity and robustness for CFO for the case when no CSI is available. For system where CSI is available, we propose an optimal block carrier allocation scheme through which both robustness to CFO and channel frequency diversity can be achieved with small blocksize for small CFO. We also propose a Critical CFO value, above which the performance of the optimal block carrier allocation loses interest. Next we propose solutions for two important issues encountered in an uplink OFDMA system. First, we propose an efficient method for joint estimation of channel impulse responses and carrier frequency at the receiver based on polynomial approximation. Our proposed joint estimation method is simpler than the existing methods without any performance degradation. Next we propose a CFO compensation method based on successive interference cancellation. The proposed cancellation method reduces the implementation complexity faced in case of large DFT matrices.
%X La mise en oeuvre de systèmes basés OFDMA nécessite une synchronisation de la fréquence très fine en raison de l'extrème sensibilité de l'OFDMA aux décalages en fréquence porteuse (CFO). La synchronisation en fréquence devient plus difficile dans les systèmes OFDMA en liaison montante. Nos objectifs comprennent l'étude et l'analyse des problèmes résultant de décalages de fréquence et la proposition de solutions pour lutter contre ces problèmes. Nous examinons d'abord les interférences résultant de décalages de fréquence porteuse présents dans l'oscillateur du terminal utilisateur. Nous démontrons que l'on doit prendre en compte le préfixe cyclique tout en analysant les interférences résultant du CFO. Ensuite, nous montrons qu'il existe une contradiction entre la diversité de fréquence de canal et la robustesse contre le CFO. Nous proposons un compromis sous la forme d'une taille de bloc de seuil, afin de permettre un bon compromis entre la diversité des canaux et de robustesse pour les CFO pour le cas où aucune connaissances du canal n'est disponible. Quand le canal est connu, nous proposons une allocation optimale par bloc grâce à laquelle la robustesse aux CFO et à la diversité de fréquence de canal peut être réalisée en utilisant une petite taille de bloc petit pour des CFO de petite valeur. Nous proposons également une valeur CFO critique, en dessous de laquelle l'allocation optimal par bloc est très performant. Ensuite, nous proposons des solutions pour deux problèmes importants rencontrés dans un système OFDMA en liaison montante. Premièrement, nous proposons une méthode efficace pour l'estimation conjointe des réponses impulsionnelles des canaux et fréquences porteuses basée sur l'approximation polynomiale. Notre méthode d'estimation conjointe est plus simple que les méthodes existantes, sans aucune dégradation de performance. Ensuite, nous proposons une méthode de compensation de CFO basée sur l'annulation des interférences successives (SIC). La méthode d'annulation proposée réduit la complexité de mise en oeuvre quand le nombre de porteuses est important.
%X We consider in this thesis the resource allocation problems in multi-user OFDM downlink systems under various Quality of service (QoS) constraints. Our goal is to propose practical solutions given partial channel-state information (CSI). We first look for the optimal subcarrier sharing in a single cell subject to differentiated QoS constraints among users. Although users are allowed to share the same subcarrier simultaneously, we prove that an orthogonal sharing (an OFDMA scheme), along with an appropriate power allocation, maximizes the weighted sum-rate. Then, we generalize OFDMA optimality to other performance criteria such as the common rate, the proportional rates, etc. Assuming an OFDMA downlink, we focus on QoS fairness by first considering the single cell case. We assume also a partial CSI consisting in the average channel gain for each user. Thus, we describe the optimal subcarrier and rate allocation that maximizes a common user rate under bounded outage probabilities. Finally, we consider the multi-cell case and we propose to use the frequency reuse partitioning as an inter-cell interference mitigation technique. Under realistic QoS fairness constraints and partial CSI, we provide a practical solution for subcarrier, base station and rate allocation that allows us to maximize the user data rate.
%X Cette thèse considère les problèmes d'allocation de ressources en OFDM multi-utilisateurs avec diverses types de contraintes de qualité de service (QoS) adaptées à plusieurs contextes applicatifs. L'objectif est de proposer des algorithmes pratiques pour la liaison descendante basés sur une connaissance partielle des canaux. Nous considérons pour commencer la question du partage optimal de sous-porteuses entre les utilisateurs d'une cellule sous contraintes de QoS hétérogènes. Nous permettons à plusieurs utilisateurs de partager la même sous-porteuse créant ainsi de l'interférence multi-utilisateurs sur chaque sous-porteuse. Nous montrons l'optimalité, vis-à-vis de la somme pondérée de débits, d'un partage orthogonal du type OFDMA assorti d'une allocation appropriée de puissance. Nous généralisons aussi l'optimalité de l'OFDMA à d'autres mesures de performance telles que le débit commun, les débits proportionnels, etc. Nous considérons en suite une liaison OFDMA descendante d'une cellule et nous mettons l'accent sur l'équité en QoS. Notre approche exploite la connaissance du canal moyen de chaque utilisateur et de la charge du système. Nous décrivons l'allocation optimale de sous-porteuses et de modulations maximisant le débit utilisateur tout en caractérisant les probabilités de coupure soumises à des valeurs maximales cibles. Enfin, nous considérons le cas multi-cellules et nous utilisons la technique de ré-utilisation fractionnelle de sous-porteuses afin de gérer les interférences entre-cellules et d'améliorer l'efficacité spectrale. Dans ce contexte, nous proposons une solution d'allocation de bande et de station de base garantissant la meilleure QoS équitable étant donnée la connaissance des canaux moyens.
%X In this thesis, we address the problem of spectrum-sharing for wireless communication where multiple users attempt to access a common spectrum resource under mutual interference constraints. Our objective is to evaluate the gains of sharing by investigating different scenarios of spectrum access. Studying the Gaussian Interference Channel with interferences considered as noise, we found a geometrical description and several characteristics of the achievable rate region. Considering a more realistic scenario, with each user having a certain QoS, we found necessary and sufficient condition to be fulfilled for simultaneous communications over the two-user Gaussian Interference Channel. Furthermore, we proposed two lower bounds for a single-primary-user mean rate, depending on the secondary user power control scheme. Specially, we investigated an original power control policy, for a secondary user, under outage performance requirement for both users and partial knowledge of the channel state information. Finally, considering a spectrumsharing with a licensee or primary user and several secondary or cognitive users, we showed the existence of an exclusive region around the primary receiver and we characterized the effects of shadowing and path-loss on this exclusive region (or no-talk zone).
%X Le spectre électromagnétique est une ressource naturelle dont l'usage doit être optimisé. Un grand nombre de travaux actuels visent à améliorer l'utilisation des fréquences radio en y introduisant un degré de flexibilité rendu possible par l'agilité en forme d'onde et en fréquence permise par la radio logicielle (SDR), ainsi que par les méthodes de traitement intelligent du signal (radio cognitive). Cette thèse se place dans ce contexte. Concrètement, nous considérons le problème de partage du spectre électromagnétique entre plusieurs utilisateurs sous contraintes d'interférence mutuelle. Notre objectif est de contribuer à l'évaluation du gain de partage de cette ressource rare qu'est le spectre électromagnétique. En étudiant le canal gaussien d'interférence avec l'interférence traitée comme du bruit additif gaussien aux différents récepteurs, nous avons trouvé une description géométrique et plusieurs caractérisations de la région des débits atteignables. Ensuite, considérant un cas plus réaliste où chaque utilisateur a une certaine qualité de service, nous avons trouvé une condition nécessaire et suffisante pour permettre la communication simultanée à travers le canal gaussien d'interférence pour deux utilisateurs. Dans un scénario de partage entre un utilisateur primaire ayant une plus grande priorité d'accès au spectre et un utilisateur secondaire, après avoir déterminé des bornes minimales pour le débit du primaire en fonction du schéma d'allocation de puissance de l'utilisateur secondaire, nous avons proposé une technique originale d'allocation de puissance pour l'utilisateur secondaire accédant de manière opportuniste au spectre sous contraintes de performance de coupure pour tous les utilisateurs. En particulier, cette technique d'allocation de puissance n'utilise que l'information sur l'état des canaux des liens directs allant de l'émetteur secondaire vers les autres points du réseau. Finalement, considérant des modèles de canaux plus réalistes; après avoir montré l'existence d'une zone d'exclusion autour du récepteur primaire (zone où il n'y a aucun transmetteur secondaire, dans le but de protéger l'utilisateur primaire contre les fortes interférences), nous avons caractérisé l'effet du shadowing et du path-loss sur la zone d'exclusion du primaire.
%X In this thesis, we consider the problem of reliable data packets transmission using single-carrier signaling over frequency-selective fading channels. Our objective is to design enhanced transceivers with improved detection performance in the absence of channel state information at the transmitter by exploiting the available time-diversity in Chase combining Hybrid Automatic Repeat reQuest (HARQ) protocols. By analyzing the performance of the transmission scheme using an optimal maximum-likelihood receiver, we establish a suitable criterion for the study of system performance based on the statistics of the Euclidean distance at the output of a frequency-selective channel. From this theoretical framework, we propose a novel transmit-diversity scheme between subsequent HARQ transmissions, called phase-precoding, which allows the mitigation of intersymbol interference for slow time-varying channels. Then, with the help of our analytical tools, we revisit another transmit-diversity scheme which is the bit-interleaving diversity scheme. In particular, we emphasize the double advantage offered by this diversity scheme including the inherent modulation diversity in addition to the intersymbol interference reduction. Subsequently, we perform a comparative study between phase-precoding and bit-interleaving diversity schemes under iterative and non-iterative receiver structures. Finally, we introduce a new adaptive retransmission protocol for a multi-layer transmission scheme for the mitigation of inter-layers interference for rapidly time-varying channels using limited feedback information.
%X Nous considérons dans cette thèse le problème de la transmission fiable de données par paquets en utilisant une transmission mono-porteuse sur des canaux sélectifs en fréquence à évanouissements. Notre objectif est de concevoir des couples émetteurs-récepteurs permettant d'améliorer les performances de la détection en l'absence d'information sur le canal à la transmission et ceci en exploitant la diversité temporelle disponible dans le cadre des protocoles de retransmission hybrides (HARQ). En analysant les performances du système de transmission avec un récepteur à maximum de vraisemblance, nous établissons un critère pertinent pour l'étude des performances du système basé sur les statistiques de la distance Euclidienne à la sortie du canal sélectif en fréquence. A partir de ce cadre théorique, nous proposons un nouveau schéma de diversité entre les différentes retransmissions, nommé précodage de phase, qui permet de combattre l'interférence entre symboles pour les canaux lentement variables dans le temps. Puis, à l'aide de nos outils d'analyse, nous revisitons un autre schéma de diversité qu'est la diversité d'entrelacement. En particulier, nous soulignons le double avantage offert par ce schéma, à savoir la diversité de modulation et la réduction de l'interférence entre symboles. Nous réalisons ensuite une étude comparative entre les deux schémas de diversité précédents sous traitement itératif ou non itératif au récepteur. Enfin, nous introduisons un nouveau protocole de retransmission adaptative pour les transmissions dîtes multi-couches afin de réduire l'interférence entre couches pour les canaux rapidement variant dans le temps utilisant des informations de retour limitées.
%X Designing an embedded system implies to look for the right algorithm/architecture compromise depending on the real-time constraints. For MPSoC an especially with reconfigurable devices which enable to modify the running executing support, a validation by a preliminar evaluation of the variable behaviors of a reactive system becomes necessary. This could be done by a high level simulation allowing to explore, early in the design flow, the architectural design space, hardware and software, especially the RTOS. The platform manager point of view is used to explore the systems reactions to the partitioning choices and also the influence of the various algorithms and the impact of implementations of the operating system's services refined in hardware or software. For that, a SystemC model composed of modular OS services allow to jointly and functionally simulate hardware, software tasks and the operating system, distributed on heterogeneous communicating execution nodes. To evaluate the perfect real-time reconfigurable architecture of a dynamical robot vision application, we explored its partitioning and the useful OS services accordingly. This model has been integrated in a big simulator of an heterogeneous chip designed to provide a Tera operations per second power.
%X Concevoir un système embarqué implique de trouver un compromis algorithme/architecture en fonction des contraintes temps-réel. Thèse : pour concevoir un MPSoC et plus particulièrement avec les circuits reconfigurables modifiant le support d'exécution en cours de fonctionnement, la nécessaire validation des comportements fluctuants d'un système réactif impose une évaluation préalable que l'on peut réaliser par simulation (de haut niveau) tout en permettant l'exploration de l'espace de conception architectural, matériel mais aussi logiciel, au plus tôt dans le flot de conception. Le point de vue du gestionnaire de la plateforme est adopté pour explorer à haut niveau les réactions du système aux choix de partitionnement impactés par l'algorithmique des services du système d'exploitation et leurs implémentations possibles. Pour cela un modèle modulaire de services d'OS simule fonctionnellement et conjointement en SystemC le matériel, les tâches logicielles et le système d'exploitation, répartis sur plusieurs noeuds d'exécution hétérogènes communicants. Ce modèle a permis d'évaluer l'architecture temps-réel idéale d'une application dynamique de vision robotique conjointement à l'exploration des services de gestion d'une zone reconfigurable modélisée. Ce modèle d'OS a aussi été intégré dans un simulateur de MPSoC hétérogène d'une puissance estimé à un Tera opérations par seconde.
%X This Ph.D Thesis stands at the crossroads of three scientific domains : algorithmarchitecture adequacy, bio-inspired vision systems in mobile robotics, and image processing. The goal is to make a robot autonomous in its visual perception, by the integration to the robot of this cognitive task, usually executed on remote processing servers. To achieve this goal, the design approach follows a path of algorithm architecture adequacy, where the different image processing steps of the vision system are minutely analysed. The image processing tasks are adapted and implemented on an embedded architecture in order to respect the real-time constraints imposed by the robotic context. Mobile robotics as an academic research topic based on bio-mimetism. The artificial vision system studied in our context uses a bio-inspired multiresolution approach, based on the extraction and formatting of interest zones of the image. Because of the complexity of these tasks and the many constraints due to the autonomy of the robot, the implementation of this vision system requires a rigorous and complete procedure for the software and hardware architectural exploration. This processus of exploration of the design space is presented in this document. The results of this exploration have led to the design of an architecture primarly based on parametrable and scalable dedicated hardware processing units (IPs), which will be implemented on an FPGA reconfigurable circuit. These IPs and the inner workings of each of them are described in the document. The impact of their architectural parameters on the FPGA resources is studied for the main processing units. The implementation of the software part is presented for several potential FPGA platforms. The achieved performance for this architectural solution are finally presented. These results allow us to conclude that the proposed solution allows the vision system to be embedded in mobile robots within the imposed real-time constraints.
%X La problématique de cette thèse se tient à l'interface des domaines scientifiques de l'adéquation algorithme architecture, des systèmes de vision bio-inspirée en robotique mobile et du traitement d'images. Le but est de rendre un robot autonome dans son processus de perception visuelle, en intégrant au sein du robot cette tâche cognitive habituellement déportée sur un serveur de calcul distant. Pour atteindre cet objectif, lapproche de conception employée suit un processus d'adéquation algorithme architecture, où les différentes étapes de traitement d'images sont analysées minutieusement. Les traitements d'image sont modifiés et déployés sur une architecture embarquée de façon à respecter des contraintes d'exécution temps-réel imposées par le contexte robotique. La robotique mobile est un sujet de recherche académique qui s'appuie notamment sur des approches bio-mimétiques. La vision artificielle étudiée dans notre contexte emploie une approche bio-inspirée multirésolution, basée sur l'extraction et la mise en forme de zones caractéristiques de l'image. Du fait de la complexité de ces traitements et des nombreuses contraintes liées à l'autonomie du robot, le déploiement de ce système de vision nécessite une démarche rigoureuse et complète d'exploration architecturale logicielle et matérielle. Ce processus d'exploration de l'espace de conception est présenté dans cette thèse. Les résultats de cette exploration ont mené à la conception d'une architecture principalement composée d'accélérateurs matériels de traitements (IP) paramétrables et modulaires, qui sera déployée sur un circuit reconfigurable de type FPGA. Ces IP et le fonctionnement interne de chacun d'entre eux sont décrits dans le document. L'impact des paramètres architecturaux sur l'utilisation des ressources matérielles est étudié pour les traitements principaux. Le déploiement de la partie logicielle restante est présenté pour plusieurs plate-formes FPGA potentielles. Les performances obtenues pour cette solution architecturale sont enfin présentées. Ces résultats nous permettent aujourd'hui de conclure que la solution proposée permet d'embarquer le système de vision dans des robots mobiles en respectant les contraintes temps-réel qui sont imposées.
%X The HESS experiment consists of a system of telescopes destined to observe cosmic rays. Since the project has achieved a high level of performances, a second phase of the project has been initiated. This implies the addition of a new telescope which is more sensitive than its predecessors and which is capable of collecting a huge amount of images. In this context, all data collected by the telescope can not be retained because of storage limitations. Therefore, a new real-time system trigger must be designed in order to select interesting events on the fly. The purpose of this thesis was to propose a trigger solution to efficiently discriminate events (images) which are captured by the telescope.The first part of this thesis was to develop pattern recognition algorithms to be implemented within the trigger. A processing chain based on neural networks and Zernike moments has been validated. The second part of the thesis has focused on the implementation of the proposed algorithms onto an FPGA target, taking into account the application constraints in terms of resources and execution time.
%X L'expérience HESS consiste en un système de télescopes permettant d'observer les rayonnements cosmiques. Compte tenu des résultats majeurs obtenus depuis son installation, la seconde phase du projet a été engagée. Celle-ci est en cours de réalisation et passe par l'ajout d'un télescope plus sensible et plus grand que ses prédécesseurs. Toutes les données collectées par ce télescope ne peuvent pas être conservées à cause des limites de stockage. Par conséquent, un système de déclencheur, dit trigger, performant doit être mis en place. L'objectif de cette thèse est de proposer une solution de reconnaissance de formes en temps réel dans un contexte fortement contraint et qui sera embarquée sur le télescope. La première partie de la thèse a consisté à élaborer une chaîne de reconnaissance des formes pour ce trigger. Une chaîne de traitement à base de réseau de neurones et des moments de Zernike a été validée. La seconde partie de la thèse a porté sur l'implantation des algorithmes retenus sur une cible FPGA en tenant compte des contraintes en termes de ressources et de temps d'exécution.
%X This Ph.D Thesis stands at the crossroads of three scientific domains : algorithm-architecture adequacy, bio-inspired vision systems in mobile robotics, and image processing.The goal is to make a robot autonomous in its visual perception, by the integration to the robot of this cognitive task, usually executed on remote processing servers.To achieve this goal, the design approach follows a path of algorithm architecture adequacy, where the different image processing steps of the vision system are minutely analysed.The image processing tasks are adapted and implemented on an embedded architecture in order to respect the real-time constraints imposed by the robotic context.Mobile robotics as an academic research topic based on bio-mimetism.The artificial vision system studied in our context uses a bio-inspired multi-resolution approach, based on the extraction and formatting of interest zones of the image.Because of the complexity of these tasks and the many constraints due to the autonomy of the robot, the implementation of this vision system requires a rigorous and complete procedure for the software and hardware architectural exploration.This processus of exploration of the design space is presented in this document.The results of this exploration have led to the design of an architecture primarly based on parametrable and scalable dedicated hardware processing units (IPs), which will be implemented on an FPGA reconfigurable circuit.These IPs and the inner workings of each of them are described in the document.The impact of their architectural parameters on the FPGA resources is studied for the main processing units.The implementation of the software part is presented for several potential FPGA platforms.The achieved performance for this architectural solution are finally presented.These results allow us to conclude that the proposed solution allows the vision system to be embedded in mobile robots within the imposed real-time constraints.
%X La problématique de cette thèse se tient à l'interface des domaines scientifiques de l'adéquation algorithme architecture, des systèmes de vision bio-inspirée en robotique mobile et du traitement d'images.Le but est de rendre un robot autonome dans son processus de perception visuelle, en intégrant au sein du robot cette tâche cognitive habituellement déportée sur un serveur de calcul distant.Pour atteindre cet objectif, l'approche de conception employée suit un processus d'adéquation algorithme architecture, où les différentes étapes de traitement d'images sont analysées minutieusement.Les traitements d'image sont modifiés et déployés sur une architecture embarquée de façon à respecter des contraintes d'exécution temps-réel imposées par le contexte robotique.La robotique mobile est un sujet de recherche académique qui s'appuie sur des approches bio-mimétiques.La vision artificielle étudiée dans notre contexte emploie une approche bio-inspirée multi-résolution, basée sur l'extraction et la mise en forme de zones caractéristiques de l'image.Du fait de la complexité de ces traitements et des nombreuses contraintes liées à l'autonomie du robot, le déploiement de ce système de vision nécessite une démarche rigoureuse et complète d'exploration architecturale logicielle et matérielle.Ce processus d'exploration de l'espace de conception est présenté dans cette thèse.Les résultats de cette exploration ont mené à la conception d'une architecture principalement composée d'accélérateurs matériels de traitements (IP) paramétrables et modulaires, qui sera déployée sur un circuit reconfigurable de type FPGA.Ces IP et le fonctionnement interne de chacun d'entre eux sont décrits dans le document.L'impact des paramètres architecturaux sur l'utilisation des ressources matérielles est étudié pour les traitements principaux.Le déploiement de la partie logicielle restante est présenté pour plusieurs plate-formes FPGA potentielles.Les performances obtenues pour cette solution architecturale sont enfin présentées.Ces résultats nous permettent aujourd'hui de conclure que la solution proposée permet d'embarquer le système de vision dans des robots mobiles en respectant les contraintes temps-réel imposées.
%X Iterative algorithms are now widely used in all areas of signal processing and digital communications. In modern communication systems, iterative algorithms are used for decoding low-density parity-check (LDPC) codes, a popular class of error-correction codes that are now widely used for their exceptional error-rate performance. In a more recent field known as compressed sensing, iterative algorithms are used as a method of reconstruction to recover a sparse signal from a linear set of measurements. This thesis primarily deals with the development of low-complexity iterative algorithms for the two aforementioned fields, namely, the design of low-complexity decoding algorithms for LDPC codes, and the development and analysis of a low complexity reconstruction algorithm called Interval-Passing Algorithm (IPA) for compressed sensing. In the first part of this thesis, we address the area of decoding algorithms for LDPC codes. It is well-known that LDPC codes suffer from the error floor phenomenon in spite of their exceptional performance, where traditional iterative decoders based on the belief propagation (BP) fail for certain low-noise configurations. Recently, a novel class of decoders called ''finite alphabet iterative decoders (FAIDs)'' were proposed that are capable of surpassing BP in the error floor at much lower complexity. In this work, we focus on the problem of selection of particularly good FAIDs for column-weight-three codes over the Binary Symmetric channel (BSC). Traditional methods for decoder selection use asymptotic techniques such as the density evolution method, which do not guarantee a good performance on finite-length codes especially in the error floor region. Instead, we propose a methodology for selection that relies on the knowledge of potentially harmful topologies that could be present in a code, using the concept of noisy trapping set. Numerical results are provided to show that FAIDs selected based on our methodology outperform BP in the error floor on several codes. In the second part of this thesis, we address the area of iterative reconstruction algorithms for compressed sensing. Iterative algorithms have been proposed for compressed sensing in order to tackle the complexity of the LP reconstruction method. In this work, we modify and analyze a low complexity reconstruction algorithm called the IPA which uses sparse matrices as measurement matrices. Similar to what has been done for decoding algorithms in the area of coding theory, we analyze the failures of the IPA and link them to the stopping sets of the binary representation of the sparse measurement matrices used. The performance of the IPA makes it a good trade-off between the complex L1-minimization reconstruction and the very simple verification decoding.
%X L'utilisation d'algorithmes itératifs est aujourd'hui largement répandue dans tous les domaines du traitement du signal et des communications numériques. Dans les systèmes de communications modernes, les algorithmes itératifs sont utilisés dans le décodage des codes "low-density parity-check" (LDPC), qui sont une classe de codes correcteurs d'erreurs utilisés pour leurs performances exceptionnelles en terme de taux d'erreur. Dans un domaine plus récent qu'est le "compressed sensing", les algorithmes itératifs sont utilisés comme méthode de reconstruction afin de recouvrer un signal ''sparse" à partir d'un ensemble d'équations linéaires, appelées observations. Cette thèse traite principalement du développement d'algorithmes itératifs à faible complexité pour les deux domaines mentionnés précédemment, à savoir le design d'algorithmes de décodage à faible complexité pour les codes LDPC, et le développement et l'analyse d'un algorithme de reconstruction à faible complexité, appelé ''Interval-Passing Algorithm (IPA)'', dans le cadre du "compressed sensing". Dans la première partie de cette thèse, nous traitons le cas des algorithmes de décodage des codes LDPC. Il est maintenu bien connu que les codes LDPC présentent un phénomène dit de ''plancher d'erreur" en raison des échecs de décodage des algorithmes de décodage traditionnels du types propagation de croyances, et ce en dépit de leurs excellentes performances de décodage. Récemment, une nouvelle classe de décodeurs à faible complexité, appelés ''finite alphabet iterative decoders (FAIDs)'' ayant de meilleures performances dans la zone de plancher d'erreur, a été proposée. Dans ce manuscrit nous nous concentrons sur le problème de la sélection de bons décodeurs FAID pour le cas de codes LDPC ayant un poids colonne de 3 et le cas du canal binaire symétrique. Les méthodes traditionnelles pour la sélection des décodeurs s'appuient sur des techniques asymptotiques telles que l'évolution de densité, mais qui ne garantit en rien de bonnes performances sur un code de longueurs finies surtout dans la région de plancher d'erreur. C'est pourquoi nous proposons ici une méthode de sélection qui se base sur la connaissance des topologies néfastes au décodage pouvant être présente dans un code en utilisant le concept de "trapping sets bruités''. Des résultats de simulation sur différents codes montrent que les décodeurs FAID sélectionnés grâce à cette méthode présentent de meilleures performance dans la zone de plancher d'erreur comparé au décodeur à propagation de croyances. Dans un second temps, nous traitons le sujet des algorithmes de reconstruction itératifs pour le compressed sensing. Des algorithmes itératifs ont été proposés pour ce domaine afin de réduire la complexité induite de la reconstruction par "linear programming''. Dans cette thèse nous avons modifié et analysé un algorithme de reconstruction à faible complexité dénommé IPA utilisant les matrices creuses comme matrices de mesures. Parallèlement aux travaux réalisés dans la littérature dans la théorie du codage, nous analysons les échecs de reconstruction de l'IPA et établissons le lien entre les "stopping sets'' de la représentation binaire des matrices de mesure creuses. Les performances de l'IPA en font un bon compromis entre la complexité de la reconstruction sous contrainte de minimisation de la norme $ell_1$ et le très simple algorithme dit de vérification.
%X The time taken by standard Monte Carlo (MC) simulation to calculate the Frame Error Rate (FER) increases exponentially with the increase in Signal-to-Noise Ratio (SNR). Importance Sampling (IS) is one of the most successful techniques used to reduce the simulation time. In this thesis, we investigate an advanced version of IS, called Adap- tive Importance Sampling (AIS) algorithm to efﬁciently evaluate the performance of Forward Error Correcting (FEC) codes at very low error rates. First we present the inspirations and motivations behind this work by analyz- ing different approaches currently in use, putting an emphasis on methods inspired by Statistical Physics. Then, based on this qualitative analysis, we present an optimized method namely Fast Flat Histogram (FFH) method, for the performance evaluation of FEC codes which is generic in nature. FFH method employs Wang Landau algorithm and is based on Markov Chain Monte Carlo (MCMC). It operates in an AIS framework and gives a good simulation gain. Sufﬁcient statistical accuracy is ensured through dif- ferent parameters. Extention to other types of error correcting codes is straight forward. We present the results for LDPC codes and turbo codes with different code- lengths and rates showing that the FFH method is generic and is applicable for different families of FEC codes having any length, rate and structure. Moreover, we show that the FFH method is a powerful tool to tease out the pseudocodewords at high SNR region using Belief Propagation as the decoding algorithm for the LDPC codes.
%X Dans cette thèse, nous abordons le sujet d'optimisation des méthodes utlisées pour l'évaluation de performance des codes correcteurs d'erreurs. La durée d'une simula- tion Monte Carlo pour estimer le taux d'erreurs dans un système de communication augmente exponentiellement avec l'accroissement du Rapport Signal sur Bruit (RSB). Importance Sampling (IS) est une des techniques qui permettent à réduire le temps de ces simulations. Dans ce travail, on a étudié et mis en oeuvre une version avancée d'IS, appelé Adaptive Importance Sampling (AIS), pour l'évaluation efﬁcace des codes cor- recteurs d'erreurs aux taux d'erreur très bas. D'abord, nous présentons les inspirations et motivations en analysant différentes approches actuellement mises en pratique. On s'intéresse plus particulièrement aux méthodes inspirées de la physique statistique. Ensuite, basé sur notre analyse qualita- tive, nous présentons une méthode optimisée, appelé la méthode de Fast Flat Histogram (FFH) qui est intrinsèquement très générique. La méthode emploie l'algorithme de Wang-Landau, l'algorithme de Metropolis-Hastings et les chaines de Markov. Elle fonctionne dans le cadre de l'AIS et nous donne un gain de simulation satisfaisant. Différents paramètres sont utilisés pour assurer une précision statistique sufﬁsante. L'extension vers d'autres types de codes correcteurs d'erreurs est directe. Nous présentons les résultats pour les codes LDPC et turbocodes ayant dif- férentes tailles et différents rendements. Par conséquent, nous montrons que la méthode FFH est générique et valable pour une large gamme des rendements, tailles et structures. De plus, nous montrons que la méthode FFH est un outil puissant pour trouver des pseudocodewords dans la région de RSB élévé en appliquant l'algorithme de décodage Belief Propagation aux codes LDPC.
%X At the heart of modern coding theory lies the fact that low-density parity-check (LDPC) codes can be efficiently decoded by message-passing algorithms which are traditionally based on the belief propagation (BP) algorithm. The BP algorithm operates on a graphical model of a code known as the Tanner graph, and computes marginals of functions on the graph. While inference using BP is exact only on loop-free graphs (trees), the BP still provides surprisingly close approximations to exact marginals on loopy graphs, and LDPC codes can asymptotically approach Shannon's capacity under BP decoding. However, on finite-length codes whose corresponding graphs are loopy, BP is sub-optimal and therefore gives rise to the error floor phenomenon. The error floor is an abrupt degradation in the slope of the error-rate performance of the code in the high signal-to-noise regime, where certain harmful structures generically termed as trapping sets present in the Tanner graph of the code, cause the decoder to fail. Moreover, the effects of finite precision that are introduced during hardware realizations of BP can further contribute to the error floor problem. In this dissertation, we introduce a new paradigm for finite precision iterative decoding of LDPC codes over the Binary Symmetric channel (BSC). These novel decoders, referred to as finite alphabet iterative decoders (FAIDs) to signify that the message values belong to a finite alphabet, are capable of surpassing the BP in the error floor region. The messages propagated by FAIDs are not quantized probabilities or log-likelihoods, and the variable node update functions do not mimic the BP decoder, which is in contrast to traditional quantized BP decoders. Rather, the update functions are simple maps designed to ensure a higher guaranteed error correction capability by using the knowledge of potentially harmful topologies that could be present in a given code. We show that on several column-weight-three codes of practical interest, there exist 3-bit precision FAIDs that can surpass the BP (floating-point) in the error floor without any compromise in decoding latency. Hence, they are able to achieve a superior performance compared to BP with only a fraction of its complexity. Additionally in this dissertation, we propose decimation-enhanced FAIDs for LDPC codes, where the technique of decimation is incorporated into the variable node update function of FAIDs. Decimation, which involves fixing certain bits of the code to a particular value during the decoding process, can significantly reduce the number of iterations required to correct a fixed number of errors while maintaining the good performance of a FAID, thereby making such decoders more amenable to analysis. We illustrate this for 3-bit precision FAIDs on column-weight-three codes. We also show how decimation can be used adaptively to further enhance the guaranteed error correction capability of FAIDs that are already good on a given code. The new adaptive decimation scheme proposed has marginally added complexity but can significantly improve the slope of the error floor performance of a particular FAID. On certain high-rate column-weight-three codes of practical interest, we show that adaptive decimation-enhanced FAIDs can achieve a guaranteed error-correction capability that is close to the theoretical limit achieved by maximum-likelihood decoding.
%X Les codes Low-Density Parity-Check (LDPC) sont au coeur de la recherche des codes correcteurs d'erreurs en raison de leur excellente performance de décodage en utilisant un algorithme de décodage itératif de type propagation de croyances (Belief Propagation - BP). Cet algorithme utilise la représentation graphique d'un code, dit graphe de Tanner, et calcule les fonctions marginales sur le graphe. Même si l'inférence calculée n'est exacte que sur un graphe acyclique (arbre), l'algorithme BP estime de manière très proche les marginales sur les graphes cycliques, et les codes LDPC peuvent asymptotiquement approcher la capacité de Shannon avec cet algorithme. Cependant, sur des codes de longueurs finies dont la représentation graphique contient des cycles, l'algorithme BP est sous-optimal et donne lieu à l'apparition du phénomène dit de plancher d'erreur. Le plancher d'erreur se manifeste par la dégradation soudaine de la pente du taux d'erreur dans la zone de fort rapport signal à bruit où les structures néfastes au décodage sont connues en termes de Trapping Sets présents dans le graphe de Tanner du code, entraînant un échec du décodage. De plus, les effets de la quantification introduite par l'implémentation en hardware de l'algorithme BP peuvent amplifier ce problème de plancher d'erreur. Dans cette thèse nous introduisons un nouveau paradigme pour le décodage itératif à précision finie des codes LDPC sur le canal binaire symétrique. Ces nouveaux décodeurs, appelés décodeurs itératifs à alphabet fini (Finite Alphabet Iterative Decoders - FAID) pour préciser que les messages appartiennent à un alphabet fini, sont capables de surpasser l'algorithme BP dans la région du plancher d'erreur. Les messages échangés par les FAID ne sont pas des probabilités ou vraisemblances quantifiées, et les fonctions de mise à jour des noeuds de variable ne copient en rien le décodage par BP ce qui contraste avec les décodeurs BP quantifiés traditionnels. En effet, les fonctions de mise à jour sont de simples tables de vérité conçues pour assurer une plus grande capacité de correction d'erreur en utilisant la connaissance de topologies potentiellement néfastes au décodage présentes dans un code donné. Nous montrons que sur de multiples codes ayant un poids colonne de trois, il existe des FAID utilisant 3 bits de précision pouvant surpasser l'algorithme BP (implémenté en précision flottante) dans la zone de plancher d'erreur sans aucun compromis dans la latence de décodage. C'est pourquoi les FAID obtiennent des performances supérieures comparées au BP avec seulement une fraction de sa complexité. Par ailleurs, nous proposons dans cette thèse une décimation améliorée des FAID pour les codes LDPC dans le traitement de la mise à jour des noeuds de variable. La décimation implique de fixer certains bits du code à une valeur particulière pendant le décodage et peut réduire de manière significative le nombre d'itérations requises pour corriger un certain nombre d'erreurs fixé tout en maintenant de bonnes performances d'un FAID, le rendant plus à même d'être analysé. Nous illustrons cette technique pour des FAID utilisant 3 bits de précision codes de poids colonne trois. Nous montrons également comment cette décimation peut être utilisée de manière adaptative pour améliorer les capacités de correction d'erreur des FAID. Le nouveau modèle proposé de décimation adaptative a, certes, une complexité un peu plus élevée, mais améliore significativement la pente du plancher d'erreur pour un FAID donné. Sur certains codes à haut rendement, nous montrons que la décimation adaptative des FAID permet d'atteindre des capacités de correction d'erreur proches de la limite théorique du décodage au sens du maximum de vraisemblance.
%X A huge effort has been made in the development of image classification systemswith the objective of creating high-quality thematic maps and to establishprecise inventories about land cover use. The peculiarities of Remote SensingImages (RSIs) combined with the traditional image classification challengesmake RSI classification a hard task. Many of the problems are related to therepresentation scale of the data, and to both the size and therepresentativeness of used training set.In this work, we addressed four research issues in order to develop effectivesolutions for interactive classification of remote sensing images.The first research issue concerns the fact that image descriptorsproposed in the literature achieve good results in various applications, butmany of them have never been used in remote sensing classification tasks.We have tested twelve descriptors that encodespectral/color properties and seven texture descriptors. We have also proposeda methodology based on the K-Nearest Neighbor (KNN) classifier for evaluationof descriptors in classification context. Experiments demonstrate that JointAuto-Correlogram (JAC), Color Bitmap, Invariant Steerable Pyramid Decomposition(SID), and Quantized Compound Change Histogram (QCCH) yield the best results incoffee and pasture recognition tasks.The second research issue refers to the problem of selecting the scaleof segmentation for object-based remote sensing classification. Recentlyproposed methods exploit features extracted from segmented objects to improvehigh-resolution image classification. However, the definition of the scale ofsegmentation is a challenging task. We have proposedtwo multiscale classification approaches based on boosting of weak classifiers.The first approach, Multiscale Classifier (MSC), builds a strongclassifier that combines features extracted from multiple scales ofsegmentation. The other, Hierarchical Multiscale Classifier (HMSC), exploits thehierarchical topology of segmented regions to improve training efficiencywithout accuracy loss when compared to the MSC. Experiments show that it isbetter to use multiple scales than use only one segmentation scale result. Wehave also analyzed and discussed about the correlation among the useddescriptors and the scales of segmentation.The third research issue concerns the selection of training examples and therefinement of classification results through multiscale segmentation. We have proposed an approach forinteractive multiscale classification of remote sensing images.It is an active learning strategy that allows the classification resultrefinement by the user along iterations. Experimentalresults show that the combination of scales produces better results thanisolated scales in a relevance feedback process. Furthermore, the interactivemethod achieves good results with few user interactions. The proposed methodneeds only a small portion of the training set to build classifiers that are asstrong as the ones generated by a supervised method that uses the whole availabletraining set.The fourth research issue refers to the problem of extracting features of ahierarchy of regions for multiscale classification. We have proposed a strategythat exploits the existing relationships among regions in a hierarchy. Thisapproach, called BoW-Propagation, exploits the bag-of-visual-word model topropagate features along multiple scales. We also extend this idea topropagate histogram-based global descriptors, the H-Propagation method. The proposedmethods speed up the feature extraction process and yield good results when compared with globallow-level extraction approaches.
%X L'objectif de cette thèse est de développer des solutions efficaces pour laclassification interactive des images de télédétection. Cet objectif a étéréalisé en répondant à quatre questions de recherche.La première question porte sur le fait que les descripteursd'images proposées dans la littérature obtiennent de bons résultats dansdiverses applications, mais beaucoup d'entre eux n'ont jamais été utilisés pour la classification des images de télédétection. Nous avons testé douzedescripteurs qui codent les propriétés spectrales et la couleur, ainsi que septdescripteurs de texture. Nous avons également proposé une méthodologie baséesur le classificateur KNN (K plus proches voisins) pour l'évaluation desdescripteurs dans le contexte de la classification. Les descripteurs Joint Auto-Correlogram (JAC),Color Bitmap, Invariant Steerable Pyramid Decomposition (SID) etQuantized Compound Change Histogram (QCCH), ont obtenu les meilleursrésultats dans les expériences de reconnaissance des plantations de café et depâturages.La deuxième question se rapporte au choix del'échelle de segmentation pour la classification d'images baséesur objets.Certaines méthodes récemment proposées exploitent des caractéristiques extraitesdes objets segmentés pour améliorer classification des images hauterésolution. Toutefois, le choix d'une bonne échelle de segmentation est unetâche difficile.Ainsi, nous avons proposé deux approches pour la classification multi-échelles fondées sur le les principes du Boosting, qui permet de combiner desclassifieurs faibles pour former un classifieur fort.La première approche, Multiscale Classifier (MSC), construit unclassifieur fort qui combine des caractéristiques extraites de plusieurséchelles de segmentation. L'autre, Hierarchical Multiscale Classifier(HMSC), exploite la topologie hiérarchique de régions segmentées afind'améliorer l'efficacité des classifications sans perte de précision parrapport au MSC. Les expériences montrent qu'il est préférable d'utiliser des plusieurs échelles plutôt qu'une seul échelle de segmentation. Nous avons également analysé et discuté la corrélation entre lesdescripteurs et des échelles de segmentation.La troisième question concerne la sélection des exemplesd'apprentissage et l'amélioration des résultats de classification basés sur lasegmentation multiéchelle. Nous avons proposé une approche pour laclassification interactive multi-échelles des images de télédétection. Ils'agit d'une stratégie d'apprentissage actif qui permet le raffinement desrésultats de classification par l'utilisateur. Les résultats des expériencesmontrent que la combinaison des échelles produit de meilleurs résultats que leschaque échelle isolément dans un processus de retour de pertinence. Par ailleurs,la méthode interactive permet d'obtenir de bons résultats avec peud'interactions de l'utilisateur. Il n'a besoin que d'une faible partie del'ensemble d'apprentissage pour construire des classificateurs qui sont aussiforts que ceux générés par une méthode supervisée qui utilise l'ensembled'apprentissage complet.La quatrième question se réfère au problème de l'extraction descaractéristiques d'un hiérarchie des régions pour la classificationmulti-échelles. Nous avons proposé une stratégie qui exploite les relationsexistantes entre les régions dans une hiérarchie. Cette approche, appelée BoW-Propagation, exploite le modèle de bag-of-visual-word pour propagerles caractéristiques entre les échelles de la hiérarchie. Nous avons égalementétendu cette idée pour propager des descripteurs globaux basés sur leshistogrammes, l'approche H-Propagation. Ces approches accélèrent leprocessus d'extraction et donnent de bons résultats par rapport à l'extractionde descripteurs globaux.
%X The objective of this thesis is to draw inspiration from the neurobiology to model low level emotional mechanisms on a robot evolving in real environment. This work presents an emotional model coherent with experimental data describing the functioning of the cerebral structures involved in emotional mechanisms. Emotions play a central part in the regulation of behavior of humans as well as animals. In agreement with the darwinian view, emotions are seen as adaptive mechanisms enhancing survival. However, their organization around essential positive and negative signals gives them a dimensional flavor. Our model considers emotions as the result of the interaction dynamics between two systems. These systems allow the evaluation of the interactions with the physical and the social environment. This bio-inspired approach of emotions gives robots a basic framework to construct their behavioral autonomy and their communication skills. In this thesis, we show that they allow the robot to adapt itself to the characteristics of the environment as well as they underlie non verbal communication. The bio-mimetic approach of this thesis is reflected in methodological terms by the use of artificial neural networks for robot control architectures but also in functional terms by the organization of these networks as models of different brain structures and their interactions (amygdala, accumbens, periaqueductal grey, hippocampus, prefrontal cortex). Following the animat paradigm, the robot is seen as an animal which vital needs are satisfied by the resources of the environment. Experimentation are conducted on navigation behaviors relying on visuo-motor conditionings (visual strategy) and on path integration (proprioceptive strategy). Conditionings between nociceptive or hedonic signals and other sensory information or actions of the robot are at the basis of emotional regulation. The robot predictions allow it to learn aversive or appetitive behavior in response to its "pain" or "pleasure" expectations. The robot can also monitor its predictions to assess the effectiveness of its behaviors. This enables it to regulate its motivations and select its strategies (visual navigation or proprioceptive) and goals (environmental resources) in order to best meet its internal balance depending on its environment. This use of low level positive and negative signals allows to build a minimal emotional model providing autonomy to the robot behavior. In a second step, we use the emotional expressiveness as the basis for communication with the robot. A mechanical head enables it to express its emotions through its facial expressions. This communication consists in giving the robot reward and punishment signals. This exchange of information with the robot allows it to learn to valuate its environment or its behavior and thus to learn interactively to solve its navigation tasks. The model of emotional mechanisms presented in this work allows to investigate issues of autonomous robotics as well as issues of Human-Robot interactions. Moreover, this approach shows the interest of putting robotics at the heart of cognitive sciences due to the perspective given by the analysis of robot's behaviors supported by relatively simple neuronal architectures.
%X L'objectif de cette thèse est de s'inspirer de la neurobiologie pour modéliser les mécanismes émotionnels de bas niveau sur un robot évoluant en environnement réel. Ce travail présente un modèle des émotions cohérent avec les données expérimentales décrivant le fonctionnement des structures cérébrales principales impliquées dans les mécanismes émotionnels. Les émotions jouent un rôle capital aussi bien pour la régulation du comportement des êtres humains que des animaux. En accord avec la vision darwinienne, les émotions sont vues comme des mécanismes adaptatifs favorisant la survie. Cependant, leur organisation autours de signaux essentiellement positifs et négatifs leur donne un caractère dimensionnel. Notre modèle considère les émotions comme le résultat de la dynamique d'interactions entre deux systèmes permettant l'évaluation des interactions avec l'environnement physique d'une part et l'environnement social d'autre part. Cette approche bioinspirée des émotions permet de donner aux robots une mécanique de base pour construire leur autonomie comportementale et leurs capacités de communication. Dans cette thèse, nous montrons qu'elles permettent autant de s'adapter aux caractéristiques de l'environnement que de servir de support à une communication non verbale. L'approche biomimétique de notre travail se traduit en termes méthodologiques par l'utilisation de réseaux de neurones formels pour les architectures de contrôle du robot mais aussi en termes fonctionnels par l'organisation de ces réseaux comme modèles de différentes structures du cerveau et de leurs interactions (amygdale, accumbens, hippocampe et cortex préfrontal). Suivant le courant animat, le robot est vu comme un animal aux besoins vitaux satisfaits par les ressources de son environnement. Les expérimentations seront illustrées sur des comportements de navigation reposant sur les apprentissages de conditionnements visuo-moteurs (stratégie visuelle) et sur l'intégration de chemin (stratégie propioceptive). Les conditionnements associant les signaux nocicepteurs et hédoniques aux autres informations sensorielles ou aux actions du robot sont à la base des régulations émotionnelles. Les prédictions que forme le robot lui permettent d'apprendre des comportements aversifs ou appétitifs en réponse à ses anticipations de "douleur" ou de "plaisir". Il peut aussi monitorer ses prédictions afin d'évaluer l'efficacité de ses comportements. C'est ce qui lui permet de réguler ses motivations et de sélectionner ses stratégies (navigation visuelle ou proprioceptive) et ses buts (ressources de l'environnement) de façon à satisfaire au mieux son équilibre interne en fonction de son environnement. Cette utilisation de signaux bas niveau positifs et négatifs permet de construire un modèle émotionnel minimal assurant au robot une autonomie comportementale. Dans un deuxième temps, nous utilisons l'expressivité émotionnelle comme base à une communication avec le robot. Une tête mécanique permet au robot d'exprimer ses émotions grâce à ses expressions faciales. Cette communication consiste à donner au robot des signaux de récompense et de punition. Nous avons développé un modèle permettant de construire de manière autonome ces signaux d'interaction en leur donnant leur valeur émotionnelle. Cet échange d'informations avec le robot lui permet d'apprendre à valuer son environnement ou son comportement et ainsi d'apprendre interactivement à résoudre ses problèmes de navigation. La modélisation des mécanismes émotionnels présentées dans cette thèse permet d'aborder aussi bien les questions de robotique autonome que d'interactions Homme-Machine. Plus largement, cette approche illustre l'intérêt de placer la robotique au coeur des sciences cognitives grâce à l'éclairage que permet l'analyse des comportements rendus possibles par des architectures neuronales relativement simples.
%X An autonomous robot collaborating with humans should be able to learn how to navigate and manipulate objects in the same task. In a classical approach, independent functional modules are considered to manage the different aspects of the task (navigation, arm control,...) . To the contrary, the goal of this thesis is to show that learning tasks of different kinds can be tackled by learning sensorimotor attractors from a few task nonspecific structures. We thus proposed an architecture which can learn and encode attractors to perform navigation tasks as well as arm control.We started by considering a model inspired from place-cells for navigation of autonomous robots. On-line and interactive learning of place-action couples can let attraction basins emerge, allowing an autonomous robot to follow a trajectory. The robot behavior can be corrected and guided by interacting with it. The successive corrections and their sensorimotor coding enables to define the attraction basin of the trajectory. My first contribution was to adapt this principle of sensorimotor attractor building for the impedance control of a robot arm. While a proprioceptive posture is maintained, the arm movements can be corrected by modifying on-line the motor command expressed as muscular activations. The resulting motor attractors are simple associations between the proprioceptive information of the arm and these motor commands. I then showed that the robot could learn visuomotor attractors by combining the proprioceptive and visual information with the motor attractors. The visuomotor control corresponds to a homeostatic system trying to maintain an equilibrium between the two kinds of information. In the case of ambiguous visual information, the robot may perceive an external stimulus (e.g. a human hand) as its own hand. According to the principle of homeostasis, the robot will act to reduce the incoherence between this external information and its proprioceptive information. It then displays a behavior of immediately observed gestures imitation. This mechanism of homeostasis, completed by a memory of the observed sequences and action inhibition capability during the observation phase, enables a robot to perform deferred imitation and learn by observation. In the case of more complex tasks, we also showed that learning transitions can be the basis for learning sequences of gestures, like in the case of cognitive map learning in navigation. The use of motivational contexts then enables to choose between different learned sequences.We then addressed the issue of integrating in the same architecture behaviors involving visuomotor navigation and robotic arm control to grab objects. The difficulty is to be able to synchronize the different actions so the robot act coherently. Erroneous behaviors of the robot are detected by evaluating the actions predicted by the model with respect to corrections forced by the human teacher. These situations can be learned as multimodal contexts modulating the action selection process in order to adapt the behavior so the robot reproduces the desired task.Finally, we will present the perspectives of this work in terms of sensorimotor control, for both navigation and robotic arm control, and its link to human robot interface issues. We will also insist on the fact that different kinds of imitation behavior can result from the emergent properties of a sensorimotor control architecture.
%X Un robot autonome collaborant avec des humains doit être capable d'apprendre à se déplacer et à manipuler des objets dans la même tâche. Dans une approche classique, on considère des modules fonctionnels indépendants gérant les différents aspects de la tâche (navigation, contrôle du bras...). A l'opposé, l'objectif de cette thèse est de montrer que l'apprentissage de tâches de natures différentes peut être abordé comme un problème d'apprentissage d'attracteurs sensorimoteurs à partir d'un petit nombre de structures non spécifiques à une tâche donnée. Nous avons donc proposé une architecture qui permet l'apprentissage et l'encodage d'attracteurs pour réaliser aussi bien des tâches de navigation que de contrôle d'un bras.Comme point de départ, nous nous sommes appuyés sur un modèle inspiré des cellules de lieu pour la navigation d'un robot autonome. Des apprentissages en ligne et interactifs de couples lieu/action sont suffisants pour faire émerger des bassins d'attraction permettant à un robot autonome de suivre une trajectoire. En interagissant avec le robot, on peut corriger ou orienter son comportement. Les corrections successives et leur encodage sensorimoteur permettent de définir le bassin d'attraction de la trajectoire. Ma première contribution a été d'étendre ce principe de construction d'attracteurs sensorimoteurs à un contrôle en impédance pour un bras robotique. Lors du maintien d'une posture proprioceptive, les mouvements du bras peuvent être corrigés par une modification en-ligne des commandes motrices exprimées sous la forme d'activations musculaires. Les attracteurs moteurs résultent alors des associations simples entre l'information proprioceptive du bras et ces commandes motrices. Dans un second temps, j'ai montré que le robot pouvait apprendre des attracteursvisuo-moteurs en combinant les informations proprioceptives et visuelles. Le contrôle visuo-moteur correspond à un homéostat qui essaie de maintenir un équilibre entre ces deux informations. Dans le cas d'une information visuelle ambiguë, le robot peut percevoir un stimulus externe (e.g. la main d'un humain) comme étant sa propre pince. Suivant le principe d'homéostasie, le robot agira pour réduire l'incohérence entre cette information externe et son information proprioceptive. Il exhibera alors un comportement d'imitation immédiate des gestes observés. Ce mécanisme d'homéostasie, complété par une mémoire des séquences observées et l'inhibition des actions durant l'observation, permet au robot de réaliser des imitations différées et d'apprendre par observation. Pour des tâches plus complexes, nous avons aussi montré que l'apprentissage de transitions peut servir de support pour l'apprentissage de séquences de gestes, comme c'était le cas pour l'apprentissage de cartes cognitives en navigation. L'utilisation de contextes motivationnels permet alors le choix entre les différentes séquences apprises.Nous avons ensuite abordé le problème de l'intégration dans une même architecture de comportements impliquant une navigation visuomotrice et le contrôle d'un bras robotique pour la préhension d'objets. La difficulté est de pouvoir synchroniser les différentes actions afin que le robot agisse de manière cohérente. Les comportements erronés du robot sont détectés grâce à l'évaluation des actions proposées par le modèle vis à vis des corrections imposées par le professeur humain. Un apprentissage de ces situations sous la forme de contextes multimodaux modulant la sélection d'action permet alors d'adapter le comportement afin que le robot reproduise la tâche désirée.Pour finir, nous présentons les perspectives de ce travail en terme de contrôle sensorimoteur, pour la navigation comme pour le contrôle d'un bras robotique, et son extension aux questions d'interface homme/robot. Nous insistons sur le fait que différents types d'imitation peuvent être le fruit des propriétés émergentes d'une architecture de contrôle sensorimotrice.
%X Efficient processing of ranking queries is an important issue in today information retrieval applications such as meta-search engines on the web, information retrieval in social networks, similarity search in multimedia databases, etc. We address the problem of top-k multi-criteria query processing, where queries are composed of a set of ranking predicates, each one expressing a measure of similarity between data objects on some specific criteria. Unlike traditional Boolean predicates returning true or false, similarity predicates return a relevance score in a given interval. The query also specifies an aggregation function that combines the scores produced by the similarity predicates. Query results are ranked following the global score and only the best k ones are returned.In this thesis, we first study the state of the art techniques and algorithms designed for top-k multi-criteria query processing in specific conditions for the type of access to the scores and cost settings, and propose a generic framework able to express any top-k algorithm. Then we propose a new breadth-first strategy that maintains the current best k objects as a whole instead of focusing only on the best one such as in all the state of the art techniques. We present Breadth-Refine (BR), a new top-k algorithm based on this strategy and able to adapt to any combination of source access types and to any cost settings. Experiments clearly indicate that BR successfully adapts to various settings, with better results than state of the art algorithms.Secondly, we propose an adaptation of top-k algorithms to approximate search aiming to a compromise between execution time and result quality. We explore approximation by early stopping of the execution and propose a first experimental study of the approximation potential of top-k algorithms. Finally, we focus on the application of multi-criteria top-k techniques to Large Scale Content-Based Image Retrieval. In this context an image is represented by one or several descriptors, usually numeric vectors that can be seen as points in a multidimensional space. We explore the k-Nearest Neighbors search on such space and propose "Multi-criteria Search Algorithm" (MSA) a new technique for approximate k-NN based on multi-criteria top-k techniques. We compare MSA with state of the art methods in the context of large multimedia databases, where the database and the index structure are stored on disk, and show that MSA quickly produces very good approximate results.
%X Le développement des techniques de traitement des requêtes de classement est un axe de recherche très actif dans le domaine de la recherche d'information. Plusieurs applications nécessitent le traitement des requêtes de classement multicritères, telles que les méta-moteurs de recherche sur le web, la recherche dans les réseaux sociaux, la recherche dans les bases de documents multimédia, etc. Contrairement aux requêtes booléennes traditionnelles, dans lesquelles le filtrage est basé sur des prédicats qui retournent vrai ou faux, les requêtes de classement utilisent des prédicats de similarité retournant un score de pertinence. Ces requêtes spécifient une fonction d'agrégation qui combine les scores individuels produits par les prédicats de similarité permettant de calculer un score global pour chaque objet. Les k objets avec les meilleurs scores globaux sont retournés dans le résultat final. Dans cette thèse, nous étudions dans un premier temps les techniques et algorithmes proposés dans la littérature conçus pour le traitement des requêtes top-k multicritères dans des contextes spécifiques de type et de coût d'accès aux scores, et nous proposons un cadre générique capable d'exprimer tous ces algorithmes. Ensuite, nous proposons une nouvelle stratégie en largeur "breadth-first", qui maintient l'ensemble courant des k meilleurs objets comme un tout, à la différence des stratégies en profondeur habituelles qui se focalisent sur le meilleur candidat. Nous présentons un nouvel algorithme "Breadth-Refine" (BR), basé sur cette stratégie et adaptable à n'importe quelle configuration de type et de coût d'accès aux scores. Nous montrons expérimentalement la supériorité de l'algorithme BR sur les algorithmes existants. Dans un deuxième temps, nous proposons une adaptation des algorithmes top-k à la recherche approximative, dont l'objectif est de trouver un compromis entre le temps de recherche et la qualité du résultat retourné. Nous explorons l'approximation par arrêt prématuré de l'exécution et proposons une première étude expérimentale du potentiel d'approximation des algorithmes top-k. Dans la dernière partie de la thèse, nous nous intéressons à l'application des techniques top-k multicritères à la recherche par le contenu dans les grandes bases de données multimédia. Dans ce contexte, un objet multimédia (une image par exemple) est représenté par un ou plusieurs descripteurs, en général sous forme de vecteurs numériques qui peuvent être vus comme des points dans un espace multidimensionnel. Nous explorons la recherche des k plus proches voisins (k-ppv) dans ces espaces et proposons une nouvelle technique de recherche k-ppv approximative "Multi-criteria Search Algorithm " (MSA) basée sur les principes des algorithmes top-k. Nous comparons MSA à des méthodes de l'état de l'art dans le contexte des grandes bases multimédia où les données ainsi que les structures d'index sont stockées sur disque, et montrons qu'il produit rapidement un très bon résultat approximatif.
%X Since its emergence in the 70s , smart cards have invaded the world market , their use has been steadily increasing and diversifying . Without necessarily realizing it , each of us has more than one in his wallet, bag, his briefcase ...All these cards have in common the fact of containing information about the holder, which can be used for identification in the different activities they want to perform . These information is present on the magnetic stripe and / or on the chip embedded in the card. With current technology and more specifically the miniaturization of electronic components , we are seeing complex components embedded in smart cards to meet greater needs for resources for applications increasingly sophisticated .The increasing use of on-board on a smart card systems leads to take into account various constraints in the design . Firstly, there are those related to embedded systems standards , such as the area, consumption and speed of execution. Followed by those related to the smart card itself , specificities related to the thickness and mechanical stress . There are also the constraints of consumption and surface.The appearance of non-contact has revolutionized the field of smart card. No more Need to insert the card into a reader to get the information . The data are not routed by the chip but via air through an integrated antenna. You have to be near the reader without necessarily take the card out of a pocket. They are known as RFID cards for Radio Frequency Identification.Other design constraints then appeared : the choice of the frequency communication for data exchange, the geometry of the antenna , the choice of the tag ...All components require a power source . The basic circuits called passive RFID draw their energy from the magnetic field near the reader but the complexity of certain circuits requires the presence of an integrated power supply into the card, in this case the circuits are called active tags. In general , thin and flexible batteries are used. Again , technology has made ​​tremendous progress and finer batteries with larger capacities emerged. All these elements constitute a real electronic circuit.This industrial thesis aims firstly to design an electronic circuit embedded in a bank card format meets the specifications defined taking into account the various constraints imposed by this format.This circuit must be flexible , autonomous and consuming the least possible energy.In a second step , once the product is produced and validated the goal is to optimize it proposing solutions to save time upstream design example or offering simple models, but taking into account all the constraints associated with this type of applications.
%X Depuis son apparition dans les années 70, les cartes à puce ont envahi le marché mondial, leur utilisation n'a cessé d'augmenter et de se diversifier. Sans forcément nous en rendre compte, chacun de nous en a plusieurs dans son portefeuille, son sac, son attaché-case... Toutes ces cartes ont pour point commun le fait de contenir des informations sur son titulaire qui servent à son identification dans les différentes actions qu'il souhaite effectuer. Ces informations sont présentes sur la piste magnétique et/ou la puce embarquée dans la carte. Avec les progrès technologiques actuels et plus précisément la miniaturisation des composants électroniques, nous sommes de plus en plus amenés à voir des composants complexes embarqués dans des cartes à puce pour satisfaire des besoins en ressources plus grands pour des applications de plus en plus sophistiquées. L'utilisation croissante du nombre des systèmes embarqués sur une carte à puce amène à prendre en compte différentes contraintes lors de la conception. Tout d'abord, il y a celles liées aux systèmes embarqués standards, telles que la surface, la consommation et la rapidité d'exécution. Ensuite viennent celles liées à la carte à puce en elle-même, des spécificités liées à l'épaisseur et aux contraintes mécaniques. On retrouve également des contraintes de consommation et de surface. L'apparition du sans-contact a révolutionné le domaine de la carte à puce. Plus besoin d'introduire la carte dans un lecteur pour lire les informations. Les données ne transitent plus par la puce mais via l'air grâce à une antenne intégrée. Il suffit de se trouver à proximité du lecteur sans forcément sortir la carte de poche ou du sac. Elles sont connues sous le nom de cartes RFID pour Radio Frequency Identification ou identifiction par radio fréquence. D'autres contraintes de conception sont alors apparues : choix de la fréquence à laquelle va se faire la communication et l'échange des données, la géométrie de l'antenne, le choix du tag... Tous les composants ont besoin d'une source d'alimentation. Les circuits RFID basiques dits passifs puisent leur énergie dans le champ magnétique produit à proximité du lecteur mais la complexité de certains circuits nécessite la présence d'une source d'alimentation intégrée dans la carte, dans ce cas les circuits sont désignés par actifs. En général, ce sont des batteries fines et flexibles qui sont utilisées. Là aussi, la technologie a fait d'immenses progrès et des batteries plus fines et avec de plus grandes capacités voient le jour. Ce sont ces batteries qui viennent alimenter les composants de la carte. Tous ces éléments constituent un véritable circuit électronique.Cette thèse industrielle a pour but dans un premier temps de concevoir un circuit électronique embarqué dans une carte au format bancaire en répondant à un cahier des charges bien défini tout en prenant en compte les différentes contraintes imposées par ce format. Ce circuit se devra d'être flexible, autonome et consommant le moins d'énergie possible. Dans un deuxième temps, une fois le produit réalisé et validé le but est de l'optimiser en proposant des solutions afin de faire gagner du temps en amont de la conception par exemple ou en proposant des modèles simples mais qui prennent en compte toutes les contraintes liées à ce type d'applications.
%X In this thesis, we focus on content based image retrieval in distributed collections. Wepresent a framework with online learning based on ant-like mobile agents. Mobile agentscrawl the network to find images matching a given example query. The images retrievedare shown to the user who labels them, following the classical relevant feedback scheme.The labels are used both to improve the similarity measure used for the retrieval and tolearn paths leading to sites containing relevant images. The relevant paths are learnedin an ethologically inspired way. We also present an extension with the re-use of learnedpaths for later sessions. This long-term learning step leads to further improvement in theinteraction time required to obtain good results.
%X Cette thèse a pour objet la recherche d’images par le contenu dans un contexte debases d’images distribuées. Nous nous plaçons dans le schéma classique des systèmes derecherche d’images interactifs, c’est-à-dire que le système présente des images à l’utili-sateur et celui-ci les annote afin d’affiner la recherche, ce que l’on appelle ”bouclage depertinence”. Nous proposons dans un premier temps, une extension de ce schéma à larecherche distribuée à l’aide d’un système multi-agents. Les agents parcourent le réseauà la recherche des images pertinentes et marquent les chemins pertinents afin de guiderles autres agents vers les sites intéressants. Notre stratégie s’inspire du comportement desfourmis et de leur marquage de l’environnement à l’aide de phéromones. Dans un secondtemps, nous nous intéressons à la ré-utilisation des marquages d’une session de rechercheà une autre. Cet apprentissage à long-terme permet aux agents de trouver plus facilementles sites contenant des images pertinentes, et offre une importante réduction du tempsd’interaction requis pour l’obtention de bons résultats.
%X For the last decade, Medical Image Analysis for Computer-Aided-Diagnosis (CAD) has been thecentral motivation of my research activity. With the constant increase of the imaging capabilities ofmedical devices and the huge amount of produced digital information, physicians are in real need for semiautomaticimage processing tools making possible fast, precise and robust analysis, including restoration,segmentation, pattern detection and recognition, quantitative analysis, etc. In this particular applicationarea, from an image processing perspective, my research work has mainly focused for the last 8 yearson two main tracks: (i) The study of the variational approach framework for image restoration andsegmentation which common point is the formalization of the related optimization problem under theform of a Partial Differential Equation (PDE); (ii) The development of embeddable pattern detectionand recognition methods based on statistical learning process for real-time in situ diagnosis.The main scientific contributions of my research activities have been since 2006: In image restoration:(i) The study of the stochastic resonance phenomenon in non-linear PDE for image restoration and (ii)The study of double-well potential functions for Gradient-Oriented-PDE in image restoration. In imagesegmentation: (i) An Active contour segmentation approach with learning-based shape prior information;(ii) An Alpha-divergence-based active contour image segmentation approach; (iii) A Fractional-entropybasedactive contour image segmentation approach. And finally in pattern recognition: The proposal of acomplete embeddable image processing scheme for in situ polyp detection in Wireless Capsule Endoscopyfor early colorectal cancer diagnosis.This manuscript proposes a detailed overview of these contributions as well as elements for my futureresearch activities.
